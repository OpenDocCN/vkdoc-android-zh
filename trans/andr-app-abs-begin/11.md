© Wallace Jackson 2017Wallace JacksonAndroid Apps for Absolute Beginners10.1007/978-1-4842-2268-3_11

# 11.数字视频:流式视频、MediaPlayer 和 MediaController 类

Wallace Jackson<sup>1</sup>(1)Lompoc, California, USAIn the previous chapter covering 2D Animation, we implemented motion graphics in Android and digital image file formats such as PNG or JPEG in conjunction with XML constructs to create frame-based animation, as well as using procedural (tween or vector) animation to rotate, scale, move, and fade UI elements.There is another way that you can play a series of frames in Android, called digital video. Digital video assets are especially well-suited for situations when you have hundreds or even thousands of frames, and cannot easily handle them all using an Android AnimationDrawable class. Additionally, digital video can be streamed over a network connection, which 2D animation assets cannot, and so new media can be external to your application.In this chapter, we are going to take all of the newfound knowledge that you gained in Chapter [10](10.html) regarding the fourth dimension of time, as well as concepts that you learned about such as frames and frame rates, and we will again expand upon that knowledge with new concepts such as bitrates and new digital video codec (file format) support in Android, including the popular MPEG-4 and WebM digital video formats that are also the formats that are used in HTML5 and JavaFX 8.We will be covering several frequently utilized Android classes you can use to implement video graphics design elements, such as a VideoView UI widget, a fullscreen UI layout container great for use with video, and three media-related Android classes that you can utilize to implement a digital video (or digital audio) transport UI, control and playback, including MediaPlayer (media playback), Uri and MediaController (transport controls). During the chapter we’ll use these digital video-related classes to create a digital video playback Activity. We’ll create a 3D flythrough with Terragen 4, and learn how to optimize digital video using Sorenson Squeeze Desktop Pro 11.

## 创建视频应用程序:FullscreenActivity

The first thing we need to do is to see if there is one of the dozen pure Android design patterns in Android Studio that is attuned to, or suits, digital video playback applications. Fortunately there is, and this gives us a chance to dive into yet another (a third so far) of the application development bootstrap templates that the New Android Studio Project series of dialogs offers to all the Android 7.x application development Absolute Beginners.Go into Android Studio 2.3 and the currently open NavDrawerPattern project and use the File ➤ Close Project menu option to close the project and open the Android Studio master dialog, as shown in Figure [11-1](#Fig1). As you can see on the left side of the dialog, your two existing projects are easily accessible at any time you wish to continue working on them. Since we are going to start a new digital video project, so that you can explore some of the other pure Android design pattern application templates that build a full working Android application for you, click on the Start a new Android Studio project option, which can be seen in the middle right of Figure [11-1](#Fig1), highlighted in red.![A324674_4_En_11_Fig1_HTML.jpg](A324674_4_En_11_Fig1_HTML.jpg)Figure 11-1.Use File ➤ Close Project on NavDrawerPattern, and then select Start a new Android Studio projectIn your Configure your new project dialog, name your application DigitalVideoMedia, and accept the other default settings and leave Include C++ Support unchecked, as is shown in the first pane in Figure [11-2](#Fig2). Click Next and accept the default settings in the Select the form factors your app will run on dialog seen in the middle of Figure [11-2](#Fig2).![A324674_4_En_11_Fig2_HTML.jpg](A324674_4_En_11_Fig2_HTML.jpg)Figure 11-2.The Configure Your New Project, Select the Form Factors, and Customize the Activity dialogsClick Next from the Select Form Factors dialog, and select the Fullscreen Activity, shown selected in blue in Figure [11-3](#Fig3), and click the Next button. In the final Customize the Activity dialog, seen on the far right in Figure [11-2](#Fig2), title the Activity DigitalVideoPlayer, and leave the default Activity and Layout names as suggested by Android Studio as FullscreenActivity and activity_fullscreen, and again click on the Finish button.![A324674_4_En_11_Fig3_HTML.jpg](A324674_4_En_11_Fig3_HTML.jpg)Figure 11-3.Select the Fullscreen Activity option in the Add an Activity to Mobile dialog and click Next buttonThis will create a DigitalVideoMedia project and FullscreenActivity in Android Studio as shown in Figure [11-4](#Fig4). Take a look at the XML UI design hierarchy to ascertain how it works like you did in Chapter [6](06.html).![A324674_4_En_11_Fig4_HTML.jpg](A324674_4_En_11_Fig4_HTML.jpg)Figure 11-4.Take a look at the activity_fullscreen.xml UI XML file to ascertain how the tag hierarchy is set upNext, let’s test the bootstrap code that Android Studio has created for us to make sure that it works. Be sure to use your AVD often at each stage of development to make sure your application works after each major code addition, even the initial bootstrap code, to make sure none of the API that underlies the code has changed.Use the Run ➤ Run ‘app’ menu sequence to execute the DigitalVideoMedia application in the Android emulator, as is shown in Figure [11-5](#Fig5). I show the initial startup screen on the far left, the fullscreen mode with dummy content in the middle pane, and the UI overlay mode (non-fullscreen content) on the far right.![A324674_4_En_11_Fig5_HTML.jpg](A324674_4_En_11_Fig5_HTML.jpg)Figure 11-5.Test the DigitalVideoMedia project with Run ➤ Run ‘app’ to see how the FullscreenActivity worksLet’s take an in-depth look at Android’s Framelayout class, which provides a foundation for this design pattern.

## FrameLayout 类:组织 DV 内容

The Android FrameLayout class is the most basic layout container class, as it provides simple frame layout for content. Often this content involves fullscreen digital video, which is why I waited until this chapter to cover the FrameLayout class in detail. A FrameLayout is often utilized to contain one single UI widget that contains some sort of new media content such as digital video or animation. An example of a complex UI widget that would be perfect to use in conjunction with this FrameLayout container would be the VideoView widget. The VideoView widget (class), which we’ll go over in detail later on this chapter, is designed to contain an MPEG-4 or WebM digital video asset. We will be covering how to create a digital video asset from scratch, and how to optimize it.The FrameLayout class is a public class that extends the ViewGroup superclass, which as you know is a master blueprint class that is used to create Android layout container subclasses. The FrameLayout class hierarchy, which starts with the Java language Object master class, would look like the following Java class hierarchy:java.lang.Object> android.view.View> android.view.ViewGroup> android.widget.FrameLayoutThe FrameLayout class was designed by Android OS developers to specify the area on the display screen that is intended to display one single item. This is why it is named using the term “frame,” as typically a frame holds a single image. For this reason, you should design a FrameLayout UI to hold one or two child widgets. Because the FrameLayout class does not have a lot of methods defined that allow a lot of layout positioning attributes (or parameters), it’s most basic of the Android layout classes. This also makes the class quite memory efficient!If you try to use multiple child UI widgets inside of your parent FrameLayout container, you will find that it can be difficult to position multiple design elements accurately in a way that is scalable across different screen sizes, shapes, and orientations. This is due to the lack of advanced layout positioning attributes.What happens if you attempt to use FrameLayout containers to organize multiple UI elements is that you would see a high occurrence of UI elements overlapping each other. This is not the professional result that you should be seeking for your pure Android design patterns. There are better layout containers, some of which you have seen already in this book, like the (horizontal or vertical) LinearLayout, RelativeLayout, and the GridLayout.The only way to control positioning of your child UI widgets within the parent FrameLayout UI container is by assigning a layout gravity parameter for each child widget. This is done using the android:layout_gravity parameter inside of each UI widget’s child tag in your FrameLayout XML user interface definition file, which you have seen in lines 9 and 23 in Figure [11-4](#Fig4). Here the TextView uses android:layout_gravity="center" and the LinearLayout container uses the android:layout_gravity="bottom|center_horizontal" to place a UI Button at the bottom of the non-fullscreen UI design. We will be learning all about the FrameLayout class during this section, as well as its nested classes, which provide FrameLayout parameters, during the next section.This layout_gravity design parameter does not allow developers to do the pixel-precise positioning, or relative positioning, that is possible using the other more advanced (and less memory-efficient) layout container classes. The FrameLayout class essentially allows Android to do all of your UI design positioning, so that you can scale your UI design to fit all the different Android device screen sizes and orientations, usually in fullscreen mode, which is the optimal mode for use with digital video assets. This gravity parameter for the FrameLayout class is provided by a nested class called FrameLayout.LayoutParams. We’ll be covering this nested class in the next section of this chapter. It’s identical to the gravity parameter that is used in other ViewGroup layout container classes. I am going to cover layout gravity in detail in this chapter, because it is an important UI design concept.NoteNested classes are attached to the parent class in Java using dot notation, so the FrameLayout nested LayoutParams class would be FrameLayout.LayoutParams.Since it is a basic UI layout class, you can also utilize the FrameLayout class as a superclass, for the purpose of creating more specialized UI-related classes. Any class that you create by subclassing the FrameLayout class would be termed a direct subclass of the FrameLayout class. If added to the Android API, it becomes a known direct subclass; otherwise if it remains private (with you), it would be termed an (unknown) direct subclass.Some of the known direct subclasses of FrameLayout, that is, FrameLayout subclasses that have already been coded for you, would include: DatePicker, TabHost, MediaController, CalendarView, ScrollView, TimePicker, ViewAnimator, HorizontalScrollView, GestureOverlayView, and the AppWidgetHostView class.FrameLayout also has several known indirect subclasses. These are part of the Android API and are subclasses of the above known direct subclasses. Indirect subclasses include: TextSwitcher, ViewFlipper, ImageSwitcher, FragmentTabHost, and the ViewSwitcher class. Next, let’s take a look at the FrameLayout.LayoutParams class.

### 框架布局。LayoutParams 嵌套类:Gravity

The FrameLayout.LayoutParams class is a nested class that subclasses the ViewGroup.MarginLayoutParams nested class, which, in turn, subclasses (extends) the ViewGroup.LayoutParams nested class, which was coded originally in order to create layout parameters for all ViewGroup subclasses. Android layout parameter, or LayoutParams, nested classes are what provide the layout parameters for your UI designs, which are usually created via XML, as you have seen throughout the course of this book. The Android Java class hierarchy would therefore be structured in the following fashion:java.lang.Object> android.view.ViewGroup.LayoutParams> android.view.ViewGroup.MarginLayoutParams> android.widget.FrameLayout.LayoutParamsFrameLayout.LayoutParams inherits all ViewGroup.LayoutParams as well as ViewGroup.MarginLayoutParams (margin parameters), and then the class adds the layout_gravity parameter and its constants, which we’re going to cover in detail in this section, since these constants are specifically intended to be used with the FrameLayout UI container that we’re going to be using for the digital video playback engine we are creating in this chapter.MarginLayoutParams was created between FrameLayout.LayoutParams and ViewGroup.LayoutParams to leverage the modularity of Java class design. This splits the margin layout parameters out from LayoutParams, so that if you do not need to include margin support in a layout container, you can subclass LayoutParams rather than MarginLayoutParams. A nested class is usually a helper class, containing constants or parameters to be used with that nesting class.The most often used gravity constant with the FrameLayout container is fill, as one usually wants content or UI elements, such as the VideoView, to be scaled up to fit the frame (display) if they are smaller (pixel dimension) than the device screen. Your second most often used constant would be center, which is similar to fill, but does not scale (upsample) the content; rather it centers the content or the UI element (the child widget) in the display.There are also constants provided to fill or center your UI widget, or any nested UI layout container, in only the horizontal (X-axis) or the vertical (Y-axis) dimensions. These would be the fill_vertical or fill_horizontal , and the center_vertical or center_horizontal constants. These constants, shown in Table [11-1](#Tab1) along with the other 14 constants, will allow you to fine-tune how Android will position your UI widget inside of your FrameLayout UI container. It is important to note that using fill_vertical or fill_horizontal may well change the content aspect ratio, which may distort that content in an undesirable way, especially if that content has human subjects in it.Table 11-1.The android:layout_gravity constants defined by nested class FrameLayout.LayoutParams

<colgroup><col> <col></colgroup> 
| 重力常数 | 使用该重力常数指定的函数 |
| --- | --- |
| 榜首 | 将 UI 元素与框架布局容器的顶部对齐 |
| 底部 | 将 UI 元素对齐或位于框架布局容器的底部 |
| 左 | 将 UI 元素对齐或位于框架布局容器的左侧 |
| 右 | 将 UI 元素对齐或位于框架布局容器的右侧 |
| 居中 _ 垂直 | 垂直居中 UI 元素(或 UI 布局容器) |
| 【中心水平】 | 将 UI 元素(或 UI 布局容器)水平居中 |
| 居中 | 将 UI 元素对齐或位于框架布局容器的中心 |
| 【垂直填充 _ T2】 | 缩放 UI 元素(或布局容器)以垂直填充框架 |
| 填充 _ 水平 | 水平缩放 UI 元素(或布局容器)以填充框架 |
| 填 | 缩放 UI 元素到填充框架布局容器 |
| 剪辑 _ 垂直 | 为 FrameLayout 裁剪 UI 元素的上下边缘 |
| 夹 _ 横 | 为 FrameLayout 剪辑 UI 元素的左右边缘 |
| 开始 | 将 UI 元素对齐到 FrameLayout 容器的开始处 |
| 结束 | 将 UI 元素对齐或位于 FrameLayout 容器的端 |

There are some more advanced constants, such as clip_horizontal and clip_vertical, which are conceptually a bit more challenging, as these will “clip” (that is, remove a portion of) your UI element or content, in either the horizontal (X-axis) or the vertical (Y-axis). In digital imaging, this operation is termed “cropping,” and instead of scaling your content (or UI design) to fit any given screen dimensions, these clipping constants will instead remove parts of your content or design, in order to make it fit the new screen size and dimensions. This prevents often unwanted aspect ratio distortion, by removing fringe pixels, rather than distorting the asset using assymetric scaling.Finally, you can use start and end constants to implement both RTL (Right To Left) and LTR (Left To Right) directional UI layouts. These would replace your left and right constants (for LTR), or right and left constants (for RTL). If you are developing for end users that use RTL languages, and you need your UI designs to be able to mirror this type of RTL language design scenario, use the start and end constants, instead of left and right.The RTL and LTR layout constants were added in Android 4.2 to allow design support for languages that are read starting on the right side of the screen, and moving toward the left side of the screen. Android 7 OS will automatically reverse the value of the start and end constants, depending on whether a RTL or LTR screen direction is being used by the user (that is, depending upon the language setting).Often these gravity constants will be used in a FrameLayout when there is more than one child widget (multiple UI widgets, or nested ViewGroup layout containers). This is similar to the way you should use the top, bottom, left, and right constants to pin UI widgets or layout containers around the sides of the FrameLayout (display). Again, be sure to not use too many child UI elements within your FrameLayout, and use gravity parameters to position them so that they do not overlay, even when screen dimensions, size, and shape change across Android devices such as smartphones, tablets and iTV sets.It is important to remember that gravity is used for generalized positioning, not for precise positioning, like the parameters that you find in the RelativeLayout class, for instance, which can provide UI designs that are precise and at the same time are also scalable to different screen sizes and shapes in a much more advanced fashion.Before we can recode our current fullscreen UI design, shown in Figure [11-4](#Fig4), into a digital video playback UI design pattern, we should get up to speed on the particulars of the VideoView UI widget and the lifestyle stages of a digital video asset and its playback parameters.

## VideoView 类:VideoPlayer 小部件

Before we get into all of the Java 8 code and how it interfaces to the XML markup to implement a video player inside of the FrameLayout container that Android Studio has installed in the DigitalVideoMedia Android application, I want to get into the VideoView class in detail, so that you have the foundational knowledge about how all of these classes work. After that we will be able to implement this knowledge and finish up the XML UI design definition that will feature a FrameLayout that contains a VideoView. After that, we’ll take some time to learn about digital video concepts as they relate to Android 7 OS, and create video using the Terragen 4 virtual world creation software package along with Sorenson Squeeze, a video encoding suite. At the end of the chapter we will learn about the MediaPlayer and MediaController classes, and finish up with the Java 8 programming.Android’s VideoView widget class is a direct subclass of the SurfaceView class, which is a direct subclass of the Android View class, which is a direct subclass of the java.lang.Object master class. Android’s VideoView class hierarchy would therefore be structured as follows:java.lang.Object> android.view.View> android.view.SurfaceView> android.widget.VideoViewThe SurfaceView superclass (a View subclass) is similar to the FrameLayout (ViewGroup subclass) in as much as it is intended to provide a class for creating View widgets that are used for one sole purpose: playing content on their surface. In the case of a VideoView subclass, this would be playing digital video content on the surface of the View object, as is clearly evident in the naming of this digital video playback optimized class.The Android VideoView class is stored in the android.widget package, making the VideoView a user interface element, which we know is called a UI widget in Android OS. For this reason, your import statement for using a VideoView class in an Android application would reference android.widget.VideoView as its package path.This VideoView class is a public class, and has two dozen method calls and callbacks that one might think of as actually being part of the Android MediaPlayer class, which we’ll be covering in a future section of the chapter.You can access these MediaPlayer functions (method calls) via the VideoView class. Thus, ultimately, you will utilize these two classes as inexorably bound together. You’ll see how these two key Android classes intertwine as we progress through this chapter, especially when we get into Java program logic a bit later on in the chapter. It is rare that a UI design class will integrate with a media playback engine in Android, so this is a special case.We will take a closer look here at some of the more useful video playback control method calls, so that you are familiar with them, in case you need to implement any of the extended digital video features in your own video playback applications. In the next section, you will also need to review your Android VideoView digital video playback lifecycle, so that you will know exactly how all of the various video playback states all fit together.The basic VideoView method calls include .pause(), .resume(), .stop(), .start(), .suspend(), and .stopPlayback(). There’s also a .setVideoURI() and a .setMediaController() method call, as well as a .setVideoPath() method call which accomplishes much of the same end result as the .setVideoURI() method call using a different parameter.There are four .get() method calls for polling or getting information about digital video assets. They include .getDuration(), .getCurrentPosition(), .getBufferPercentage() and .getAudioSessionId() as well as an .isPlaying() method call, so that you can see if the digital video asset is playing back at the current time.There are also three .can() method calls that ascertain what actions the VideoView can (or cannot) do, regarding the MediaPlayer object. These include: .canPause(), .canSeekBackward(), and .canSeekForward() method calls.There are also all of the standard event handling method calls, that will be inherited from the Android View superclass. These include the .onTouchEvent(), onKeyDown(), and onTrackballEvent() method calls, among all of the other event handlers. The event handler that is usually used with the VideoView, for instance, to bring up the MediaController transport UI control panel, is the onTouchEvent() event listener.Finally, there are specialized method calls, such as .resolveAdjustedSize(), or .onInitializeAccessibilityEvent(), which are included to allow developers to implement accessibility standards if needed for their video playback.

### VideoView 生命周期:视频回放阶段

Before you start working with the Android digital video-related classes, learn about digital video concepts, and create custom 3D digital video assets, you’ll need to understand the different stages which a digital video asset goes through in Android. Playing digital video may seem simple from an end user’s perspective. A Play, Pause, Rewind and Stop function will provide basic video transport control. All these are involved in the overall video playback process, which is sometimes referred to as the video playback lifecycle.There are some other under-the-hood stages that allow Android to load the video asset into memory, or to set parameters for playback, and similar system-level, behind-the-scenes considerations. These unseen digital video lifecycle stages will allow developers to have the flexibility to create an optimized digital video user experience, and these will serve to provide Android developers with a much wider variety of playback options.When you implement a VideoView widget, you are also instantiating a MediaPlayer object, even though you do not have to write any XML markup, or even write any Java code, to create the MediaPlayer object! MediaPlayer objects are essentially video playback engines, and will play digital video assets associated with the VideoView UI element. This is done using a URI object, and a video asset reference, which the URI object contains.The digital video codec (stands for: COder-DECoder) algorithm references the digital video asset using the URI, places it into memory and then decodes it, placing the result in the FrameLayout, using the VideoView. We will cover URI and MediaPlayer classes later on in this chapter. The video playback states are shown in Table [11-2](#Tab2).Table 11-2.Video Playback States, and how these affect the Android MediaPlayer object and its video playback

<colgroup><col> <col></colgroup> 
| 数字视频播放状态 | media player 对象发生了什么变化(数字视频播放阶段) |
| --- | --- |
| 空闲状态 | MediaPlayer 对象被实例化并准备配置 |
| 初始化状态 | MediaPlayer 对象被用数据路径初始化，使用URIT5】 |
| 准备状态 | MediaPlayer 对象被配置，或“准备好”，用于播放 |
| 开始状态 | MediaPlayer 对象为启动，正在解码视频流 |
| 暂停状态 | MediaPlayer 对象被暂停并停止解码视频帧 |
| 停止状态 | MediaPlayer 对象被停止并停止解码视频帧 |
| 回放完成 | MediaPlayer 对象被完成解码视频数据流 |
| 结束状态 | MediaPlayer 对象被结束和从系统内存中删除和 |

I’ll go through these eight states in the logical order in which they are used, as well as in the order in which they are listed in Table [11-2](#Tab2).

1.  1。当 MediaPlayer 对象第一次被实例化时，它实际上不会进行任何活动的视频回放。因此， MediaPlayer 对象最初将处于所谓的空闲状态，这很像一辆汽车在没有挂档和啮合时的空闲状态。
2.  2。一旦你用数字视频数据参考加载你的 MediaPlayer 对象，使用你的 URI 对象，使用 Uri.parse() 方法调用或者。setDataSource() 方法调用时， MediaPlayer 对象将进入所谓的初始化状态。在初始化的 MediaPlayer 对象状态和启动的 MediaPlayer 对象状态之间还有一个中间状态，称为准备好的 MediaPlayer 对象状态。T31】
3.  3。使用 MediaPlayer 进入准备状态。OnPreparedListener ，这是一个嵌套类，我们将在本章的 MediaPlayer 小节中学习，稍后我们将在 DigitalVideoMedia 应用程序 Java 代码的MainActivity.java app compat activity子类中使用它，我们将重新编码该子类以播放视频资产。
4.  4。一旦 MediaPlayer 对象已经初始化，并且加载了视频数据，通常就是准备(也就是配置，使用各种播放选项设置)。完成后， MediaPlayer 对象可以启动，这意味着数字视频资产帧将开始使用编解码器从系统内存中解码出来，并放入 VideoView 中，在 FrameLayout 内，它可以(并将)全屏显示以实现最佳播放质量。
5.  5。一旦开始，视频正在播放，可以使用使停止。stop() 方法调用，或者使用使暂停。暂停()方法调用。这三种视频状态，开始(播放)、停止(停止)和暂停(暂停)应该是所有数码视频用户最熟悉的，因为它们由视频走带控制条上的三个主要按钮表示。在 Android 操作系统中，数字视频传输条是使用 MediaController 类提供的，您很快就会看到。
6.  6。最后的 MediaPlayer 对象数字视频回放状态称为回放完成状态。当 MediaPlayer 对象达到此状态时，表示视频资产已停止播放，并已到达视频资产内部的 EOF (文件结束)标记。如果您调用了，您将绕过此播放完成状态。setLooping(true) 方法调用和 looping on (true)布尔标志。这将从 MediaPlayer 对象中调用。在这个用例中，您的数字视频将继续无缝循环，直到您特别调用。stop() 方法停止它。注也有。start() 和。reset() 方法调用可用于 MediaPlayer 对象，它将根据 Java 8 程序逻辑的需要，随时启动和重置 MediaPlayer 对象。如果你叫。start() 方法，您的数字视频资产将进入启动状态；反之，如果你调用。stop() 方法，您的数字视频播放将被停止。方法将对您的视频资源做它们的方法名称所暗示的事情。
7.  7 .。最后，还有就是。release() 方法调用，为 MediaPlayer 对象调用一个结束的状态。这将终止您的 MediaPlayer 对象，这意味着 Android 操作系统会将其从 Android 设备的系统内存中完全删除，从而为用户在他们的 Android 设备上运行其他应用程序留出空间。

As you will see in the MediaPlayer sections of this chapter, there are other nested classes that will also allow you to do things such as listen for errors, such as MediaPlayer.OnErrorListener, as well as for other states of the MediaPlayer, such as when it reaches the Playback Completed state (MediaPlayer.OnCompletionListener).First, you will need to create your <VideoView> widget inside of your parent <FrameLayout> as that is the next logical step in implementing video inside of your DigitalVideoMedia application. We will do this in the next section, and then we can get into some foundational information regarding digital video assets and formats, and show you how exactly you would create digital video content using open source software packages, and how to optimize it for Android devices using something like DaVinci Resolve, EditShare Lightworks or Sorenson Squeeze Desktop Pro.Once we get through all of that foundational digital video information, we can review the MediaPlayer class and all of its related classes, and then get into some Java coding to implement digital video setup and playback!

### 使用您的 XML 创建视频视图布局设计

Let’s change the DigitalVideoMedia bootstrap FullscreenActivity application, which Android Studio 2.3 generated for us, and which works well, as we saw in Figure [11-5](#Fig5), into a digital video playback application. The first thing that we need to do is to change the placeholder <TextView> tag into a <VideoView> tag, as is seen in Figure [11-6](#Fig6), and remove any related parameters that do not apply (android:text parameters) or which will be covered up by the fullscreen video (android:background). Android will still put a default background color in place, and this will not be seen, but a custom background color is no longer needed, and we are optimizing system memory here by removing objects and attributes that would have taken memory locations to store and process later on.![A324674_4_En_11_Fig6_HTML.jpg](A324674_4_En_11_Fig6_HTML.jpg)Figure 11-6.Change the opening <TextView child tag into a <VideoView child tag and delete unused parametersI optimized the remaining code for screen space, as you’ll see in Figure [11-7](#Fig7), and optimized the strings.xml file.![A324674_4_En_11_Fig7_HTML.jpg](A324674_4_En_11_Fig7_HTML.jpg)Figure 11-7.Condense the XML; change the <Button> android:text parameter to reference @string/button_labelI removed the text constant for the dummy text, and changed the dummy_button text to “Swap Video” and then I changed the name of that <string> attribute to be button_label, as you can see referenced in Figure [11-7](#Fig7) in the lower-right corner, highlighted in your <Button> child tag, as well as in the strings.xml file seen in Figure [11-8](#Fig8).![A324674_4_En_11_Fig8_HTML.jpg](A324674_4_En_11_Fig8_HTML.jpg)Figure 11-8.Edit the strings.xml file to remove the dummy text, and create a button_label named “Swap Video”Note that Android Studio uses different colors (blue, green, yellow, and orange in this case) to highlight the nesting hierarchy, and shows the nesting hierarchy at the top of the XML markup editing pane as well using these same color values. The reason I did not change the ID parameter of dummy_button is because that is referenced in the Java code as an R.id.dummy_button. We will update that later on, when we change our Java code into what we want this application to do for us. The reason I left the <LinearLayout> background constant (black_overlay) is because it has an alpha value, as you can see in Figure [11-5](#Fig5). Let’s take a break from Android Studio 2.3, and get up to speed on concepts in digital video media.

## 数字视频概念:比特率和编解码器

Like the 2D animation we learned about in the previous chapter, digital video extends digital imaging into 4D, the fourth dimension of time, by using something called frames in the digital video (and film) industry. Video is therefore comprised of an ordered sequence of frames that are displayed rapidly over time. The difference from animation, at least in real-world use, inside of Android, is that digital video usually has a fairly massive number of frames (up to 30 for every second of video playback), which require a different asset optimization approach.The optimization concept using frames in a digital video is very similar to the one regarding pixels in an image (the resolution of the digital image), because video frames increase data footprint with each frame used, as will the number of pixels in each frame. In digital video, not only does the frame’s (image) resolution greatly impact file size, but so does the number of frames that the codec has to look at to encode. This is commonly referred to as FPS or the “frame rate.” Standard industry video uses 30 frames per second, but you can use less if you want!Since digital video is made up of a collection of thousands of digital image frames, the concept of digital video frame rate, expressed as frames per second, or more commonly referred to as FPS, is also very important when it comes to digital video data footprint optimization. This is because with video optimization, lowering a frames per second value the codec looks at encoding will lower the total amount of data encoded, which lowers file size.In Chapter [8](08.html), we learned that if we multiply the number of pixels in the image by its number of color channels, we’ll get the raw data footprint for the image. With digital video, we will now multiply that number again using the number of frames per second at which our digital video is set to play back, and again by the number of total seconds that represent the duration of a video “clip” being encoded into a digital video asset file. You can see why having a digital video codec that can compress this raw data footprint down is extremely important.You’ll be amazed (later on in this chapter) at some of the digital video data compression ratios that the MPEG-4 video file format can achieve, once you understand exactly how to optimize the digital video compression work process by using the correct bitrate, frame rate, and frame resolution for your digital video content. We will also get into the concept of bitrates, as well as video optimization, during the next few sections of the chapter. In this next section, let’s review the different open source digital video codecs that the Android 7.1.1 OS currently supports. These can also be used in Java9, JavaFX, HTML5 or other platforms, so this will be of significant interest for the developers out there that want to use their digital video assets across all of those popular open source platforms.

### Android 中的数字视频:MPEG4 H.264 和 WebM

Android supports both MPEG-4 H.264 (MPEG stands for: Motion Picture Experts Group) as well as the ON2 VP8 or VP9 formats, which were acquired by Google from ON2 Technologies, and distributed under a WebM moniker, and then released into the open source environment. These open source formats are quite optimal from a content production standpoint, as video content that a developer produces and optimizes could then be used both in HTML5 engines, such as HTML5 apps, browsers, and devices, as well as in JavaFX and in Android OS. This open source digital video format cross-platform support thus affords us content developers with a “produce once, deliver everywhere” production scenario. This will reduce content development cost, thus increasing your revenues, as long as this “economy of scale in content development” is taken advantage of by app developers.Since Android devices these days have displays that are using a medium (1280x720) to high (HD 1920x1080) resolution, or even UHD 3840x2160 resolution, if you are going to use MPEG-4 file format, you should utilize the MPEG4 H.264 AVC format, which is currently the digital video format most often used in the world today, for Android and HTML5 apps, as well as on the Internet and Business and Government formats, such as PDF. This MPEG-4 H.264 AVC (Advanced Video Coding) digital video file format is supported across all Android OS versions for video playback, and under Android 3.0 (and later versions) for video recording. It is important to note that recording video is only supported if the Android device hardware has video camera capabilities.If you’re a video content producer, you will find that the MPEG4 H.264 format has the best compression result, especially if you’re using one of the more advanced encoding suites like the Sorenson Squeeze Desktop Pro 11 software, which we will be using to optimize our 3D planet-fly-over video asset later on during this chapter.File extension support for MPEG-4 video files includes a .3GP (MPEG-4 SP which stands for “Standard Play”) and a .MP4 (MPEG-4 H.264 AVC). I suggested using the latter (.MP4 AVC), as that is what I use for HTML5 apps, and MP4 is more common to stream in AVC format. Either type of file should work just fine in Android apps, depending on what Android OS versions (1.5, 2.3, 3, 4.4, 5, 6, 7) you are targeting delivery of your app to.A more recent digital video format that Android supports is called the WebM (VP8, VP9) digital video format . The format provides great quality results with a small data footprint. This is a reason why Google acquired ON2, the company that developed the VP codec. VP is used in OGG Theora (VP3), JavaFX 8 (VP6), and Android 7.1.1 (VP8 and VP9). We’ll learn about codecs later on in this chapter. WebM video playback was first natively supported in Android 2.3\. The term native support is used with code (in this case, it is a codec) that has become natively a part of the operating system software, which means it is included with the rest of the operating system and API.WebM also supports something called video streaming , which you will also be learning about in a later section of the chapter. WebM video streaming playback capability is supported only if your users have Android version 4.0 and later. For this reason, I would recommend using WebM only for captive video assets, as Android 2.3 through 4.4 supports non-streaming WebM codec use. If you’re only delivering to later version Android devices you can use WebM, to deliver across all Android devices and versions, use MPEG-4\. In case you’re wondering, captive video is video that is not streamed, meaning video assets are captive inside of the /res/raw folder. Use an MPEG4 H.264 AVC if you are only going to be streaming video, as all of the Android versions, including Android 7.x, support that codec, both for captive video playback, as well as for streaming video playback.

### 数字视频压缩:比特率和流

Let’s start out covering the primary resolutions used in commercial video. Before HDTV, or High Definition TV, came along, video was usually called SD, or Standard Definition, and used a standard vertical resolution of 480 pixels. High Definition (called HD) video comes in two resolutions, 1280x720, which I call “Pseudo HD” and a higher resolution 1920x1080, which the industry calls “True HD.” There’s also a new “Ultra HD” (UHD) resolution, which is 3840x2160\. All use a 16:9 widescreen aspect ratio, and are used not only in film, television, and iTV sets, but also in smartphones (Razor HD was 1280 by 720) and tablets (Kindle Fire HD is 1920x1200).This 1920x1200 resolution is, by the way, a less wide, or taller, 16:10 pixel aspect ratio, and is becoming more common as a widescreen device aspect ratio, as is a 16:8 (or 2:1) aspect ratio, with 2160x1080 screens out now. There is even a 2560x1440 resolution screen on the Samsung Galaxy S5 smartphone. Why this resolution, you may be wondering? Power of Two (even) up sampling of the most common 1280x720 digital video content will provide the best viewing results. Multiply 1280 by 2 and 720 by 2 and see what resulting screen resolution you come up with.There is also 16:10 Pseudo HD resolution, which features 1280 by 800 pixels. In fact, this is a common laptop, netbook, and mini-tablet resolution. I would not at all be surprised to see the 16:8 1280 by 640 screen offered at some point in time as well. Generally, most content developers try to match their video content resolution to the resolution (and thus, the aspect ratio as well) of each Android device upon which the video asset will be viewed.Regardless of the resolution you use for the digital video content, your application can access videos in a couple different ways. The way I do it, because I’m a data optimization guru, is captive to the application. This means the data is inside of (captive to) the Android application APK file itself, inside the /res/raw data resource folder.The other way to access video inside your Android app is by using a remote video data server. In this case, the video is streamed from this remote server, over the Internet, and into your user’s Android device as the video is playing back, in “real time.” Let’s hope that your video server does not crash, lose power, get hacked, or get too many playback requests (that is, lots of data traffic), as these scenarios are some of the downsides of streaming.Video streaming is inherently more complicated than simply playing back captive video data. This is because an Android device is communicating in real time with a remote data server, receiving video data packets, decoding the data packets as the video plays, and writing the frames to the Android hardware display. Video streaming is supported via WebM format on Android 4 and later devices, or using MPEG4 across all Android OS versions.The last concept that we need to cover in this section is the concept of bitrate. Bitrate is a key setting used in the video compression process, as you will see when we utilize Sorenson Squeeze Pro, later on in the chapter. Bitrates represent the target bandwidth, or data pipe size, which is able to accommodate a certain number of data bits streaming through it every second. Bitrates must also take into consideration CPU processing power for any given Android phone, making video data optimization even more important to video playback quality.This is because once bits travel through the data pipe, they also need to be processed and displayed on the device screen. In fact, captive video assets that are included in Android application APK files only need optimization for processing power. This is because if you’re using captive video files, there is no data pipe for the video asset to travel through, and no data transfer overhead. Therefore, bitrates for digital video assets need to be optimized not only for bandwidth, but also in anticipation of variances in CPU capability. We’ll look at data footprint optimization next.In general, the smaller the video data file size you are able to achieve, the faster the data will travel through any data pipe, the easier it will be to decode the data using the codec and the CPU, and the smaller the APK file size will be, for obvious reasons. Single-core CPUs in devices such as smartwatches may not be able to decode high-resolution, high bitrate digital video assets without “dropping” frames. This is a playback quality issue, so make sure to thoroughly optimize lower bitrate video assets if you plan to target older (or cheap) devices, so they have fewer bits (smaller file sizes) to process, which uses less memory overhead and therefore fewer CPU cycles.

### 数字视频优化:编解码器和设置

Digital Video is compressed using a software utility called a codec, which stands for COde-DECode. There are two opposing sides to each video codec; one will encode the video data (for captive or streaming), and the other will decode this video data (captive video or streamed video). A video decoder will be part of a platform (Java 9, JavaFX 8, Android 7.x, or HTML5), or an HTML5 browser, across all operating systems. The decoder side of the codec will always be optimized for speed, as smoothness of video playback is a key issue, and the encoder side will be optimized to reduce data footprint for the digital video asset that it is generating, which can also boost playback speed. For this reason, an encoding process may take a long time, depending on how many cores your workstation contains. Most digital video content production workstations should support 6, 8, 12, 16 or 20 processor cores (threads).Codecs (on the encoder side) are like plug-ins, in the sense that they can be installed into different digital video editing software packages, in order to enable them to encode different digital video asset file formats. Since the Android OS supports H.263, H.264 or H.265 MPEG-4 formats, and ON2 VP8/VP9 WebM formats for video, you need to make sure that you are using one of the codecs that encodes video data into these digital video file formats.More than one software manufacturer makes MPEG encoding software, so there will be different MPEG codecs (encoder software) that will yield different (better or worse) results, as far as encoding speed and file size goes. The professional solution I recommend you secure if you wish to produce professional video is called Sorenson Squeeze , which is currently at Sorenson Squeeze Desktop Pro version 11\. Squeeze has a professional-level version, which I will be using in this book, which costs less than $750, and whose value is significantly in excess of that suggested list price amount.There is also an open source solution called EditShare LightWorks 12. 6 that is scheduled to natively support output using MPEG-4 and WebM VP9 codecs sometime in 2018\. So for now, I will have to use Squeeze Pro 11 for this book, until the codec support for Android 7.1.1, Java 9 and HTML5 is added to EditShare LightWorks 14\. DaVinci Resolve 12.5.4 may also soon add this support as well, as it is now free.When optimizing for digital video data file size using encoder settings, there are a number of important settings that directly affect the data footprint. I’ll cover these in the order in which they affect file size, from the most impact to the least impact, so you know which parameters to “tweak” or adjust in order to obtain the best result.As in digital image compression, the resolution, or number of pixels, in each frame of video, is the best place to start your data optimizion work process. If you are targeting 1280x720 or 1920x1080 smartphones, tablets, iTV Sets, or auto you don’t need to use 3840x2160 resolution to get great visual results from the digital video assets.With high-density (termed high dot pitch) displays (XHDPI and XXHDPI) currently common in the Android market, you can scale 1280 video up 33% and it will look reasonably good. The exception to this might be iTV apps for GoogleTV, which has a medium (or even low) dot pitch, due to large 50 to 70 inch screen sizes. In this use case, if you are developing applications for iTV sets, you’ll want to use “True HD,” 1920x1080 resolution.The next level of optimization will come in the number of frames used for each second of video, called FPS, assuming the actual seconds contained in the video itself cannot be shortened by editing. This is known as your frame rate, and instead of setting the video standard 30 FPS frame rate, consider using a film standard frame rate of 24 FPS, or the multimedia standard frame rate of 20 FPS.You may even be able to use a low 15 FPS frame rate, depending upon your content. Note that 15 FPS is half as much data as 30 FPS, a 100% reduction in data going to the encoder. For some video content this may playback (look) the same as 30 FPS content. The only reliable way to test how low you can get the frame rate before you start to affect video playback quality is to set, encode, and review with these standard video framerate settings. This would be done during your content optimization (final original or raw video asset encoding) work process.The next most optimal setting to tweak (experiment with settings for) in obtaining a smaller data footprint will be the bitrate that you set for a codec to try and achieve. Bitrate settings equate to an amount of compression applied, and thus sets the visual quality for video data. It is important to note that you could simply use 30 FPS, HD 1920x1080 video and specify a low bitrate ceiling. If you do this, your results might not look as good as if you first experimented with lower frame rates and resolutions while using the higher (quality) bitrate settings. The only way to find out what any give codec (encoder algorithm) will do is to experiment with the settings and look at the resulting filesize, playback speed, and visual quality for the resulting asset using an Android decoder.The next most effective setting in obtaining a small data footprint is the number of keyframes. The codec uses your keyframe settings to know when to sample the digital video. Video codecs apply compression by looking at a frame, and then encoding only the changes, or offsets, over the next few frames, so that it does not have to encode every single frame in your video data stream. This is why a talking-head video will encode better than a video where every pixel moves on every frame, such as video with fast panning or rapid zooming, for instance.The keyframe setting in the encoder will force the codec to take a fresh frame sample of a video data asset every so often. There is usually an auto setting for keyframes; this allows the codec to decide how many keyframes to sample. There is also a manual setting that allows you to specify a keyframe sampling every so often, usually a certain number of times per second, or a certain number of times over the duration of the video (total frames). The more keyframes the codec needs to sample (and store in the file) the larger the resulting file size will be.The next most effective setting in obtaining a small data footprint is the quality or sharpness setting, which is usually implemented using some sort of slider or spinner user interface element. Sharpness controls the amount of blur that a codec will apply to the video pixels before compression. A 100% quality doesn’t apply algorithms (pixel processing on any frames); as you reduce this percentage, the amount of algorithmic processing increases.In case you are wondering how this blur trick works, so that you can apply it yourself in GIMP during your own digital image optimization work process, applying a slight blur to your still image or video, which is usually not desirable, can allow for better lossy compression, such as that found in JPEG and MPEG, or WebP and WebM.The reason for this is that a sharp transition in a raster image, such as the sharp edges between colors, are more difficult for the codec to encode optimally (that is, using less data). More precisely (no pun intended), sharp or abrupt transitions in color will take more data to reproduce than soft transitions will. This does not hold true in vector images (SVG, AI, EPS or PDF) as the edges between strokes and fills are rendered and then anti-aliased.I would recommend keeping the quality or sharpness slider between an 80% and 100% quality setting, and try to get your data footprint reduction using many of the other variables (data footprint optimization approaches), which we have discussed in this section of the chapter.Ultimately, there are a significant number of different variables that you’ll need to fine-tune in order to achieve the best data footprint optimization for each particular video data asset. Each video asset will be different (mathematically) to the codec, as each video asset is a different array or collection of pixel color data.For this reason, there is no “standard” collection of settings you can develop to achieve any given result. Your experience tweaking various settings will eventually allow you to get a better feel, over time, as to the settings you need to change as far as all the parameters go, to get your desired result with different types of uncompressed video source assets.Next, lets create some digital video content using Terragen 4.0, which came out recently, is a professional-level production software package from Planetside Software, which also happens to have a free version and a very affordable professional version as well. I highly recommend this software, which, like Blackmagic Fusion, has a visual programming paradigm that makes advanced production easier and more visual, like Android Studio’s new Visual Design Editor.

## 创建数字视频内容:Terragen4

The next thing that you need to learn is how to create digital video content that you can use to show the various concepts that you just learned about in the previous sections of this chapter. I’m going to use Terragen 4.0 , a world creation 3D animation software package from Planetside Software, because it is not only an impressive 3D software package, but is also a professional-level 3D production software package. Fortunately, there is the free version as well as a paid Pro version, which I suggest that you purchase if you are serious about having all the top production tools in your quiver. Go to the website, at Planetside.co.uk, and download the latest version of Terragen 4\. After you download and install the software, you’ll follow the following steps:

1.  1。使用你的快捷图标启动 Terragen 4，你会看到启动画面，如图 [11-9](#Fig9) 。通过查看这个启动屏幕，您可以确切地看到这个 3D 软件的能力，因为这些云不是拍摄的，而是使用 Terragen 算法创建的！![A324674_4_En_11_Fig9_HTML.jpg](A324674_4_En_11_Fig9_HTML.jpg)图 11-9。使用 Terragen 4 world building 软件创建用于 Android 7.x 的数字视频内容
2.  2。首先，你会打开一个基本的无缝循环摄像机飞越一个基本的世界，你会发现在这本书的资产文件夹名为looping orbit _ v 03 . tgd(tgd 是 TerraGen 数据)。在软件中使用文件➤打开菜单序列将其打开，打开结果如图 [11-10](#Fig10) 所示。使用编辑➤首选项菜单序列，并在文件保存部分选择 BMP 格式和编号动画文件输出路径，在我的例子中是 C:\Terragen4 。![A324674_4_En_11_Fig10_HTML.jpg](A324674_4_En_11_Fig10_HTML.jpg)图 11-10。启动 Terragen3 并使用文件➤打开菜单序列打开 loopingorbit _ v03
3.  3。顶部的添加渲染器选项卡，如图 [11-11](#Fig11) 所示，使用渲染器按钮访问，用红色圈出，设置一个 1080 像素的图像宽度和一个 1920 像素的图像高度。这是你的 Nexus 5 AVD 分辨率。这种真高清分辨率将拥有足够的像素，能够放大(到 UHD)或缩小(到蓝光)并带来良好的视觉效果。将所有其他渲染设置保留为默认设置。如果您只想渲染一帧，您可以使用位于该对话框中间的渲染图像按钮，但这不会创建一个帧序列，您将需要该序列来创建运动视频数据。渲染所有到磁盘也不会创建一系列编号的文件，尽管看起来会。在该选项卡的底部，您会看到七个选项卡，控制高级设置。![A324674_4_En_11_Fig11_HTML.jpg](A324674_4_En_11_Fig11_HTML.jpg)图 11-11。使用项目设置和渲染器按钮打开选项对话框，然后渲染序列按钮
4.  4。点击第七个(最右边)标签为序列/输出的标签，设置输出文件规格以及图像序列设置。在输出图像文件名字段输入您的项目文件目录，如图 [11-11](#Fig11) 所示。我的是 C:\Terragen4 如你所见。
5.  5。确保您的序列第一个字段被设置为数据值 1 ，并在序列最后一个数据字段中设置 400 的值。将序列步长设置为 1 帧以使摄像机移动更加平稳。
6.  6。设置好渲染的所有参数后，点击序列/输出选项卡底部的渲染序列按钮。这将指示 Terragen 为您生成 400 帧自定义数字视频飞越。因为 Terragen 输出带编号的文件，而不是。AVI 格式，大多数 NLE 软件需要，除非你是使用挤压 11，这将读取编号文件，你将需要使用一个软件工具称为虚拟杜布创建一个 AVI 文件。

Figure [11-12](#Fig12) shows one of the frames of the rendering sequence, which took 4 days on my 64-bit OctaCore PC.![A324674_4_En_11_Fig12_HTML.jpg](A324674_4_En_11_Fig12_HTML.jpg)Figure 11-12.Make sure render output window shows Frame number and total render timeNext, let’s take a look at how to turn these rendered frames into a digital video asset, using Sorenson Squeeze Desktop Pro 11, and take a look at some of the settings we discussed in the data footprint optimization section.

### 数字视频压缩:索伦森挤压 11

Next, we’re going to use Sorenson Squeeze Desktop 11 to compress the digital video asset. One of the reasons I utilized Terragen 4 was to create completely uncompressed source video with zero compression artifacts, so that you can see what a codec can do with clean data, as video camera data tends to be somewhat chaotic from the codec’s perspective of pixel data values. 3D software tends to output adjacent pixel values that relate in some way or another to each other, whereas a camera CCD will simply output what it sees in real life, which tends to be more chaotic, unless you are filming the night sky (or a clear blue sky), in which pixels will closely relate to each other in both location and color. Also, many video cameras pre-compress data using Motion JPEG or even MPEG to fit more minutes on a cartridge, so if you want less chaos (less compression artifacts) using a camera, you should use Firewire, capturing full frame uncompressed (raw) video data to your hard disk drive, instead of going through the on-camera MPEG or Motion JPEG codec and onto a captive digital video cartridge inside of the camera.The other reason I am using Sorenson Squeeze Desktop is because it is the most professional video compression solution, has affordable $199 (Lite), $549 (Standard), and $749 (Professional) price points, and has support for every platform (as you’ll see in Figure [11-14](#Fig14)) that you will ever want to optimize digital video for. There is also a 30-day trial version that you can use to follow along with this work process on the SorensonMedia website, and I recommend purchasing one of the non-expiring versions for your digital media production workstation, as this is currently the industry standard for digital media data footprint optimization.Let’s take a look at the work process for optimizing a Terragen 4 flythrough, using Sorenson Squeeze Desktop.

1.  1。安装索伦森挤压桌面 Pro 11，并启动它。使用文件➤导入图 [11-13](#Fig13) 中编号为 1 的图像序列，打开选择图像序列打开对话框。选择显示为数字 2 的 Terragen4 文件夹(或存储文件的任何地方),并选择显示为数字 3 的导入图像序列中的第一个文件。最后点击打开按钮，如数字 4 所示，设置 25 FPS 帧率，如数字 5 所示。我使用 25 FPS，因为它是 400 的偶数倍，给我们一个完整的 16 秒循环视频。要调用导入到 Sorenson Squeeze Desktop 11，点击 OK 按钮，如图6 号[11-13](#Fig13)T24】。![A324674_4_En_11_Fig13_HTML.jpg](A324674_4_En_11_Fig13_HTML.jpg)图 11-13。导入一系列图像帧以用作数字视频剪辑的六步工作流程
2.  2。如果你使用的是未压缩的 AVI(微软)或 MOV(苹果)格式的数字视频文件，点击左上角的导入文件图标，如图 [11-14](#Fig14) 。请注意，Squeeze 软件保留了用于保存编解码器格式、收藏夹设置和工作流程选项的已安装面板，以及顶部预览区域和底部时间线区域，我们将使用这些区域将编解码器预设应用到图像序列，在浅蓝色中显示为选中状态。这是通过将编解码器拖放到视频源上来实现的。这表明您希望将该编解码器应用到源视频(在本例中为图像序列)。![A324674_4_En_11_Fig14_HTML.jpg](A324674_4_En_11_Fig14_HTML.jpg)图 11-14。【挤压桌面】11 用户界面面板(左)、源预览(上)、时间轴(下)
3.  3。接下来，让我们看看如何编辑现有的编解码器，了解它们的默认设置，以及如何创建(重命名和添加)为您的项目定制的新编解码器。打开挤压预设选项卡中的 MPEG-4 (mp4) 编解码器部分，如蓝色选中所示，在图左侧 [11-14](#Fig14) 。找到 HEVC_3300Kbps_1080p 预置，在图 [11-15](#Fig15) 中以蓝色显示选中，点击右键，从弹出的上下文菜单中选择编辑选项。这将为该编解码器设置集合打开一个预置对话框，如图 [11-15](#Fig15) 中间所示。MPEG-4 H.265(技术上是 MPEG-H 编解码器)是一种高效的视频编解码器(因此是 HEVC)，它最近被添加到 64 位 Android 操作系统中，所以我在这里先看一下。它使用 x265 编解码器，如图 [11-15](#Fig15) 所示，一种单程、可变比特率(VBR)编码方法(算法)，目标 3Mbps ( 3068 Kbps)质量等级，选择场景变化时自动关键帧，允许编解码器插入关键帧(算法上)。这些是您将调整以获得最佳数据占用空间的设置。![A324674_4_En_11_Fig15_HTML.jpg](A324674_4_En_11_Fig15_HTML.jpg)图 11-15。右击 HEVC 1080 编解码器，使用编辑打开编解码器预置对话框进行编解码器设置
4.  4。点击确定关闭 HEVC 编解码器，因为我们将使用一个更广泛支持的 AVC，或高级视频编解码器，用于本练习。这是为了让我们可以跨越早期版本的 Android 操作系统，这些操作系统不包含 HEVC 编解码器，但在 Android 设备中占很大比例。如果您正在制作 Android TV 应用程序，您会希望使用 HEVC 编解码器来获得更高质量的高清/UHD 效果，因为 Android TV 最近才发布。
5.  5。让我们创建一个Android _ 1080 pMPEG-4h . 264 AVC 预设，方法是选择一个 Apple_TV 预设，并重新配置它以供我们在 Android 中使用(版本 1.6 到 7.x 支持解码 AVC)。在图 [11-16](#Fig16) 中看到的 Apple_TV_1080p 编解码器上点击右键，使用 Edit 命令在预置编辑对话框中打开。选择 MainConcepts 编解码器，因为我从我以前的 Android 书籍中知道它工作得很好，或者选择 Sorenson 编解码器，这是该软件的新功能。事实上，您可以使用相同的设置来尝试这两种方法，看看哪种方法可以提供更好的数据占用空间，从而进行一些使用该软件包的练习！我将两遍 VBR(较慢但更好的压缩)设置为方法，将 2Mbps (2000 Kbps)设置为数据速率目标，并使用约束最大数据速率来确保编解码器不会超过这个值。我还将预置名称重命名为 Android_1080p ，Android OS 的预置 Desc 重命名为支持 1080p 分辨率，如图 [11-16](#Fig16) 所示对话框顶部。有趣的是，苹果并不把他们的 Apple_TV 产品称为 iTV(像 iPad、iPhone 一样)，因为 iTV 是互动电视(或者智能电视，或者互联网电视，如果你喜欢的话)的“公共领域”术语。智能电视是三星的商标。联网电视也是私有商标。![A324674_4_En_11_Fig16_HTML.jpg](A324674_4_En_11_Fig16_HTML.jpg)图 11-16。创建一个 Android_1080p 编解码器，通过编辑一个 AppleTV 编解码器并选择 MainConcept 的编解码器
6.  6。要创建压缩视频文件，点击预设对话框中的确定按钮创建 Android_1080p 预设，并将其拖出格式(预设)选项卡，放在源(编号图像文件)上。要开始压缩过程，单击挤压它！挤压桌面右下角的按钮。如果您的 HEVC 1080 编解码器仍然连接到视频源，请在时间轴视图中右键单击该条目，并在应用您刚刚创建的 Android_1080p 编解码器预设之前，使用上下文菜单将其删除。

As you will see the HEVC codec gives you a 6.75MB file and the AVC codec gives you a 4MB file, because of the high 2MB compression setting. The HEVC file will have much better quality and could be used for Android TV, whereas we’ll use the AVC file across Android devices as well as across all Android OS revisions as well. Now we are ready to create the /res/raw folder to hold the video asset and continue with our Java programming.

### 创建数字视频文件夹:原始资源

Go back into Android Studio, and right-click on your DigitalVideoMedia project’s /app/res folder, and select the New ➤ Android resource directory menu sequence, shown on the left side of Figure [11-17](#Fig17) to create a resource subfolder for already optimized digital video (as well as digital audio) assets. Android will not attempt to double compress (which would reduce the visual quality) and assets in this folder, which is what this /res/raw folder is used for. I use this folder quite a bit, as it allows me to control optimization for files contained in the .APK file.![A324674_4_En_11_Fig17_HTML.jpg](A324674_4_En_11_Fig17_HTML.jpg)Figure 11-17.Right-click on the project /res folder and use the New ➤ Android resource directory menu sequenceIn the New Resource Directory dialog, in your Resource type drop-down selector, select the raw option, as is seen in Figure [11-18](#Fig18) in blue, and make sure the Directory name is raw, and then click the OK button to create the app/res/raw/ folder. Now all you have to do is install the digital video asset in this folder using your explorer file management utility, and we’ll be able to move on to finish all the Java 8 code to implement the digital video.![A324674_4_En_11_Fig18_HTML.jpg](A324674_4_En_11_Fig18_HTML.jpg)Figure 11-18.Select a raw resource type in the second drop-down selector in a New Resources Directory dialogOpen your OS file management utility, for Windows 10 this is the Windows Explorer, and find the digital video asset, in my case this was C:\Terragen4\1080temp.001_Android_1080p.m4v, and right-click on it, and select the Copy option. Then find your AndroidStudioProjects\DigitalVideoMedia folder, in my case, this was located at C:\Users\user\AndroidStudioProjects\DigitalVideoMedia\app\scrc\main\res\raw, as shown at the top of Figure [11-19](#Fig19), and right-click on the folder (or in the empty area to the right), and select the Paste option.![A324674_4_En_11_Fig19_HTML.jpg](A324674_4_En_11_Fig19_HTML.jpg)Figure 11-19.Place the MPEG-4 file you created in the project /res/raw folder, and rename it to be flythru3d.mp4Since Android OS will look for an .MP4 file, you will also need to rename your file generated by Squeeze 11 to be flythru3d.mp4. Right-click on your file and select the Rename menu item from the context sensitive menu, and replace the Squeeze generated .m4v filename with a custom flythru3d.mp4 file name you’ll use in Android.Before we get back into Java coding, let’s take an in-depth look at how digital video assets are referenced using a URI in Android OS, as well as how that Uri object is parsed, and passed into your VideoView’s MediaPlayer.

## Uri 类:引用视频数据

URI is an abbreviation for Uniform Resource Identifier . Uniform because it is standardized, Resource because it references a data path to some data (content) that applications will operate on and utilize. It is an Identifier because it identifies where to go and load the data, which is also known as the content’s data path.The Android Uri class only capitalizes the U, the industry term capitalizes all three (URI) letters in the term. A URI has four parts. The first is a URI schema such as HTTP://. Next comes an authority, like apress.com. Next comes the data path, such as /data/video. Finally comes a data object itself in its file format such as asset.mp4.A Uri object in Android contains a reference to a data path that will be used to access raw or specialized data, of one type or another. One example of data would be a SQLite database, or in this case, the digital video asset. Other examples might include your website’s URL, or similar types of content which the application might use.Android’s Uri class is a direct subclass of the java.lang.Object class, and therefore, was created specifically for holding URI references. Just so that you don’t get confused when you look at current Android 7.1.1 developer’s site documentation, note that the java.net.Uri class exists alongside the android.net.Uri class. However, I suggest that you use the Android-specific version of the Uri class, since it is optimized for use within the Android 7.1.1 OS. The Uri class is a public abstract class, and has over three dozen methods that allow developers to work with Uri objects (and data path references).Since this is an Absolute Beginner’s book, we will not be getting into this Uri class at a great level of depth, but you’re welcome to research it yourself, on the Android developer website. The Android Uri class hierarchy is structured as follows:java.lang.Object> android.net.UriThe Android Uri class is kept in the android.net package, making it a tool for accessing data across a network. For this reason, the import statement for using the Uri class inside of your Android application would reference a package path of android.net.Uri, as you will see in the next section.The Android Uri class allows developers to create Uri objects which provide what is termed an immutable URI reference. Immutable objects and variables cannot be changed (think “mutate,” or something that has been changed, usually with undesirable results), and you certainly do not want something critical like your URI reference to change. In Android, you make objects immutable by placing them into system memory for use, and you’ll need to do this for your URI data path reference, by using Android’s Uri class, and its Uri.parse() method.Your Uri object reference includes a URI specifier, as well as a data path reference, which is the component of the URI that follows the ’://’. The Uri class will take care of the process of building and parsing the Uri object, which will then reference data in a manner that will conform to the popular RFC 2396 technical specification.To optimize the Android operating system and application performance, a Uri class performs a minimal amount of data path validation. What this means is that Uri methods aren’t specifically defined for handling invalid data input, so you will need to define your own data validation. This means the Uri class is very forgiving in the face of an invalid input specification. It also means that if data is invalid, your user may not get the result you desire!This means that as a developer you have to be very careful about what you are doing, as Uri objects could return garbage data rather than throw an exception, unless you specify otherwise in the Java code. Thus, error trapping and data path validation are left up to the developer to create, inside their code. This is why URI is an advanced area, which we are only covering at an introductory level, so that you will be able to load the digital video data.Next, we will create your Uri object as well as creating your VideoView object, so we will be able to access our digital video asset. Let’s do that next, and after that we can get into the MediaPlayer class and its related classes.

### Uri.parse()方法:加载视频视图

Next, let’s declare the Uri and VideoView objects at the top of the FullscreenActivity class, so that we can later instantiate them inside of the onCreate() method to set up the MediaPlayer engine, so we can play digital video.

1.  1。声明一个 VideoView 对象在 FullscreenActivity 类的顶部将其命名为video holder如图 [11-20](#Fig20) 所示。要让 Android Studio 为您创建导入语句，请使用 Alt+Enter 快捷键来指示 Android Studio 编写代码。您将在 FullscreenActivity 类顶部编写的两条 Java 语句应该如下所示:![A324674_4_En_11_Fig20_HTML.jpg](A324674_4_En_11_Fig20_HTML.jpg)图 11-20。在类的顶部声明一个名为 videoHolder 的 VideoView 对象，用 Alt+Enter 导入public 类 FullscreenActivity 扩展 app compat activity {video viewvideo holder；UrivideoAssetUri；//你班上的其他人都在这里}
2.  2。接下来，在 videoHolder 对象下面声明你的 Uri 对象，并将其命名为 videoAssetUri 。使用一个video holder =(video view)findViewById(r . id . full screen _ content)Java 语句在 onCreate() 内实例化一个 VideoView ，如图 [11-21](#Fig21) 中突出显示的。![A324674_4_En_11_Fig21_HTML.jpg](A324674_4_En_11_Fig21_HTML.jpg)图 11-21在类的顶部声明一个名为 videoHolder 的 VideoView 对象，并使用 Alt+Enter 导入

Once you’ve instantiated the VideoView, you will then instantiate the Uri object by setting it equal to the result of a Uri.parse() method call. This is done by using the following Java statement, shown being created in Figure [11-22](#Fig22), using the Android Studio helper drop-down selector that comes up once a videoAsset.Uri = Uri. (portion) of the Java statement has been typed into the onCreate() method, after the VideoView instantiation:![A324674_4_En_11_Fig22_HTML.jpg](A324674_4_En_11_Fig22_HTML.jpg)Figure 11-22.Use a New ➤ Folder menu sequence to create a /res/raw folder, then copy the fly-over asset into itvideoAssetUri = Uri.parse( "android.resource://" + getPackageName() + "/" + R.raw.flythru3d );This Java statement sets the videoAssetUri (Uri) object to the result of the Uri.parse() method call. Inside this method call is a concatenation operation that uses a + operator, which is used in Java to concatenate things together. What this concatenation inside of the Uri.parse() method parameter area does is create the URI path to the video data asset by concatenating the android.resource:// with the package name and then the resource path. The resulting URI, shown below as equated by this concatenation, provides Android OS with the full path that starts with the Android resource area down to the package name, and finally down to your digital video asset:android.resource://com.example.user.digitalvideomedia/R.raw.flythru3d.mp4Figure [11-23](#Fig23) shows the latter part of Android Studio creating this Java code statement for you, which it will do regarding the Uri.parse(), getPackage() and R.raw.flythru3d portions of this Java Uri object instantiation.![A324674_4_En_11_Fig23_HTML.jpg](A324674_4_En_11_Fig23_HTML.jpg)Figure 11-23.Load the videoAssetUri object using the Uri.parse( ) method, and the data path to /res/raw/flythru3dNow that your videoAssetUri Uri object is loaded with the correct URI data path reference, you can “wire” this videoAssetUri (Uri) object to the videoHolder (VideoView) object using the .setVideoURI() method call, using the compact line of Java code shown in Figure [11-24](#Fig24), which looks like the following Java statement:![A324674_4_En_11_Fig24_HTML.jpg](A324674_4_En_11_Fig24_HTML.jpg)Figure 11-24.Configure videoHolder VideoView object to reference the videoAssetUri object with .setVideoURI( )videoHolder.setVideoURI(videoAssetUri);This Java statement uses the setVideoURI called off of the videoHolder VideoView to install the videoAssetUri object containing the URI reference to the digital video asset that needs to be played.The only line of Java code that we have not yet put into place is the call to the .start() method, which we will make off of your videoHolder VideoView object. Since you have loaded with a URI for /res/raw/flythru3d.mp4, you now have a digital video asset that can be played, and so you can call the .start() method. Make sure that you call the .start() method after you have wired all of the other components and new media assets together!A call to this .start() method off of the videoPlayer VideoView object is done using the following simple line of Java code (more of a Java statement, actually) which is seen in Figure [11-25](#Fig25):![A324674_4_En_11_Fig25_HTML.jpg](A324674_4_En_11_Fig25_HTML.jpg)Figure 11-25.Add the videoHolder.start( ) method call to finish implementation of a VideoView and start playbackvideoHolder.start();As you can see in Figure [11-25](#Fig25), you have implemented digital video playback in your Java code in a half-dozen lines of Java code, not including two new import statements, which brings the total new lines of code to eight.If you want to use Run ➤ Run ‘app’ to run the application, you can if you wish, and as seen in Figure [11-30](#Fig30), the digital video asset does indeed play fullscreen, which is our objective in this chapter, to do a fullscreen video playback application!As you can see in Figure [11-30](#Fig30), we will also be adding a media control transport using the MediaController class (and object) after we learn more about the MediaPlayer class and the MediaController class.Let’s take a look at two of Android’s media playback related classes before we continue with more Java coding.

## Android media player:video layback 引擎

The MediaPlayer class is a direct subclass of the java.lang.Object master class. As you know, this indicates that this Android MediaPlayer class was designed specifically for the purpose of providing MediaPlayer objects. A MediaPlayer object is a part of your VideoView widget, and you will learn how to make the MediaPlayer object visible inside your Java code in a future section of this chapter. The MediaPlayer class hierarchy looks like this:java.lang.Object> android.media.MediaPlayerThe MediaPlayer class belongs to the android.media package. The import statement for using the MediaPlayer class in an app would reference the android.media.MediaPlayer. This MediaPlayer class is a public class, and features nine nested classes. Eight of the nested classes offer callbacks for determining information regarding operation of MediaPlayer’s video playback engine. We’ll be using one of these in a future section of the chapter.The ninth nested class, the MediaPlayer.TrackInfo nested class, is utilized to return video, audio, or subtitle track metadata information. The MediaPlayer nested class callback that we’ll be implementing later on in the chapter is the MediaPlayer.OnPreparedListener, which allows us to configure a MediaPlayer object before the digital video asset playback starts for the first time.Other often-used callbacks include your MediaPlayer.OnErrorListener, which responds to (that is, handles) error messages relating to a digital video asset or a network connection, MediaPlayer.OnCompletionListener, which you can use to trigger Java programming structures once your video asset playback cycle has completed, MediaPlayer.OnSeekCompletedListener, which is called when a digital video seek operation has completed, and a MediaPlayer.OnBufferingUpdateListener, which is called in order to obtain data buffering status for a video asset that is being streamed over a network.There are also a couple of less-often-utilized nested classes, such as the MediaPlayer.OnTimedTextListener , used when video timed text becomes available for display, and the MediaPlayer.OnInfoListener, used when information or warnings regarding video media being used become available for display. These nested class callbacks are not used that often, at least not to my knowledge, but they are available to you if you need them for specialized digital video implementation scenarios within your Android applications.

## android meddiscotroller:视频传输

The MediaController class is a direct subclass of the android.widget.FrameLayout class. As you know, this is a ViewGroup layout container class, and a FrameLayout is a static layout, in this case used to create a fixed video transport UI element. A MediaTansport object is used to provide a video transport control set, which is a collection of Play, Pause, Stop, and Rewind buttons, along with a Shuttle slider and current time readouts. You will learn how to “wire” (connect or reference) a MediaController object up to your VideoView object in the next section of this chapter. The MediaController class hierarchy should look like the following Java class hierarchy:java.lang.Object> android.view.View> android.view.ViewGroup> android.widget.FrameLayout> android.widget.MediaControllerAndroid’s MediaController class belongs to your android.widget package. The import statement for using the MediaController class in an app would reference the android.widget.MediaController. The MediaController class is a public class, and features one nested class: MediaController.MediaPlayerControl. This class is the public static interface that contains eleven methods used to control a media player using the MediaController class. These include seekTo(), isPlaying(), getCurrentPosition(), getAudioSessionId(), getDuration(), getBufferPercentage(), canSeekForward(), canSeekBackward(), canPause(), pause() and start().A MediaController at the highest level (other than Object) is a View object containing the transport controls for a MediaPlayer object that it is wired to via the VideoView which accesses that MediaPlayer object. Typically, a video playback transport UI will contain buttons such as play, pause, rewind, fast forward, go to start, and a progress bar and current position. The MediaController class takes care of the synchronizing of the UI controls with the state of the MediaPlayer (object) being used by the VideoView (object) inside your fullscreen layout.The way to use this class, as you will see in the next section of this chapter, is to instantiate it programmatically. The MediaController object will then create a default set of controls using its FrameLayout superclass and place these in a window floating above your application.Specifically, the controls will float above the view that is specified by the developer using the setAnchorView() method call, in most cases this will be the VideoView which is hosting your digital video asset and playing it using the MediaPlayer object that the VideoView utilizes to play back that new media asset.The video transport window will appear when the (in this case, fullscreen) video is touched or clicked, and will disappear if left unused for video control for three seconds. It will then reappear again when the user touches the anchor view; in our case this is a VideoView inside a FullscreenActivity, which means anywhere on the screen.

### 使用 MediaController 添加视频传输 UI

Next, let’s implement your MediaController UI transport controller, the UI buttons used to control your video.

1.  1。一旦你声明一个 Javamedia controller video transport；语句，然后按 Alt+Enter 让 Android Studio 编写您的import Android . widget . media controller语句，您将能够在您的 onCreate() 方法中实例化和构造您的新 MediaController 对象。下一步是将 new 关键字与 MediaController(this) 构造函数方法结合使用，传递的参数包含您的标准 Java Context 对象，在本例中是使用 Java 关键字 this 传递的。这段代码将使用下面的 Java 编程构造来构造一个名为 videoTransport 的 MediaController 对象，如图[11-26](#Fig26):![A324674_4_En_11_Fig26_HTML.jpg](A324674_4_En_11_Fig26_HTML.jpg)图 11-26 所示。声明一个名为 videoTransport 的 MediaController，构造你的新 MediaController()对象video transport = new media controller(this)；

Notice in both Figures [11-26](#Fig26) and [11-27](#Fig27) that I’ve clicked on the videoTransport MediaController object in the Java code, which tells Android Studio to highlight the (purple colored) tracking of the instantiation. I did this in the next screenshot to show both your videoTransport MediaController object declaration and instantiation, as well as the object usage for the videoTransport MediaController object. This is an effective technique to track an object’s use through your Java logic. The next thing you’ll need to do is to wire the VideoView object to the MediaController object, such that they know each other are there, and so they will work together seamlessly.![A324674_4_En_11_Fig27_HTML.jpg](A324674_4_En_11_Fig27_HTML.jpg)Figure 11-27.Wire the videoTransport and videoHolder together with .setAnchorView( ) and .setMediaController( )In pseudo-code speak, we need to tell the MediaController object that it is controlling the VideoView, and tell the VideoView to use the MediaController object to control its new media asset (in this case, digital video). This cross-wiring of the two objects will take two lines of Java code, which, after they are in place, will give your user the ability to click (or touch) the digital video content and bring up the MediaPlayer transport UI element. This MediaController, or the MediaPlayer transport, whichever way you want to look at it, will always work, regardless of whether you have your digital video asset set up to loop, or to only play once.

1.  1。如图 [11-27](#Fig27) 所示，我们将首先使用。setAnchorView() 方法调用，脱离 videoTransport 对象，将 videoTransport 和 videoHolder 对象连接在一起。这告诉 videoTransport 对象: videoHolder VideoView 对象是你的锚视图。
2.  2。那么我们就用一个。setMediaController() 方法调用 videoHolder 对象，将 videoHolder 和 videoTransport 对象连接在一起。这将告诉 videoHolder 对象“使用video transport media controller对象作为名为 videoHolder 的 VideoView 对象的 MediaController 这两行 Java 逻辑应该是这样的:video transport。setAnchorView(video holder)；视频主持人。setMediaController(video transport)；

Use the Run ➤ Run ‘app’ menu sequence and launch your Nexus 5 AVD, as seen in Figure [11-28](#Fig28), and rotate the device 90 degrees by using the rotate icon on the AVD icon control bar, shown on the right, circled in red.![A324674_4_En_11_Fig28_HTML.jpg](A324674_4_En_11_Fig28_HTML.jpg)Figure 11-28.Use Run ➤ Run ‘app’ and test your fullscreen VideoView, MediaPlayer and MediaControllerOnce the digital video starts playing, even if it is set to loop, which I will be showing you how to do in the next section of this chapter, you can click on the screen at any time, and bring up the video media transport controls. After a few seconds, these transport UI controls will fade away, if they are not being actively used to start, stop, pause, rewind (reset), or shuttle the digital video asset’s frames. Also notice in Figure [11-28](#Fig28) that the current time (00:14) and the duration (00:16) are also shown in the transport bar user interface element (MediaController).Congratulations, you’ve essentially mastered the basics of digital video playback for your Android applications development thus far during the chapter, doing everything from learning the fundamentals of digital video asset concepts and creation to optimization and encoding digital video assets, to coding a FullscreenActivity for video playback. Soon we will add looping capabilities and streaming video playback support for your users to use.Pretty comprehensive, for just one single chapter! If you wanted to venture more deeply into this subject, look for my titles Pro Android Graphics (Apress, 2014) and Pro Android UI (Apress, 2014) that delve deeper into this subject area, and combine it with more advanced graphic design and user interface design topics.Now, let’s continue with Java programming, and expose a MediaPlayer object using OnPreparedListener. This will allow you to set your digital video asset to loop continuously. Fortunately you learned about event listeners early on in the book, in Chapter [7](07.html), so that I could cover more advanced callbacks, such as this one, during the second half of the book.

### 循环数字视频:使用 OnPrepareListener

Now we are going to implement one of the most-used nested interfaces from the MediaPlayer class, MediaPlayer.OnPreparedListener, which will allow you to configure the videoHolder VideoView digital video asset to loop seamlessly. You’ll do this by calling a .setOnPreparedListener() method off of your videoHolder object. Inside of that Java programming construct, you will then utilize a new keyword to create an OnPreparedListener implementation. Inside of that construct will live the onPrepared() event listener. This will be accomplished via the following initial Java statement that creates an empty event listening structure:videoHolder.setOnPreparedListener(new MediaPlayer.OnPreparedListener(){empty onPrepared() method });As you can see in Figure [11-29](#Fig29), as you type in the videoHolder.setOnPreparedListener() method call, off of your videoPlayer VideoView object, as you type in the .setOn part of the statement, Android Studio will figure out what you are trying to do, and will pop up the VideoView event listener helper dialog, shown at the bottom-right corner of the partial screenshot.![A324674_4_En_11_Fig29_HTML.jpg](A324674_4_En_11_Fig29_HTML.jpg)Figure 11-29.Type videoPlayer.setOn, and select setOnPreparedListener(OnPreparedListener l) optionIt is important to notice that I am calling the .setOnPreparedListener() method before I call the .start() method, because I don’t want to start the video before I prepare (configure) it for use! Remember, the order of Java programming statements is extremely important, and putting method calls in the wrong order would generate unintended results!Inside videoHolder.setOnPreparedListener, type new MediaPlayer and a period and Android Studio will pop up your MediaPlayer nested class callback helper dialog. As you will see, this dialog contains all nine of the nested class callbacks that you learned about in the earlier Android MediaPlayer section of this chapter. Select the first MediaPlayer.OnPreparedListener anonymous inner type option and double-click on it, which will insert it into your Java code structure.Not only does it insert the OnPreparedListener() structure, but it also will add in an unimplemented onPrepared() method structure, which you usually have to do via another mouse-over operation. All you will have to do now is to add those operations that you want to happen during this video preparation phase of the video asset. In this case, that would be to set the setLooping() method’s parameter to be true, so that your video will loop forever.Next, all you have to do is to add the .setLooping() method call and parameter to the public void onPrepared() method that is inside of the OnPreparedListener() structure, called off a MediaPlayer object named videoPlayer, which Android Studio has created for you. This is done using the following Java logic, shown in Figure [11-30](#Fig30):videoHolder.setOnPreparedListener(new MediaPlayer.OnPreparedListener() {@Overridepublic void onPrepared(MediaPlayer videoPlayer) {videoPlayer.setLooping(true);}});As you can see in Figure [11-30](#Fig30), the code is error free and ready to test using a Run ➤ Run ‘app’ menu sequence. The digital video asset now both loops and fills your screen, and (not in a screenshot; it should look like Figure [11-28](#Fig28)) you can now watch the 3D planet flythrough digital video asset loop seamlessly on the screen. You have implemented looping digital video in an application using only 15 lines of Java code, not counting three import statements that Android Studio wrote for you, which brings the total to a dozen and a half. The Java statement count is so low because again, Android has written the majority of the code for you using the classes we learned about during this chapter (MediaPlayer, MediaController, Uri, VideoView, FullscreenActivity, FrameLayout).![A324674_4_En_11_Fig30_HTML.jpg](A324674_4_En_11_Fig30_HTML.jpg)Figure 11-30.Adding the onPrepared( ) method to our new MediaPlayer.OnPreparedListener( ) callback structureNext, I am going to show you how to modify your current Java code to stream video, instead of using a captive video asset in Android. All that you have to do is change your Uri object to reference the HTTP:// instead of an android.resource:// in the URI datapath referencing string. I already have a pag800x480landscape.mp4 version and a pag480x800portrait.mp4 version of the flythrough on one of my servers at [HTTP://www.e-bookclub.com/](http://www.e-bookclub.com/) that you can use to test the code, so let’s get started and stream digital video to an Android FullscreenActivity.

### 流式数字视频:在 URI 使用 HTTP URL

Since Android handles all of the logistics regarding streaming video from the Internet into the hardware device, all we as developers really have to do is to provide the correct HTTP:// URL, or Uniform Resource Locator, in the place of the android.resource://com.example.user.digitalvideomedia/R.raw.flythru3d URI reference, which we have been using thus far in the chapter. Fortunately, an HTTP URL can also be used in the Uri object.Do this by replacing the android.resource://com.example.user.digitalvideomedia/R.raw.flythru3d URI path with an [HTTP://www.e-bookclub.com/pag480x800portrait.mp4](http://www.e-bookclub.com/pag480x800portrait.mp4) reference to an external server, using this code:VideoAssetUri = Uri.parse( "HTTP://www.e-bookclub.com/pag480x800portrait.mp4" );As you can see in Figure [11-31](#Fig31), the Uri.parse() method call will accommodate this HTTP:// URL reference, as easily as an android.resource:// URI reference. This makes it easy for us to switch our URI path references from captive video to streaming video. Notice the HTTP URL is contained in quotes, and includes the file extension, which will tell Android which codec family is utilized. This is needed as it is now “outside” of the Android OS.![A324674_4_En_11_Fig31_HTML.jpg](A324674_4_En_11_Fig31_HTML.jpg)Figure 11-31.Streaming digital video into your Android application using an HTTP URL in the Uri.parse( ) methodTest the streaming video using Run ➤ Run ‘app’, and watch the video stream! It is important to note that once the video streams over the network the first time, the asset’s data will loop out of the system memory thereafter.

## 摘要

In this chapter, you learned all digital video concepts, formats, codecs, principles, creation, optimization, and coding for Android, expanding on 2D animation concepts, formats, and principles you learned in Chapter [10](10.html). You learned about Android’s FrameLayout UI layout container class, and the FrameLayout.LayoutParams nested class along with the concept of layout gravity and how to position UI elements inside of a FrameLayout container. This FrameLayout superclass was used to create the VideoView widget used to contain your video, and to play it back using the MediaPlayer engine, and eventually, the MediaController transport UI element.You created a FullscreenActivity class and modified its FrameLayout UI XML definition file, so that you had a foundation for adding digital video functionality. After doing that, you learned about the VideoView class and the video lifecycle stages and then you added a VideoView to the FrameLayout. You learned about the Android Uri class and its Uri.parse() method, used to implement the address or path to your digital video asset.You learned about the foundational concepts of digital video encoding and optimization, including frame rates, bitrates, codecs, resolution, quality (blur), and how these all work together to allow you to optimize the digital video asset’s data footprint. After that, you learned how to use Terragen 4 and Squeeze 11 to create a 3D planet-fly over video asset and optimize that asset from image frames into MPEG-4 format, taking data that was over 2310MB and turning it into a usable 4MB digital video asset. Amazing compression technology.You learned about the Android MediaPlayer class and its nine nested classes, most used for callbacks allowing you to control the user’s digital video experience. You learned about how the VideoView uses the MediaPlayer internally, as well as how to expose it for use in your Java code.You implemented a MediaController object named videoTransport and then wired it up to your videoHolder VideoView by using the .setAnchorView() and the .setMediaController() method calls.You added an OnPreparedListener event listener to your videoHolder VideoView object and then used a .setLooping(true) method call to tell the video asset to loop forever. Then you learned how to alter a URI so that you were streaming video, instead of using captive video. You have learned a plethora of core Android information, tricks, classes, methods, callbacks, and techniques relating to digital video!Next, in Chapter [12](12.html), you’ll learn about digital audio in Android, including foundational digital audio theory and concepts, what digital audio file formats are optimal to use in Android, and how to create digital audio assets for use with the Android SoundPool audio sequencing class.