© Wallace Jackson 2017Wallace JacksonAndroid Apps for Absolute Beginners10.1007/978-1-4842-2268-3_12

# 12.数字音频:使用 SoundPool 对音频排序

Wallace Jackson<sup>1</sup>(1)Lompoc, California, USAIn the previous chapter on digital video, we covered the Uri, MediaPlayer, and MediaController classes, which you can also use with digital audio, which we are going to cover in this chapter. Since these classes are used in the same exact way with digital audio assets, I am going to show you how to use the Android SoundPool audio sequencing class in this chapter, so I can cover as many of Android’s new media classes as possible in this book.If you want to play long-form digital audio , such as songs, albums, or audio books, you would use the Android MediaPlayer class along with the Android MediaController and URI classes, using the SeekBar widget, instead of the VideoView. You can also loop long-form audio for background music without using the SeekBar widget, as you will see in Chapter [13](13.html) when we do just that using the Service class.If you want to play short-form digital audio however, such as sound effects for your games or your UI elements, for use as aural (that is, audible) feedback, you would use the Android SoundPool class. The SoundPool “engine” is actually more versatile than the MediaPlayer class, when it comes to controlling digital audio assets. SoundPool is a powerful digital audio sequencing engine, basically allowing you to composite audio in the same way that you use layers in digital image compositing to composite imagery. SoundPool is a complex and versatile digital audio class, which Android 7.x continues to improve, and I wanted to cover some of Android’s more powerful classes during the course of this book.Digital audio is a bit different from digital imagery and digital video as you can’t see it; you have to rely on your ears. Instead of using waves of light, as color does, digital audio uses waves of sound, and as such, the technical fundamentals are completely different. If you are new to digital audio, part of this chapter will cover the theory and the concepts behind digital audio, as well as the plethora of digital audio codecs, that is, digital audio file formats, which are supported in Android 7.1.1, as well as what each of them will be used for inside of an Android application.In this chapter, we’ll utilize Android’s SoundPool digital audio sequencing class to add amazing sound effects to ImageButton UI elements in the ScrollingActivity design pattern. We will do this to add aural feedback for your users. We’ll learn how to implement this SoundPool engine correctly in your Android applications, as this is no easy task. Digital audio sequencing and synthesis is an advanced topic indeed, but a key part of an Android app, and something I will cover even though this is an Absolute Beginners title.

## 音频波:历史、概念和理论

For Android 7 developers to use digital audio assets wisely and optimally, they would need to know the basic foundation of both analog audio and digital audio—where they came from, why they do what they do, and how to correctly “harness” them. I felt that this chapter would not be complete without an in-depth discussion of analog and digital audio.

### 模拟音频的基础:空气的声波

Those of you who are “audiophiles” already know that sound is created by sound waves pulsing through the air. This is the reason you see sub-woofers with massive 18- to 30-inch cones rapidly pushing out thunderous sound waves into audiences containing thousands of screaming fans at rock concerts. Before the digital audio industry existed, the analog audio industry was one of the major consumer electronics forces. In fact, it still is today, with sound waves being created with complex analog electronics products, featuring capacitors, resistors, oscillators, crystals, vacuum tubes, circuit boards, speaker cones, cardiod microphones, and similar advanced high-quality analog audio technologies.As mentioned, digital audio is quite complex, and part of this complexity comes from the need to bridge analog audio technology and digital audio technology together. Analog audio is usually generated using speaker cones of different sizes, manufactured using resilient membranes made out of one space-age material or another. The speakers generate sound waves by vibrating, or pulsing, them into existence. Our ears receive this analog audio in exactly the opposite fashion, by catching and receiving pulses of air, or vibrations with different wavelengths, and then turning them back into “data” that our brains can process. This is how we “hear” sound waves, and our brains interpret different audio sound wave frequencies as different notes, words, sounds, or tones.Sound waves generate various tones depending on the frequency of a sound wave. A wide or infrequent (long) wave produces a lower (bass) tone, whereas a more frequent (short) wavelength produces a higher (treble) tone. It is interesting to note that different frequencies of light produce different colors, so there is a close correlation between analog sound (audio) and analog light (color), which also carries through to digital content production.The volume of a sound wave will be predicated on the amplitude, or height, of that sound wave. The frequency of the sound waves equates to how closely together waves are spaced, along the X-axis. The amplitude equates to how tall the waves are as measured along the Y-axis. Sound waves can be shaped uniquely, allowing them to “carry” different sound effects. A baseline type of sound wave is called a sine wave , which you learned about in high school math, with your sine, cosine, and tangent math functions. Those of you who are familiar with audio synthesis are aware that there are other types of sound waves, utilized in sound design, such as the saw wave , which looks like the edge of a saw (hence its name), and the pulse wave , which is shaped by using right angles, resulting in immediate on and off sounds, which translate into pulses.Even randomized waveforms, such as noise, are used in sound design to obtain “edgy” sound results. As you may have ascertained, using previous knowledge regarding data footprint optimization, the more “chaos,” or “noise,” that is present in your sound waves, the harder they will be to compress for a codec, resulting in a larger digital audio file size for that particular sound. The next section takes a closer look at how an analog audio sound wave is turned into digital audio data, by using a process called sampling . The “sampler” is the core audio sample production tool for sound designers and music synthesis.

### 数字音频:样本、分辨率和频率

The process of turning analog audio (sound waves) into digital audio data is called sampling. If you work in the music industry, you have probably heard about a type of keyboard (or rack-mount audio equipment) called a sampler. Sampling is the process of slicing an audio wave into segments, so that you can store the shape of that wave as digital audio data using a digital audio codec format. This turns an infinitely accurate analog sound wave into a discreet amount of digital data, that is, into zeroes and ones. The more zeroes and ones that are used, the more accurate the reproduction of that infinitely accurate (original) analog sound wave. Sample accuracy determines how many zeroes and ones will be used to reproduce the analog sound wave, which is also the data footprint, so I will get into that discussion next, so that you know how to optimize your Android digital audio assets.Each digital segment of a sampled audio sound wave is called a sample, because it samples that sound wave at that exact point in time. The precision of a sample is determined by how much data is used to define each wave slice’s height. Just like with digital imaging, this precision is termed the resolution, or more accurately (no pun intended), the sample resolution . Sample resolution in digital audio is usually defined with 8-bit, 12-bit, 16-bit, 24-bit, or 32-bit resolution samples. Today’s HD audio uses 24-bit audio samples, and uncompressed audio usually uses 32-bit.In digital imaging as well as digital video, resolution is quantified using a number of colors, and in digital audio, the resolution is quantified in how many bits of data are used to define each of the audio samples taken. Just like in digital imaging (more color yields better quality), a higher sample resolution yields better sound reproduction. The only difference between the two is that digital audio supports the 12-bit sample resolution. Higher sampling resolution—using more data to reproduce a given sound wave sample—will yield higher audio playback quality, at the expense of a larger data footprint. This is the reason that 16-bit audio, termed “CD-quality” audio, sounds better than 8-bit audio, just like true color imagery will always look better than indexed color imagery.In digital audio there is now a 24-bit audio sampling, which is known as “HD audio” in the consumer electronics industry. HD digital audio broadcast radio uses the 24-bit sample resolution, so each audio sample, or slice of a sound wave, contains 16,777,216 different potential units of sample resolution, like 24-bit color allows 16.8M potential colors. Most new Android devices now support HD audio, such as the smartphones you’ll see advertised featuring “HD quality” audio, and more recently, HD and UHD iTV sets. This means they have 24-bit audio hardware, 24-bit capable audio decoder chips in their circuitry.Beside a digital audio sample resolution, there is also a digital audio sampling frequency . This is how many of these samples at a particular sample resolution are taken during one second of sample duration. In digital image editing, the sampling frequency would be analogous to the number of pixels that are contained within an image. More pixels in an image would equate to an analog image being sampled more frequently, just as a higher audio sample frequency would equate to a sound wave being sampled more frequently, yielding a better reproduction.Sampling frequency can also be called the sampling rate. You are probably familiar with the term CD-quality audio, which is defined as using a 16-bit sample resolution and a 44.1kHz sampling rate. This is taking 44,100 samples, each of which contains 16-bits of sample resolution, yielding 65,536 bits of audio data in each sample. Let’s do some math, and find out how many bits of data are available to provide one second of “uncompressed” digital audio data. This is calculated by multiplying the 65,536 sample resolution by the 44,100 sample frequency. This gives you a maximum potential value of 2,890,137,600 bits to represent one second of CD quality audio.Divide this by 8 to get 361,267,200 bytes, and by 1,024 to get 352,800 kilobytes, and by 1,024 again to get 344 megabytes. Not every CD quality 16-bit sample will use all of these potential data bits, however, thus your original raw, uncompressed audio samples will be much smaller than this, usually only a few dozen megabytes. So, to figure out raw data in an audio file, you would multiply the sampling bit-rate by the sampling frequency by the number of seconds in that audio snippet. You can see that it can potentially be a huge number! Audio codecs are great at optimizing this sampled data down to an amazingly small data footprint, with little audible loss in quality.In a visual medium, optimization is achieved using color depth and pixels. With digital video, this also includes frames. In the aural medium, optimization is controlled via sample resolution in combination with the sampling rate. Common sample rates in the digital audio industry include 8kHz, 22kHz, 32kHz, 44.1kHz, 48kHz, 96kHz, 192kHz, and even 384kHz. Lower sampling rates, such as 8kHz, 22.5kHz, and 32kHz, are optimal for sampling any “voice-based” digital audio, such as a movie dialog or e-book narration track, for instance. Higher audio sample rates are more appropriate for music and other sound effects, such as rumbling thunder, which require a high dynamic range (high fidelity). High sample rates are best for game or movie theater (THX) sound quality.

### 数字属性:高清、流和比特率

As mentioned, the industry “baseline” for superior audio quality is known as CD-quality audio, and it is defined using the 16-bit data sample resolution at a 44.1kHz data sample frequency. This was used to produce audio CD products way back in the 20th century, and it is still used as a minimum quality standard today. There’s also the more recent HD audio standard, which uses a 24-bit data sample at a 48kHz or 96kHz sample frequency. This is used today in HD radio as well as HD audio-compatible Android devices, such as Hi-Fi HD audio smartphones and iTV sets.If you are going to use HD audio in your Android applications, you need to make sure that your target users will own the HD audio-compatible hardware that will be required to utilize a higher level of audio fidelity. Just like with digital video data, digital audio data can either be captive within the application, with data files in the /raw folder, or digital audio can be streamed with remote data servers. Similar to digital video, the upside to streaming digital audio data is that it can reduce the data footprint of an application just as streamed digital video data can. The downside is reliability. Many of the same concepts apply equally well to digital audio and digital video.Streaming audio will shrink the size of your application’s data footprint, because you do not have to include all of that heavy new media digital audio data inside of your .APK files, so if you are planning on coding a digital Jukebox application, you may want to consider streaming the digital audio data. Otherwise, try to optimize your digital audio data so you can include it inside the .APK file. This way, the digital audio will always be available.The downside to streaming digital audio is that if your user’s connection (or your audio server) goes down, your digital audio file may not be available for your users to play and listen to! The reliability and availability of your digital audio data is a key factor to be considered on the other side of this streaming audio versus captive digital audio data trade-off. The same trade-offs that are discussed in this book for digital video assets could also be applied to digital audio assets. Just like with digital video, one of the primary concepts regarding streaming your digital audio is the bitrate of that digital audio data. As you learned in Chapter [11](11.html), this bitrate will be defined during your compression process. As with digital video, digital audio files that need to support lower bitrate settings are going to have more compression applied to the data, which will result in a lower digital audio quality level.

### Android 数字音频:数字音频格式

There are considerably more digital audio codecs in Android than there are digital video codecs, as there are only two video codecs—MPEG-4 and WebM. Android audio support includes .MP3 (MPEG-3) files, WAVE (PCM, or Pulse Code Modulated) .WAV files, .MP4 (or .M4A) MPEG-4 audio, OGG Vorbis (.OGG) audio files, Matroska (.MKS) audio files, FLAC (.FLAC) or Free Lossless Audio Codec audio files, and even MIDI (.MID, .MXMF, and .XMF) Musical Instrument Digital Interface files, which technically are not digital audio. Let me explain what MIDI is first, since it is not a format that we are going to be using in the digital audio application we will be creating next.MIDI stands for Musical Instrument Digital Interface , and it is one of the very first ways that digital audio and computers could work together, dating all the way back to ancient times (the 1980s). The very first computer to feature integrated MIDI data port hardware was Atari’s ST1040\. This computer allowed me to plug a keyboard synthesizer, at the time, it was my Yamaha TX-802 and Roland D50, into that MIDI port. MIDI allowed me to play and record performance data into the computer using the MIDI data format, along with an audio software genre called a MIDI sequencer, and MIDI Sequencing software called Final Track, created by Charles Faris.A MIDI file contains zero audio sample data, it only contains performance data. When this performance data is played back into the synthesizer, using the MIDI hardware (cable and ports), the synthesizer generates the audio tones using this MIDI performance data. MIDI would record which keys on the synth or sampler keyboard were pressed, when they were pressed, keypress duration, how hard it was pressed (after-touch), and similar nuances. When MIDI files are played back through a synthesizer, the synth replicates the exact performance of the performer or composer, even though that person is no longer playing that performance track; the computer is now playing that performance data exactly the way that it was originally performed.The way that MIDI data was used in MIDI sequencing software is that you can play an instrument track, record it as MIDI data, and the sequencer will then play it back for you, while you play another instrument track right alongside of it. This enables digital songwriters to assemble complex musical arrangements using the computer, instead of hiring a studio full of musicians. You can download open source MIDI software called Rosegarden at rosegardenmusic.com; it not only contains a full MIDI sequencer, but also contains a music notation (scoring) program as well. Rosegarden was originally for Linux OS, but is currently being ported to the Windows OS.Android supports playback of MIDI files, but doesn’t implement a MIDI class, although SoundPool can be used as a sequencer. It would not be an easy task to code a MIDI sequencer for Android although some on the coding forums have been talking about it. For that reason, it is beyond the scope of this book; I mention it here, only to educate you as to the history and scope of digital audio. MIDI played an important role early on in the evolution of digital audio, and is still part of digital audio today, especially if you are a songwriter or a sound designer.The most common digital audio format supported by Android is the popular MPEG-3, or MP3, digital audio file format. Most of you are familiar with MP3 digital audio files, due to music download websites, such as Napster. Most people collect songs in this format to use on popular MP3 players and via CD-ROM- or DVD-ROM-based music collections. The reason the MP3 digital audio file format is popular is because it has a good compression-to-quality ratio and because the codec needed to play the audio back can be found almost anywhere, even in the Android OS. MP3 is an acceptable format to use in Android applications, as long as you get the highest quality level possible out of the codec, by using an optimal encoding work process, and original (raw) audio data input.It is important to note that MP3 is a lossy audio file format, like JPEG is for imagery, where some of the audio data (and thus, quality) is thrown away during the compression process, and cannot be recovered. Android does have a lossless audio compression codec, called FLAC. This stands for Free Lossless Audio Codec . FLAC is an open source audio codec whose support is widespread, primarily due to the free nature of the software decoder.FLAC is also very fast, so the decoder is highly optimized for speed, it supports 24-bit HD audio, and there are no patent concerns for using it. This is a great audio codec to use if you need very high-quality audio within a reasonable data footprint. FLAC supports a wide range of sample resolutions, from 4-bits per sample up to 32-bits. It also supports a wide range of sampling frequencies from 1Hz to 65,535kHz (65kHz), in 1Hz increments so it is extremely flexible. From an audio playback hardware standpoint, I would suggest using the 16-bit sample resolution and either a 44.1kHz or 48kHz sample frequency. FLAC is supported in Android 3.1 and later, so, if your users are using modern Android devices, you should be able to safely utilize the FLAC codec. Therefore, it is possible to use completely lossless new media assets for your Android applications, by using PNG24, PNG32, and FLAC as long as you are targeting Android Version 3.1 and later OS revisions, which you probably will be.Another open source digital audio codec supported by Android is the OGG Vorbis codec , another lossy audio codec from the Xiph.Org Foundation. The Vorbis codec data is most often held in an .OGG data file container, and thus Vorbis is commonly called OGG Vorbis digital audio data format. The OGG Vorbis supports sampling rates from 8kHz up to 192kHz, and supports up to 255 discrete channels of digital audio (as you now know, this represents 8-bits worth of audio channels). OGG Vorbis is supported across every version of the Android OS. Vorbis is quickly approaching the quality of HE-AAC and WMA (Windows Media Audio) Professional, and is superior in quality to MP3, MPEG-4 AAC-LC and WMA. It uses a lossy format, so the FLAC codec is superior to OGG Vorbis, as it contains all of the original digital audio sample data (and as such, it is therefore lossless).Android supports the popular MPEG-4 AAC (Advanced Audio Coding) codecs, including AAC-LC, HE-AAC and AAC-ELD. These can all be contained in MPEG-4 containers (.3gp, .mp4, and .m4a) or file extensions, and most of them can be played back across all versions of Android. The one exception to this is AAC-ELD, which is supported only after Android 4.1\. ELD stands for Enhanced Low Delay, and this codec is intended for usage in a real-time, two-way communications application, such as a digital walkie-talkie or Dick Tracy smartwatch.The simplest and most widely used AAC codec is AAC-LC, or Low Complexity codec. It should be sufficient for most digital audio encoding applications. The AAC-LC should yield a higher quality result, at a lower data footprint than an MP3 format. The next most complicated AAC codec is the HE-AAC or High Efficiency AAC codec. Its codec supports sampling rates from 8kHz to 48kHz and both Stereo and Dolby 5.1 channel encoding. Android supports decoding both the v1 and v2 levels of this codec, and Android will also encode audio, using this HE-AAC v1 codec after Android Version 4.1\. Use AAC-LC codec to support earlier versions of Android.For encoding speech that usually features a different type of sound wave than music there are two other AMR or Adaptive Multi-Rate audio codecs . They are highly efficient for encoding things like speech or short-burst sound effects that do not need high-quality reproduction (such as a bomb blast sound effect). There is also the AMR-WB (Adaptive Multi-Rate Wide-Band) standard in Android, which supports nine discrete settings from 6.6 to 23.85kbps audio bitrates, sampled at 16kHz. This is a high enough sample rate where voice is concerned. This is the codec to use for a narrator track, if you are creating interactive e-book applications, for instance.There is also the AMR-NB (Adaptive Multi-Rate Narrow-Band) standard in Android OS, which supports eight discrete settings, from 4.75 to 12.2kbps audio bitrates sampled. This can be an adequate sample rate if the data going into the codec is high quality, or if the resulting audio sample does not require a high level of quality due to the noisy nature of the content (such as a bomb blast).Finally there is PCM (Pulse Code Modulated) audio, commonly known as the WAVE or .WAV audio format. Many of you are familiar with this lossless digital audio format, as it is the original audio format used with the Windows operating system. It is lossless because there is no compression applied whatsoever! PCM audio is commonly used for CD-ROM content, as well as for telephony applications. This is because WAVE audio is an uncompressed digital audio format, and therefore has no CPU-intensive decompression algorithms applied to the data stream. Thus, decoding overhead the data is not an issue for the telephony equipment or for CD players.For this reason, when you start compressing digital audio assets into the various file formats, you can use PCM as a “baseline” file format. You probably won’t put it into an APK file, however, because there are other formats (such as FLAC and MPEG-4 AAC) that will give you the same quality, using an order of magnitude less data. Ultimately, the only real way to find out which audio formats supported by Android have the best digital audio codec result for any given audio data instance is to actually encode your digital audio in the primary codecs that you know are well supported and efficient. We will be looking at how that is done as well during this chapter.

### 数字音频优化:设备兼容

Optimizing your digital audio assets for playback across the widest range of Android devices in the marketplace is easier than optimizing your digital video or digital imagery (and thus animation) across Android devices. This is because there is a much wider disparity of screen resolutions and display aspect ratios than there is a disparity of digital audio playback hardware support across Android devices, except for some Android hardware devices that feature 24-bit (HD) audio playback hardware compatibility. This is because your ears cannot perceive the same quality difference in audio that your eyes can, with digital imagery, animation, or digital video. Generally, there are three primary “sweet spots” of digital audio support, across all Android devices, which you should target.Lower-fidelity audio, such as narration tracks, or short sound effects, can use a 22kHZ or 32kHz sampling rate, with 8-bit, 12-bit, or 16-bit sampling resolution. Your high-quality audio targets include CD-quality audio, also known as 16-bit data sampling at 44.1kHz and HD-quality audio will be at the other end of this high-end audio spectrum, using a 24-bit data sampling resolution and a 48kHz audio data sampling rate. There is an unnamed “somewhere in the middle” specification, which uses 16-bit data sampling at a 48kHz sampling rate. Ultimately, however, it comes down to the quality-to-file-size results that emerge from your digital audio data footprint optimization process, which can yield some amazing results.Thus, the initial process for optimizing your digital audio assets across all of the Android devices is going to be to sample 32-bit assets at 44.1kHz or 48kHz, and then optimize (compress) them, using different audio formats supported in Android. Once that process has been completed, you’ll then see which digital audio assets provide the highest quality digital audio playback in conjunction with the lowest possible data footprint. You’ll do this using the latest version of the open source Audacity 2.1 digital audio editing and engineering software package. This software package is freely available at [www.audacityteam.org](http://www.audacityteam.org) , and will be accessible to all readers, no matter which OS platforms they prefer, whether it be Windows, macOS, Ubuntu, Fedora, RedHat or SUSE Linux.

## 音频排序:概念和原则

As you now know, the earliest forms of digital audio sequencing actually utilized MIDI. MIDI sequencers such as the Rosegarden for Linux (and soon Windows) software are still popular and allow performance sequencing. This is where a composer plays each instrumental part into a computer using a synthesizer to play an instrument sample, say a guitar, bass, drum, or piano sample, and then the computer plays back this performance data later on, while the composer accompanies the computer-replayed version of that performance.Eventually, MIDI sequencing software added digital audio capabilities alongside MIDI playback capabilities, as increased computer processing power, as well as specialized digital audio adapters, such as Creative Labs X-Fi, became available to consumers at affordable prices. It turns out that this concept of audio sequencing is applied equally well to digital audio samples that are manipulated directly by a computer, as long as this computer is powerful enough. The Linux Qtractor software package combines MIDI and Audio Sampling, and the Ardour software for Linux is a DAW, or Digital Audio Workstation, which focus primarily on digital audio sampling. In fact, my Ubuntu 17.04 Linux workstation has Ardour, Qtractor, Audacity, and Rosegarden all installed on it right now!Computers—in this case, Linux-based Android devices—keep getting more and more powerful, and all feature four or eight processor cores, and one, two, or even four billion bytes of system memory. This means Android devices today can hold several digital audio samples in memory, and thus memory optimization is an issue with Android SoundPool, as you will soon see. I wanted to cover SoundPool, an advanced audio sequencing engine class in Android, during the book, even though the book is technically supposed to be for “Absolute Beginners.” The reason for this is because if you want to utilize digital audio assets in your application, especially using samples or short audio snippets, rather than playing back long-form audio or songs, SoundPool is the way to go.

### 音频合成:概念和原理

Some of the very first MIDI keyboards were digital audio samplers. These played back digital audio samples, prerecorded (sampled) by digital audio sample design professionals such as Frank Serafine, using a range of sample resolutions and sample frequencies. You learned about samples in a previous section, so what you are going to focus on here is how those samples are taken to the next level using audio synthesis (via algorithmic processing). Synthesizers can also apply these algorithms to raw waveforms, such as sine, saw, or pulse waves.Synthesizers take digital audio—whether it’s a generated wave, borne out of an oscillator on a circuit board in a consumer electronics device, or a more complex sampled waveform, such as a sample of a plucked instrument string—and apply algorithmic processing to the waveform to create a different tonality, sound, or special effect. We’re all familiar with the synthesized instruments in popular music today; these virtual instruments are created solely by using math and programming code! One of the foundational mathematical manipulations that can be applied to audio waveforms within the digital audio domain is called pitch-shifting . This was a core technology that made keyboard samplers viable, as one sample could be used up and down the keyboard, simply by shifting its pitch! Pitch-shifting algorithms can take a sound wave up or down an octave (or even a small fraction of an octave, which is commonly known as a pitch) to create a usable range for that sample, much as though you were playing it up and down the keys of an electronic piano, a sampler keyboard, or a synthesizer keyboard.As you learned previously, the tone of a waveform can be determined by the frequency of that waveform itself, so it becomes a fairly straightforward mathematical computation to be able to accurately shift that pitch (wave) up an octave by shortening that wavelength by cutting it in half, or shift the pitch down an octave by doubling that wavelength. Any fraction between these two extremes changes the pitch of the audio sample, which is how you would get different notes along a keyboard using a single waveform. You can even create fractions between “known” pitches (common notes such as A, B, D, E, F, and G), which can be used to create “micro-tonal” music. Digital audio synthesis is amazing and the Android SoundPool class we’ll be using later supports these features.SoundPool can perform pitch-shifting on your digital audio samples, which is why you are learning about these concepts in the first part of this chapter. SoundPool has impressive audio engineering capabilities and probably will add even more features in future versions of the Android OS. You’ll need to understand these digital audio synthesis concepts in order to leverage what SoundPool can do for your application effectively and optimally. If you need the SoundPool engine, you’ll know how to use it correctly, as it can use a lot of the system memory. After this chapter, you’ll understand why you need to optimize audio data in this way to get SoundPool to work well.Another core audio synthesis mathematical manipulation is the combination, or compositing , of digital audio waveforms. This will allow the playback of two or more sounds at the same time, using a single oscillator, or using the speaker hardware. Just as with digital imaging, 2D animation or digital video compositing, this will involve adding two or more different sample data values to arrive at the final data value. Today’s Android audio hardware features impressive multi-channel support and will probably have a capability for playing stereo (two channels) or quadrophonic (four channels) quality audio (effects, music, vocal tracks, and so on) with the audio hardware that is inside of any given consumer electronics device (iTV set, tablet, e-reader, smartphone, etc.).But what if you want to combine six or eight tracks of digital audio in real-time, like an audio sequencer can? This is why SoundPool is important to master, because it provides you with a digital audio sequencing engine right inside your Android application. The Android SoundPool audio sequencing and synthesis engine is a very complex class, as you might well imagine. To make it work properly, you need to load it with the most highly optimized samples possible. This class is in Android to stay, and its code will continue to be debugged, refined, and improved, so if your Android app is going to be audio-centric, you need to master it. This chapter gets you up to speed on how to best use SoundPool, as well as what it can accomplish for your apps.

### 原始音频数据优化:内存占用

What is important, if you are going to attempt the real-time audio compositing of six or eight audio samples, is that each of these samples is well optimized. This makes what you’ll learn about digital audio data optimization extremely relevant, especially when it comes to using SoundPool. For instance, if you don’t really need HD (24-bit sample resolution) audio, in order to get your quality target, you should use CD-quality 16-bit audio, or even 12-bit audio, as you will save valuable memory, and get the same result. If you get the same audio quality using the 32kHz sample rate instead of the 48kHz sample rate, you are using 50% less sample (system) memory! For voiceover or sound effects audio these memory savings are there for the taking, as often you can sample a bomb or laser blast effectively using only 8-bit resolution with an 8kHz sample rate. You often won’t be able to detect much aural difference between 16-bit 48kHz stereo audio and the lower bitrate mono audio, as you will see in later on in this chapter, when we dive deeper into the data footprint optimization concepts and techniques.If you don’t absolutely need stereo samples, and can mix them down into mono samples, you will save 100% in memory. Combine this with lowering bitrate for the sample frequency and bit depth for the sample resolution, and you can get an even greater digital audio data footprint optimization result with the same level of quality, at least from the end user’s perspective, oftentimes using a hundred times less data in many circumstances. It’s important to remember that your end users don’t hear the “before” (uncompressed) and the “after” (compressed) audio samples like you do. As long as they sound similar, you are good to go!The other significant variable you can optimize is the length (in time) of the sample. Reducing sample durations by removing silent or unnecessary audio data can result in a reduction in raw audio data that has been sampled in the first place. This kind of data savings can add up the more digital audio samples that you are using.I just showed you three different levels (sample resolution, stereo vs. mono, and sample duration) of audio data reduction. You can think of this as system memory usage once the digital audio data has been decompressed by the Android OS into the device’s memory. This is before you get into file size optimization using the encoders.Codec optimization will affect your application’s .APK file size, but when the audio sample needs to play back inside your app, this audio sample still needs to be re-created in (decompressed into) system memory, before it can be triggered by the SoundPool engine.Therefore there are really two stages to audio optimization. The first is what you do to a raw audio wave sample prior to encoding, relating to the sample resolution, sample frequency, sample duration, and mono versus stereo sample playback. The second is what you do when you export to various codecs, using settings to ascertain how much APK file size they can save your application as captive digital audio assets.

### 数字音频合成和序列说明

Just like with digital imaging, animation, and digital video, optimizing your digital audio assets is important for two completely different, but related, reasons. With digital audio samples, especially in regard to using Android SoundPool, you must consider the amount of system memory needed to hold each sample once they have been decompressed by a codec, and placed into the raw, uncompressed state, inside of the Android device’s memory. The second reason that well-optimized audio is important is the CPU processing part of the equation. It is pretty obvious that with less audio (duration, resolution, frequency, and the number of stereo/mono tracks) to process, even if that is just sending the audio data to the audio playback hardware, there are less CPU cycles being used.Therefore, if you can get the same basic audio quality result with a lower sample resolution (fewer bits per slice of audio) or lower sample frequency (fewer slices of audio waveform each second), or fewer data tracks (mono, or one audio track instead of stereo, or two audio tracks), and shorter playback duration, you will be saving both your Android operating system’s memory resources, and your user’s Android CPU processing power resources.The reason I’m going into all of this audio sample optimization information in such great detail in this chapter is because Android’s SoundPool class will often get a bad rap, because the raw audio sample sizes that developers load into SoundPool are too data heavy. The SoundPool engine gets blamed for sluggish performance and slow playback response times due to this, rather than the developer’s lacking a data footprint optimization skill set. I am making sure that this does not happen here, and that you have the best chance for success using SoundPool.Raw audio data optimization thus becomes more and more important, at least where SoundPool is concerned, as the number of digital audio samples that you require increases. This is again true for both system memory use as well as system processing cycle usage considerations, because as you add in samples both of these resources are utilized more and more. Don’t forget that there are other things the application might be processing, such as user interface event handling, digital imagery, animation, digital video, 3D rendering, Internet access, and so on.Another reason providing highly optimized digital audio samples is so important when using a SoundPool class is because there is currently a one megabyte limit on digital audio sample data when using a SoundPool engine. Although the limit might be increased in future Android API revisions of this digital audio sequencing class, it’s still always best practice to optimize any digital audio assets effectively and efficiently. Therefore, digital audio synthesis and sequencing using SoundPool in the Android application is a balancing act, both within the device that you are testing it on at the moment, as well as across all devices that your application will ever be run on.If a given hardware platform (smartphone, tablet, e-book reader, iTV set, auto dashboard, or smartwatch) can’t handle playing a given audio data load, then it will simply not play a given sample. As time goes on, this would happen less and less due to better code and faster device processor and memory hardware. As you have learned thus far, digital audio synthesis, sequencing, and compositing are heavily predicated on the speed of a processor, the number of processor cores available, and the amount of system memory that is available to hold all of the digital audio samples that will be needed, in their uncompressed (raw or PCM) data format.The bottom line is that you need to be extremely smart about how you are doing things with SoundPool. This is not as much about how you write your code, although that is certainly important, but more about how you set up your audio samples, so that they use less memory, and can be leveraged further in the application. The common mistake many Android developers make regarding SoundPool is trying to use it more like a sequencer than like an audio synthesizer. Developers focus on SoundPool’s ability to load multiple audio assets in memory, but do not leverage its processing capability for creating new waveforms, by using a few waveforms and pitch-shifting.Here’s a good example of sequencer versus synthesis (optimization) use. SoundPool allows pitch-shifting across two full octaves, from a setting of 0.5 or down one full octave (half of your original sample waveform) up to 2.0 or up one full octave (twice of your original waveform’s width). Remember that the waveform height equates to amplitude, commonly referred to as volume, and waveform width equates to pitch (tone, or octave). Developers tend not to use this pitch-shifting feature, but instead, use different samples to achieve different notes. This fills up memory rapidly and the end result is an app works less and less well, especially across the older devices. The correct way to use SoundPool is to take your samples: say one string pluck from a guitar, one horn blow from a saxophone, one piano key strike, and three different drum samples, and using only six short mono 48kHz 16-bit high-quality samples, make a basic synthesizer that has all four basic jazz instruments using the pitch-shifting.Using this basic synthesizer setup, your users would be able to play instruments up and down two full octaves. This application would use less than a megabyte of memory to hold these 16-bit 48kHz uncompressed samples. If you used a high-quality microphone for the sampling process you would be amazed at the high-quality results that you can obtain these days using a 16-bit 48kHz Mono sampling format. If you wanted to save memory, you could also use a mono 16-bit 44.1kHz CD-quality audio, or mono 16-bit 32kHz audio with similarly acceptable results. I hope I’ve covered enough digital audio sampling and synthesis concepts in the first part of this chapter to give you some real insight as to how to optimize your digital audio assets for use with the SoundPool engine!

## Audacity 2:创建数字音频资产

In this section of the chapter, you will learn how to use the open source audio engineering software Audacity, currently at version 2.1.2\. First, we will make sure that Audacity and all of its plug-ins and codecs are installed that are available for free so that you have the audio software that will make your audio editing environment both professional and powerful. Then we’ll learn how to use this software to optimize digital audio assets in some of the Android-supported digital audio codecs (formats), including some that are from the open source domain.

### Audacity 2.1.2:安装软件和编解码器

First, you need to download and install Audacity 2.1.2, as well as to add in some plug-ins, which will greatly enhance the feature set of the software. You’ll need to download and add the popular codecs that are supported in Android. Visit the audacityteam.org website seen in Figure [12-1](#Fig1), download Audacity, and then install it.![A324674_4_En_12_Fig1_HTML.jpg](A324674_4_En_12_Fig1_HTML.jpg)Figure 12-1.Visit audacityteam.org and download Audacity and LADSPA, LAME, FFMPEG and Nyquist plug-insUse the Download ➤ Plug-ins menu sequence, also seen in Figure [12-1](#Fig1), and download and install the LADSPA, LAME, FFMPEG, and Nyquist plug-ins.Algorithms that add features and codecs to Audacity 2 are kept in the Plug-Ins folder, so adding features to this software package is as easy as exiting the software (if it is running), copying a file or files into this folder, and restarting the software again. Figure [12-2](#Fig2) shows the Audacity\Plug-Ins folder, with 122 plug-in files installed.![A324674_4_En_12_Fig2_HTML.jpg](A324674_4_En_12_Fig2_HTML.jpg)Figure 12-2.The 122 plug-in files after the full installation of Audacity 2.1.2 with LADSPA, LAME, and FFMPEGThe LADSPA installation process alone will have added around a hundred plug-in files to this folder, as shown in Figure [12-2](#Fig2). To install the LADSPA plug-ins, locate and click on the LADSPA plug-ins 0.4.15 link, which is found in the Plug-Ins section of the Audacity site, shown in Figure [12-1](#Fig1), and download the.EXE file. Do the same for the FFMPEG and LAME encoders.When these downloads finish, launch the file and select the language you are using (I chose English) and click the Next button to go through the dialog series for each collection of plug-in algorithms. Be sure to accept the license agreement and use the suggested default destination location suggested. Once you proceed through the installation dialogs, you can click on the Finish button, exiting the install process.Audacity supports other plug-ins, such as Steinberg (Cubase) VST and Nyquist that can add many other digital audio editing, waveform analysis, and sound synthesis algorithms (capabilities) to your Audacity installation. I feel the more power the better, so I suggest getting every plug-in that is currently available for Audacity, but, then again, multimedia production is my hobby, so I don’t look at it as work; so have some fun with Audacity!To check and see if everything you need to perform basic digital audio editing for Android is installed, use your control panel in your OS and go to the (installed) programs and features utility, which is shown in Figure [12-3](#Fig3). I have circled in red the entries on my system showing that these effects plug-ins and audio codecs have all been installed successfully.![A324674_4_En_12_Fig3_HTML.jpg](A324674_4_En_12_Fig3_HTML.jpg)Figure 12-3.LADSPA Setup series of dialogs; Welcome, License Agreement, and Destination LocationNext, let’s get some audio samples for your Android digital audio application. I’ll include a short section on free digital audio sample searches next, so that we can find some free uncompressed PCM samples to use during the rest of this chapter.

### 免费音频:定位数字音频音序器音频

To find some free-for-commercial-use audio samples, I’m going to use the Google Search Engine, and type in a query for something like Free PCM Audio Samples, Free Digital Audio Samples, Free PCM Audio Files, or Free Digital Audio Files, and similar Google search term combinations. It’s important to note that each of these Google searches will turn up different results, due to keywords used in each of the different websites that offer these digital audio assets. Be advised that many of the paid audio sample websites will put the word “free” in their websites (as an SEO tactic), so that they will come up on these types of free PCM audio sample searches. To find word combinations, use the plus symbol, such as: Free+Digital+Audio+Files, for instance. This will tell the search engine that you want to find sites where these words are located adjacent (next) to each other.There are dozens of good free audio sample websites, all of which will fit the bill for your needs, so be sure and investigate these further when you have some spare time. Make sure that the ones that you use for your Android application development are free for commercial use, do not require any royalty payments, and do not have any copyright (usage) restrictions. What you want to look for is high-quality, uncompressed PCM (.wav file format) samples, using 16-bit or better (24-bit or 32-bit) sample resolution, with a 32 kHz, 44.1 kHz, or 48 kHz sample frequency (sample rates). Note that if you use MP3 files (which most of these sites also offer), they will already have been compressed, and be ready for use, but you will not have any control over the data footprint optimization process. This is because much of the original audio sample data will have already been thrown away during the compression process, and you do not want to compress any kind of data that is already (lossy) compressed!I decided to use the freewavesamples.com website, which features some animal sounds (samples) that we can use for an educational app where kids can click on the picture of the animal and hear the sound that they make. I show the website in Figure [12-4](#Fig4); go there and find the animal sounds section and download your animal sounds.![A324674_4_En_12_Fig4_HTML.jpg](A324674_4_En_12_Fig4_HTML.jpg)Figure 12-4.The Free Wave Samples website offers animal sounds you can use for your educational applicationI’m going to download animal sounds for a cat, shown in Figure [12-5](#Fig5), dog, monkey, lion, horse, and bird. To go to the download page and link, use the 1 attachment link in the sample description area for each animal. Notice that these samples are each 16-bit, 44,100 Hz, CD quality, stereo format, PCM so we can practice some data footprint optimization techniques to see how significantly system memory can be optimized for the Android 7.1.1 SoundPool digital audio engine.![A324674_4_En_12_Fig5_HTML.jpg](A324674_4_En_12_Fig5_HTML.jpg)Figure 12-5.Locate the cat, dog, lion, horse, monkey, and bird samples ; click the 1 attachment link and downloadNow that you have seven uncompressed samples (I downloaded both bird samples), you can listen to them and see which animal sounds you want to use for your DigitalAudioSampler application. That is the first step in the digital audio assets optimization process, to decide which samples you must absolutely use in your application. The next steps, which we’ll cover after this, are to see what minimum sample rate is needed to retain sample quality, to see if stereo samples are needed, or if mono samples can be used, and compress using the two most widely used, and Android, JavaFX 8 and HTML5 supported, MPEG-4 AAC and OGG Vorbis codecs using a maximum quality setting. I will go through the work process for this in the next section of the chapter, using Audacity 2.1.2, as that software is open source, free for commercial use, and available on Windows, OSX, and all Linux distros.

### 数字音频优化:概念和格式

Let’s launch Audacity 2.1.2 by clicking its Quick Launch icon on your Taskbar, and use the File ➤ Open menu command sequence to open the Galloping-Horse.wav file. This is the one with the heaviest data footprint. The first time you open (or more accurately import) an audio file in Audacity, you will get a warning dialog. I selected the Make a copy of the files before editing (safer) radio button option, then I selected the Don't warn again and always use my choice above check box, and then I clicked the OK button to load the initial sample, which can be seen in Figure [12-6](#Fig6) in Audacity 2.1.2 for Windows 10.![A324674_4_En_12_Fig6_HTML.jpg](A324674_4_En_12_Fig6_HTML.jpg)Figure 12-6.Use the Project Rate (Hz) drop-down selector to set your down-sample from 44100 Hz to 22050 HzUsing these audio file import settings, and using a copy of the file, instead of the actual file itself, is called non-destructive audio editing and this is a common practice in the digital audio editing and special effects industry. The reason for using non-destructive audio editing practices is because if you mess up in your audio sweetening and special effects application and damage the audio data, you can always go back to square one by going back to loading the original audio data.Once the Galloping-Horse.wav sample data is loaded into Audacity, you’ll see a screen with an audio transport control, including pause, play, stop, back, forward, and record, as well as editing tools and level meters that show green, yellow, and red signal peak indicators, showing when the audio is playing. There are microphone and speaker selector drop-downs, where you can set which hardware you wish to utilize with Audacity as well.In the Project Rate (Hz) drop-down selector at the lower-left corner of Audacity, select a 22,050 Hz sample rate, as this is a 100% reduction in data used, and a 2X down-sampling of data, which will provide the best results, just like the concept we use for digital imagery and digital video scaling. As I mentioned, many of the concepts apply equally well to digital audio, digital imaging, and digital video. A 4X down-sample, to 11,025 Hz, would also be an optimal down-sample to make, but might remove too much audio quality (clarity). Try it and see, to get a feel for the Audacity software. The 22,050 Hz setting is shown highlighted in Figure [12-6](#Fig6) on the lower left.Use the File ➤ Export Audio menu sequence, and access the Export Audio dialog, as shown in Figure [12-7](#Fig7), and export the file as Galloping-Horse-22050Hz.wav using the WAV (Microsoft) signed 16-bit PCM option.![A324674_4_En_12_Fig7_HTML.jpg](A324674_4_En_12_Fig7_HTML.jpg)Figure 12-7.Use the Export Audio dialog to export different versions of each audio asset using different codecsAs you can see in Figure [12-7](#Fig7) numbered 1 and 2, the data footprint has decreased 100% from 575KB to 288KB, and you heard using the green Play icon in the transport. For both 44,100 Hz and 22,050 Hz settings, the result is virtually identical in sound fidelity. A bit later, we will reduce this data another 100% (or 4X total reduction) by making the Stereo sample into a Mono sample, which is fine for animal noises for a game or child’s application.This 288KB Stereo sample represents how much system memory will be needed to hold a Stereo Horse sample. Let’s save the Stereo sample as an MPEG-4 AAC file, to see how much data the MPEG-4 AAC codec can take out and still give us a high (the 500 setting shown circled in red at the bottom right of Figure [12-7](#Fig7)) quality result so that the APK size is even smaller. Remember the decompressed Stereo sample will still use 288KB of system memory, which is why I saved out a 22050Hz Wave file, to get a “baseline” for other codec compression ratios.Use the same File ➤ Export Audio menu sequence, and name the file “horse,” using lowercase letters (required by Android), and select the M4A (AAC) Files (FFmpeg) option from the Save as type selector, shown in blue in the third panel in Figure [12-7](#Fig7). This will generate an 84KB horse.m4a file, shown in Figure [12-7](#Fig7) as number 3.I have just created a Stereo MPEG-4 AAC file for the application that takes only 84KB to store of the original 44,100 HZ 575KB file (a 685% data footprint reduction) and a 343% data footprint reduction on the 22,050 Hz PCM file. Next, let’s reduce memory data footprint another 100% by making the Stereo sample a Mono sample.To take the uncompressed audio data needed by memory to a mere 144KB for our largest animal sound sample, we will open the Galloping-Horse-22050Hz.wav file and use the Audacity Tracks ➤ Stereo Track to Mono (track) menu sequence, which is shown selected in light blue at the top left in Figure [12-8](#Fig8).![A324674_4_En_12_Fig8_HTML.jpg](A324674_4_En_12_Fig8_HTML.jpg)Figure 12-8.Open the 22050Hz version of each sample, and use the Tracks ➤ Stereo Track to Mono algorithmWhat this does is to invoke an algorithm in Audacity, which merges the stereo channel data formerly held in two tracks into one track, which reduces the amount of data going into the codec (or PCM encoder) by 100% (half).This should give us another reduction in data footprint, this time from 288KB to 144KB, which is a 432KB data reduction from the 575KB original sample, which represents a more than 75% reduction in system memory use, which means we should be able to get these 2MB of animal sound samples into less than half a megabyte of Android system memory overhead. This is exactly what we will need to do to get the SoundPool engine (class) to perform as expected. Next, let’s use the open source OGG Vorbis codec to compress this new mono sample.As you can see on the left in Figure [12-9](#Fig9), I saved out another PCM sample for this Mono track I just created, as Galloping-Horse-22050Hz-mono.wav to see if it was 144KB, which it was. I then used File ➤ Export Audio and saved out a horsemono.ogg asset, this time, using another high-quality audio codec supported by Android, OGG Vorbis. This impressive codec is supported in Android, JavaFX, and HTML5, and is 100% open source, whereas the MPEG-4 patents do not completely expire until 2027\. It gives us a 43KB data footprint in the .APK file with a full quality-level setting of 10 (100%), which is a 335% reduction in data footprint (that is, 144KB / 43KB).![A324674_4_En_12_Fig9_HTML.jpg](A324674_4_En_12_Fig9_HTML.jpg)Figure 12-9.Save out mono Wave file to see the 4X data reduction and then as OGG Vorbis for a 14X reductionAs you can see, the OGG file is about half as big as the M4A file, so their compression is similar at maximum quality, given that we are compressing a Mono track here (half of stereo data). We will have a half dozen stereo M4A files, and a half dozen mono OGG files, for use in our application. If we use stereo files, we will be using 721KB of system memory. If we use mono files, we will be using 361KB of system memory. The stereo M4A files total 219KB of APK storage space. The mono OGG files total 116KB of APK storage space, using 100% quality settings no less. So, as you can see, these two codecs (and Audacity 2) are nothing short of phenomenal.Given that the original samples total 1.4MB, this represents a significant data footprint optimization result, with little to no loss of aural quality, as is seen in Figure [12-10](#Fig10). Even before compression with the two most popular high-quality codecs (MPEG4 and OGG Vorbis are still being improved today), we got a 400% memory footprint reduction using a lower sample rate and a mono track with good-quality animal sound effects. You can see each sample’s data go down by half along the left side files (PCM Wave), and applying codecs at full quality takes a data footprint down to 8 to 84K. The stereo M4A files achieve a 640% compression (1400 / 219), and the mono OGG files achieve a 1200% compression (1400 / 116), again with no audible decrease in quality.![A324674_4_En_12_Fig10_HTML.jpg](A324674_4_En_12_Fig10_HTML.jpg)Figure 12-10.Compare data footprint savings for your baseline PCM files with your compressed files in ExplorerRemember, your users cannot hear the before and after sample comparison, only the final result, so as long as it is effective audio for the application, get the most memory and storage data footprint optimization that you can!You may have noticed that in Audacity, once you click on the Export Audio dialog’s Save button for any codec, an Edit Metadata dialog will appear. This dialog will offer data fields containing text values for Artist Name, Track Title, Album Title, Track Number, Year, Genre, and Comments. Since our application doesn’t require any audio metadata, I am leaving these fields blank for now, so that we can get an accurate read on what the precise file size is; that is, the size of a file containing only the audio data. If you’re wondering if Android can read, and therefore support audio metadata, if you do want to put this data into audio files, the answer is a resounding yes.Android has a MediaMetadataRetriever class, which developers can utilize for this very specific purpose. If, for some reason, your audio application needs to leverage audio media metadata, you can use an Edit Metadata dialog, which will show itself every single time you save any type of audio file format in Audacity 2, along with the Android MediaMetadataRetriever class, which you can research and learn about at the following URL:[http:// developer.android.com/reference /android/ media/MediaMetadataRetriever.html](http://developer.android.com/reference/android/media/MediaMetadataRetriever.html)You should really spend some time with Audacity experimenting with data compression using some of the other Android 7 supported formats, such as AMR (for voice) and FLAC (for lossless HD audio). The MP3 codec has been improved upon by the MPEG-4 AAC codec, so unless you are using audio assets someone else has already compressed, you will want to use open source OGG or FLAC, or something from the MPEG4 family of codecs. Next let’s create a ScrollingActivity class for this chapter’s application and then we’ll take a look at SoundPool.

## 数字音频测序仪:滚动活动

As you know, I am trying to use all of the primary Android design patterns that Android Studio will code for you automatically, to show you how these can be used to create Android applications quickly, and without a ton of experience. We’ve already covered basic, fullscreen, and navigation drawer Activity subclasses, so now we’re going to explore the ScrollingActivity subclass of AppCompatActivity, which, as you know, allows the Android application to work across all versions of Android OS. Pretty powerful development for an Absolute Beginner!Use the File ➤ Close Project menu sequence to close the DigitalVideoMedia project, and select the Create New Project option from the Android Studio primary dialog that appears, and enter your Create New Project series of dialogs that are shown in Figure [12-11](#Fig11). Name the application DigitalAudioSequencer and accept the other defaults in the first dialog, seen on the far left, and also accept the defaults in the Target Android Devices dialog seen in the middle of Figure [12-11](#Fig11). We will be coding the SoundPool to work across all versions of Android OS, and setting the API (15) to Android 4 better gives us coverage of close to 98% of Android hardware devices out there.![A324674_4_En_12_Fig11_HTML.jpg](A324674_4_En_12_Fig11_HTML.jpg)Figure 12-11.Create your Digital Audio Sequencer project by using the Create New Project series of dialogsNext, select the ScrollingActivity Android design pattern shown selected in blue in Figure [12-12](#Fig12), and click the Next button, to advance to the Customize Activity dialog, shown on the far right in Figure [12-11](#Fig11). Change your application Title to DigitalAudioSequencer and leave the Android Activity, Layout ,and Menu Resource Naming at their default Android component and resource naming convention settings.![A324674_4_En_12_Fig12_HTML.jpg](A324674_4_En_12_Fig12_HTML.jpg)Figure 12-12.Select the ScrollingActivity pure Android design pattern and have Android Studio code your projectAs you’ll see in Figure [12-13](#Fig13), Android Studio will also create a content_scrolling.xml user interface definition file, which will eventually hold your scrolling content. For this application, this will be a series of animal image assets, which your target users, in this case, young children, will be able to tap, or click on, and hear the animal sounds that they make, courtesy of the Android SoundPool digital audio sequencing engine.![A324674_4_En_12_Fig13_HTML.jpg](A324674_4_En_12_Fig13_HTML.jpg)Figure 12-13.The bootstrap ScrollingActivity class has the basic Android Toolbar, Menu, and Activity featuresClick on the Finish button, shown on the bottom right in Figure [12-11](#Fig11), and create your digital audio sequencer project, so that we can take a look at the bootstrap (Android Studio 2.3 generated) code, and see what it does, next.As you can see in Figure [12-13](#Fig13), your ScrollingActivity subclass of AppCompatActivity has all of the familiar components, which we have already covered during the book. This will allow us to focus on some new coding structures during this chapter, including creating your own custom method; using if-else structures to detect the user’s Android OS version; and how to instantiate, load, and play back SoundPool engine digital audio samples.I opened the import statements, so that you can see what Android classes we are starting out with, including the core Bundle, View, Menu, MenuItem, Toolbar and AppCompatActivity classes, which are used in most of these pure Android design patterns that you can have Android Studio code for you.We will be removing the FloatingActionButton and Snackbar class import statements, as we will be modifying this ScrollingActivity to provide scrolling animal imagery that users can use to trigger animal-related audio samples. We will be adding import statements for the digital audio-related classes we will be learning about during this chapter, including SoundPool, AudioManager, AudioAttributes, and Build, as well as the ImageView class used to hold the digital image assets. Build is an object builder class that we will use to build SoundPool and AudioAttributes objects.We will remove the FloatingActionButton functionality, and replace the Snackbar functionality with triggers of SoundPool digital audio samples inside of the View.OnClickListener() structures. We’ll also create a custom method to instantiate (initialize) the SoundPool engine, and configure it for use, and load it with audio samples.The first thing that we are going to modify is in the top-level activity_scrolling.xml user interface definition file, so right-click on the app/res/layout/activity_scrolling.xml file in the Android ➤ Project pane, shown on the left side of Figure [12-13](#Fig13), and select the Jump To Source context menu option. This will open the XML markup in a tab for editing, which can be seen in Figure [12-14](#Fig14).![A324674_4_En_12_Fig14_HTML.jpg](A324674_4_En_12_Fig14_HTML.jpg)Figure 12-14.Delete the FloatingActionButton widget so we just have a scrolling container for our animal imagesDelete the FloatingActionButton child UI element, at the bottom of this top-level UI definition, underneath the <include> child element that references the content_activity.xml UI definition that contains the Nested Scroll View. The XML markup to delete is shown at the bottom of Figure [12-14](#Fig14), highlighted in red.Now we are ready to add our ImageView content to the content_scrolling.xml file. We will be replacing the <TextView> child UI element with a <LinearLayout> element full of <ImageView> child elements. The reason we have to use the LinearLayout is because the NestedScrollView parent element must have only a single child UI element defined.The only other thing that we have to do so that the <ImageView> UI elements have something to reference is to go onto the Internet and get some free online image assets to use inside of the <ImageView> UI elements. I am obviously going to use animal imagery to go along with those six animal sounds digital audio samples we found online.Once we download, optimize, and install these in the app/res/drawable folder, we will be ready to code!Next, go to Pexels.com (Figure [12-15](#Fig15)) and get animal images (I chose dog, cat, lion, bird, horse, and monkey) to use in the app.![A324674_4_En_12_Fig15_HTML.jpg](A324674_4_En_12_Fig15_HTML.jpg)Figure 12-15.Go to pexels.com , and search for “animals” and find dog, cat, lion, bird, horse, and monkey imagesI made all of the images the same iTV HDTV resolution and 16:9 aspect ratio and saved them in the book repository as JPEG images (50% to 75% quality). I also created MDPI 480 by 720 versions, and saved them as PNG files, as can be seen in Figure [12-16](#Fig16), and copied these into the /app/res/drawable folder for the project. If you like you can get some practice with GIMP 2.8.20 and create HDPI versions by down-sampling the HDTV resolution versions by 100% to 960 by 540\. To use all DPI versions, you create an app/res/drawable-hdpi and /app/res/drawable-xhdpi folder in Android Studio 2.3 (by right-clicking on /app/res and using New ➤ Android resource folder), and copy the JPEG originals into the XHDPI folder and the 960 by 540 versions (can be JPEG or PNG) into the HDPI folder . Make sure all of these files are using only the animal name, in all lowercase letters, as shown in Figure [12-16](#Fig16).![A324674_4_En_12_Fig16_HTML.jpg](A324674_4_En_12_Fig16_HTML.jpg)Figure 12-16.Copy the 480 by 270 PNG24 images into the DigitalAudioSequencer project’s /res/drawable folderReplace the <TextView> UI element in the content_scrolling.xml tab with the <LinearLayout> UI layout container, which will contain the six <ImageView> UI elements. Set the orientation to vertical and the layout parameters to match the dimensions of the parent NestedScrollView layout container using the match_parent constant. To match the blue color value in the Toolbar, use the android:background="@color/colorPrimary" attribute, as is shown in the following XML markup , also seen in its finished state in Figure [12-17](#Fig17):![A324674_4_En_12_Fig17_HTML.jpg](A324674_4_En_12_Fig17_HTML.jpg)Figure 12-17.Create a <LinearLayout> and child <ImageLayout> elements inside a <NestedScrollView> parent<LinearLayout xmlns:android="http://schemas.android.com/apk/res/android"android:orientation="vertical" android:layout_width="match_parent"android:layout_height="match_parent" android:background="@color/colorPrimary" ><ImageView android:id="@+id/bird" android:src="@drawable/bird"android:layout_width="match_parent" android:layout_height="wrap_content" /></LinearLayout>For the <ImageView> child tag elements , you can do one as shown above, and then copy and paste it five more times, then change the animal names so that you have all six animals with their own ImageView display pane in the scrolling view. Notice that I use the match_parent constant to span the scrolling view container’s width, but wrap_content in the height (Y) dimension in order to maintain the 16:9 aspect ratio for the imagery. If you used match_parent on both layout parameters, your imagery would expand to fill the container and there would be no blue bars between the images, and the images themselves look distorted and unnatural (especially the horse). If you like, you can try this and see for yourself the difference (see Figure [12-19](#Fig19) for the correct image aspect ratio) just to get a feel for what wrap_content versus match_parent will do, regarding X-axis and Y-axis image scaling.Remove the FloatingActionButton class import statement in the ScrollingActivity.java tab seen in Figure [12-18](#Fig18), add an import android.widget.ImageView; statement, and change the FloatingActionButton instantiation to an ImageView instantiation. Call the .setOnClickListener() off the ImageView object, instead of the fab object.![A324674_4_En_12_Fig18_HTML.jpg](A324674_4_En_12_Fig18_HTML.jpg)Figure 12-18.Remove FloatingActionButton import, and change FloatingActionButton instantiation to ImageViewThis essentially uses the FloatingActionButton instantiation and event handling structure, and steals it for use with the ImageView, using the following Java 8 code that replaces the previous FloatingActionButton fab code :import android.widget.ImageView;...ImageView bird = (ImageView) findViewById(R.id.bird);bird.setOnClickListener( new View.OnClickListener() {@Overridepublic void onClick(View view) {// Event Handling: Currently is Snackbar code. Will Be: SoundPool.play(sampleIdAnimalName);}});Later, we will replace the SnackBar code with SoundPool code , and then copy and paste this structure five more times, to create programming structures for all six ImageViews. Let’s use the Run ➤ Run ‘app’ menu sequence and make sure that the code replacement works and that the NestedScrollView now shows six animal images. As you can see in Figure [12-19](#Fig19), this application is starting to look really great! Scroll through all six to test your app. Now all we have to do is to create a custom method setting up and configuring your SoundPool digital audio sequencing engine.![A324674_4_En_12_Fig19_HTML.jpg](A324674_4_En_12_Fig19_HTML.jpg)Figure 12-19.The ScrollingActivity is converted to a scrolling image repository, filled with beautiful animal imagesNow we are ready to start learning about SoundPool, and Android’s digital audio-related classes and methods.

## Android SoundPool:数字音频引擎

The Android SoundPool class is a direct subclass of the java.lang.Object master class. It’s important to note that SoundPool is not a subclass of the Android MediaPlayer class, as one might be liable to assume. However, like the MediaPlayer class, it is part of the android.media package, and thus, the complete path to the class (as used in an import statement) would be android.media.SoundPool. The Java class hierarchy looks like the following:java.lang.Object> android.media.SoundPoolSince SoundPool is a direct subclass of java.lang.Object, we can infer that it is its own, “scratch-coded” digital audio sequencing engine creation. It is also important to note that you can use a SoundPool object and MediaPlayer objects at the same time, if you need to. In fact, there are distinct applications for both of these audio playback classes. You should use MediaPlayer for “long-format” audio (and video) data, such as albums, songs, audio books, TV shows, or movies. SoundPool is best used for “short-form” audio snippets, especially when they need to be played in rapid succession and (or) combined together, such as in a 2D or 3D game, e-book, iTV show, Android Wear Watch Faces, Android Auto application, or any gamified application.You can load your SoundPool collection of samples into system memory from one of two places. The first, and most common, would be from inside of the APK file, which I call captive new media assets, in which case, they would live in the app/res/raw project folder, as they did for your DigitalVideoMedia application in Chapter [11](11.html). The second place you can load samples from is an SD Card or similar storage location. This is what one would term the Android 7.1.1 OS file system.The SoundPool uses the Android MediaPlayer Service to decode an audio asset into memory. We’ll be covering Android Service classes in the next chapter in this book (are you starting to see the logical progression here?). It does this using uncompressed 16-bit PCM mono or stereo audio. This is the main reason that I’ve been teaching you the work process that optimizes digital audio using a 16-bit sampling resolution, because if you use 8-bit audio, Android up-samples it to 16-bit, and you end up with wasted data, which could have been spent on better quality.This means that you should optimize your sample frequency, but not your sample resolution (use 16-bit). Don’t use stereo audio unless you absolutely need to. It is very important to conform your optimization work process to how SoundPool works to get optimal results across the largest number of consumer electronics devices. The 48 kHz is the best sample frequency to use if you can, with the 44.1 kHz coming in second, and 32 kHz coming in third. To optimize, keep a sample short and mono, and use a modern codec, such as MPEG4 AAC, OGG, or FLAC, to retain the most quality, and still get a reasonable amount of data compression for your APK file. You can calculate system memory requirements by using the original PCM uncompressed digital audio file size as your baseline.When the SoundPool object is constructed in Java 8, as you will be doing later on during this chapter, a developer will set a maxStreams parameter using an integer value. This parameter will predetermine how many audio streams you can composite, or render, at the same time. This parameter sets aside system memory that can be used for digital audio. In our application, we’ll set this to six, but you could use less, as a user will play only one sample at a time for this particular application.Setting the maximum number of streams parameter to as small a number as possible is a good standard practice. This is because doing so will help to minimize CPU cycles used for processing audio samples, and will reduce any likelihood that the SoundPool audio mixing will impact other areas of your application performance. Thus, you could use maxStreams as low as 1 or 2 for the application we are coding as there are only one or two image occurrences per screen, as you can see in Figure [12-19](#Fig19).The SoundPool engine will track the number of active (playing) audio streams (samples), to make sure that it does not exceed the maxStreams setting. If a maximum number of audio streams is ever exceeded, SoundPool will abort a previously playing stream. It will do this based on a sample priority value that you can specify. I simply specified 1 through 6, as it is unlikely that the user will scroll and click that rapidly for this application. If SoundPool finds two or more audio samples playing that have an equal sample priority value, it will make the decision regarding which sample to stop playing based upon sample age, which means the sample that has been playing the longest is the one that’s terminated (playback stopped). I like to call this the Logan's Run principle!Priority level values are evaluated from low to high numeric values. This means that higher (large) numbers will represent the higher priority levels. Priority is evaluated when a call to the SoundPool .play() method causes the number of active streams to exceed the maxStreams value, which is set when a SoundPool object is instantiated. In the case where the sample priority for the new stream is lower than all the active streams, the new sound will not play, and the .play() function will return a streamID of zero (nothing played). For this reason, be sure your application’s Java 8 code keeps track of exactly what is going on with your audio sample priority-level settings, if you’re doing something like a game that needs dynamic, real-time audio sample playback decisions to be made.Samples are looped in SoundPool by setting any non-zero looping value. The exception to this is that a value of -1 will cause samples to loop forever, and under this circumstance, the application code must make a call to the SoundPool . stop() method to stop the looping sample. So a non-zero integer value will cause a sample to repeat itself that specified number of times; thus, a value of 7 will cause your sample to play back a total of 8 times, as computers start counting using the number 0 instead of 1\. For instance with the horse sample, you could remove some of the “dead space” (silence) before and after the hoof beats so that you could extend this sound effect by using a numeric value greater than 1, which would cause a longer hoof beats playback. The reason I did not do this is because the public domain sample applied the Doppler Effect to the sample, causing the hoof beats to fade into the distance. Get some practice with SoundPool, and try this for yourself, as a variation on what we are doing here.You can change each sample playback rate using SoundPool, which as mentioned makes this class into an audio synthesis tool. A sample playback rate equal to 1.0 will cause your sample to play back at its original frequency. A sample playback rate of 2.0 will cause the sample to be played at twice its original frequency, which will shift it up a full octave higher, if it is a musical instrument note. Similarly, a sample playback rate set to 0.5 will cause SoundPool to play the sample at half of its original frequency. This will sound like it is playing an octave lower. The sample playback rate range of SoundPool is currently limited to 0.5 to 2.0; however, this could be upgraded in a future API revision to, say, 0.25 to 4, which would give us developers a five-octave sample playback range.Now it is time to learn how to implement a SoundPool object, and learn about a couple of other Android digital audio utility classes that are used in conjunction with SoundPool. As you can see, I’m trying to cover as many powerful Android classes in this book as is humanly possible, especially those new media classes that allow you to set your applications apart from the rest of the applications in the Google Play application marketplace.

### 将 SoundPool 引擎添加到 DigitalAudioSequencer

Now it’s time to get into Java programming in the ScrollingActivity.java AppCompatActivity class, and add the SoundPool engine so that you can add different animal sounds to match your six ImageView UI elements. Open Android Studio and open ScrollingActivity.java in an editing tab, and declare a SoundPool object at the top of your class. Name it animalSamples using the following object declaration statement, seen at the top of Figure [12-20](#Fig20):![A324674_4_En_12_Fig20_HTML.jpg](A324674_4_En_12_Fig20_HTML.jpg)Figure 12-20.Add a SoundPool object named animalSamples and then a method named setupAnimalSamples( )SoundPool animalSamples;Once you type in SoundPool if you double-click on the SoundPool (android.media.SoundPool) pop-up Android Studio will also write the import statement for SoundPool for you.Add a space after the onCreate() by using the newline character (the return key on the keyboard) and type in void, and then select the public void setAnimalSamples option, to have Android Studio create a new method infrastructure for you as seen at the bottom of Figure [12-20](#Fig20). Note that because you added an animalSamples SoundPool object, that Android Studio 2.3 suggests a method that will allow you to set up (configure) this SoundPool object.Edit the setAnimalSamples() method name that Android Studio 2.3 created for you to instead be setupAnimalSamples() as that more accurately represents what the method is going to be doing. Inside of the method body, create an if-else conditional logic construct that will use SoundPool.Builder() if Android OS is at version 5 or later. As you create this code, be sure to use Alt+Enter to have Android Studio 2.3 add an import os.Build; statement for you.This will be accomplished using the following top-level pseudo-code (with some real code) programming structure:public void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP){ SoundPool.Builder() method call }else { SoundPool() constructor method }Inside the if(Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP){...} part of the conditional logic construct, instantiate your animalSamples SoundPool object, using the SoundPool.Builder() approach, as is shown in Figure [12-21](#Fig21), and chain a .setMaxStreams() method call and finally a . build() method call, like this:![A324674_4_En_12_Fig21_HTML.jpg](A324674_4_En_12_Fig21_HTML.jpg)Figure 12-21.Type animalSamples = new SoundPool, and select the Builder class to import and insert into codepublic void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {animalSamples = new SoundPool.Builder().setMaxStreams(6).build(); }In the else {...} section, use the deprecated SoundPool constructor method , seen in Figure [12-22](#Fig22), to instantiate the animalSamples SoundPool object using the deprecated constructor.![A324674_4_En_12_Fig22_HTML.jpg](A324674_4_En_12_Fig22_HTML.jpg)Figure 12-22.Instantiate animalSamples in the else section using a new SoundPool( ) deprecated constructor callAs you can see highlighted in Figure [12-22](#Fig22), this constructor method call is deprecated and therefore is lined-out in Android Studio, which I find unnerving, as it’s like Android Studio is saying “you can’t use this” when in fact you can use it, especially if you are developing for 32-bit (Pre Android 5) Android devices. When you write this code, be sure to have Android Studio code the import android.media.AudioManager statement for you.animalSamples = new SoundPool(this, AudioManager.STREAM_MUSIC, 0);The parameters include the Java this keyword, an AudioManager stream type, and the sample rate conversion quality level (zero being 100%, this is currently unimplemented but portends more SoundPool development as a synthesizer, which is great for developers). If you type in the AudioManager class and a period, a pop-up dialog in Android Studio 2.3 will list all of the constants currently available to you using this class.The AudioManager class provides access to audio volume and phone ringer mode controls (as constants) for the Android 7.1.1 OS. Take some time and go over these constants, and see what types of digital audio attributes for the Android OS that you as a developer are allowed to control. The 90 constants are located at the following URL:[https:// developer.android.com /reference/android/media/ AudioManager .html](https://developer.android.com/reference/android/media/AudioManager.html)The AudioManager constant we’ll be using is the STREAM_MUSIC constant, used for music or sound effects in Android applications using the MediaPlayer audio (and video) playback engine, which is why the constant is using the word “stream” as many developers choose to stream their audio and video content from an external data server. I prefer to optimize the content and have it in an APK file, so that I do not require an external server or that the end-user must be on-line in order to use the app.The setupAnimalSamples() method , and its core if-else structure, which is used to decide if the deprecated constructor method (used prior to API 21 Lollipop), or the new SoundPool.Builder() class, is now error-free, and performs the SoundPool engine initialization function that will be used to create the SoundPool object. The Java 8 code that creates this initial method structure should at this point look like the following Java statements, which can be seen error free and highlighted in Figure [12-23](#Fig23):![A324674_4_En_12_Fig23_HTML.jpg](A324674_4_En_12_Fig23_HTML.jpg)Figure 12-23.Call setupAnimalSamples( ) in onCreate( ) to implement the basic SoundPool object instantiationspublic void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {animalSamples = new SoundPool.Builder().setMaxStreams(6).build();} else {animalSamples = new SoundPool(6, AudioManager.STREAM_MUSIC, 0);}// Sample loading statements will be added here using animalSamples.load() method calls}We will be adding some initialization (sample loading) Java statements after this if-else construct a bit later on, after we learn about the new AudioAttributes, but at this point, the method is now usable for instantiation of the SoundPool object, so we can now add it to our onCreate() method without generating any red error highlighting.To make this setupAnimalSamples() method active in the ScrollingActivity class, we will need to add a method call invoking it inside of the onCreate() method, after the super.onCreate() and setContentView() method calls. This is the “organizational equivalent” of putting the code in setupAnimalSamples() directly inside onCreate(). Add in this setupAnimalSamples(); method call, inside of your onCreate() method after the super.onCreate() and after the setContentView() method calls, by using the following Java statement, as shown highlighted in red in the middle of Figure [12-23](#Fig23):setupAnimalSamples();Let’s take a break from coding, and learn about AudioAttributes so that we can configure the SoundPool object.

### Android 音频属性:配置 SoundPool

The Android public final class AudioAttributes extends the Java Object class and also implements the Parcelable interface. The AudioAttributes Java class hierarchy therefore would look like the following:java.lang.Object> android.media.AudioAttributesThis AudioAttributes class was added in Android OS API 21 (Lollipop) to encapsulate a collection of attributes that describe information about the audio stream, for use with the new SoundPool.Builder() object instantiation approach. This AudioAttributes object replaces the AudioManager class stream type constants (for instance, the STREAM_MUSIC or STREAM_ALARM constants) previously used for defining behavior for audio playback.Attributes will allow the developer to specify more information than is conveyed in the stream type by allowing the application to define how it is using digital audio, that is, why you’re playing that sound, and what purposes that sound is being used for. This is achieved using audio “usage” and “content type” information attributes.Examples of audio usage constants include USAGE_MEDIA, which we will be using, and USAGE_ALARM. These two constants are the closest thing AudioAttributes has to AudioManager stream type constants. Usage information is more detailed than stream type information, allowing certain platforms or routing policies to use the information for refining hardware volume and routing decisions. Usage is important information to supply in your AudioAttributes object, and it is strongly recommended by Google to build any object instance with this information supplied, as we will be doing in the next section of the chapter.The content type attribute, on the other hand, involves what type of audio content you are going to be playing. The content type attribute expresses the general category for your digital audio content. This audio information is optional, but I’m going to use it, so I can show you how to be thorough in your digital audio implementation.There are five primary content types, as you will see in Figure [12-26](#Fig26), for instance, CONTENT_TYPE_MOVIE would be specified for a movie streaming service, CONTENT_TYPE_MUSIC for music playback applications, CONTENT_TYPE_SONIFICATION for sounds used to accompany UI design elements or app content (this is what we’re going to be using for animal sounds that accompany animal imagery), CONTENT_TYPE_SPEECH should be specified for vocal tracks, and CONTENT_TYPE_UNKNOWN for anything else. This digital audio information can be used by an audio framework to selectively configure audio post-processing code structures.AudioAttributes can be used with Android audio classes other than SoundPool as well. For example, one of the AudioTrack class constructors, AudioTrack(AudioAttributes, AudioFormat, int, int, int) will allow you to configure your MediaPlayer using the .setAudioAttributes(AudioAttributes) method call, or to configure a Notification with audio. An AudioAttributes instance is built using a AudioAttributes.Builder() builder class, which we will be using in the next section of this chapter. In fact, let’s dive in and get our hands dirty now, configuring our SoundPool engine object by using AudioAttributes and AudioAttributes.Builder().

### 使用音频属性配置声音池

At the top of your ScrollingActivity.java class, add in the AudioAttributes object declaration , underneath your animalSamples SoundPool object declaration. If you type in the AudioAttributes and then double-click on the pop-up helper that shows the class and package information, Android Studio will code the import statement for you, as is shown in Figure [12-24](#Fig24). The Java object declaration statement should look like the following code:![A324674_4_En_12_Fig24_HTML.jpg](A324674_4_En_12_Fig24_HTML.jpg)Figure 12-24.Add AudioAttributes object named sampleAttributes and instantiate it with AudioAttributes.Builder( )AudioAttributes sampleAttributes;Next, we have to instantiate the sampleAttributes AudioAttributes object inside of the if statement which is in the setupAnimalSamples() method. Since this class was introduced in API 21 it uses the Builder nested class, just like the new SoundPool class Builder also used in API 21 and later, we will put this instantiation in the if portion of the conditional statement, as is shown in Figure [12-24](#Fig24). Your new Java code should look like this:public void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {sampleAttributes = new AudioAttributes.Builder().build();animalSamples = new SoundPool.Builder().setMaxStreams(6).build();} else { ... }This code will build an empty AudioAttributes object named sampleAttributes that will eventually be wired into the SoundPool object using, you guessed it, the .setAudioAttributes(sampleAttributes) method call off of the SoundPool.Builder object instantiation chain (this is in Figure [12-27](#Fig27), if you wanted to look ahead).To add configuration parameters (AudioAttributes class constants) for the USAGE and CONTENT_TYPE attributes we discussed in the previous section, you will insert the .setUsage() and .setContentType() method calls in the method call chain prior to the final .build() method call.Let’s configure the AudioAttributes USAGE attribute (or characteristic, or parameter, or setting if you prefer) first, since that is more important to set, according to the Android developer website documentation. This would be done using the following AudioAttributes.Builder() Java statement, as shown highlighted in Figure [12-25](#Fig25):![A324674_4_En_12_Fig25_HTML.jpg](A324674_4_En_12_Fig25_HTML.jpg)Figure 12-25.Add the .setUsage( ) method in the .Builder( ) method chain, and specify a USAGE_GAME constantpublic void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {sampleAttributes = new AudioAttributes.Builder().setUsage(AudioAttributes.USAGE_GAME).build();animalSamples = new SoundPool.Builder().setMaxStreams(6).build();} else { ... }Notice in Figure [12-25](#Fig25) that you can have Android Studio provide you pop-up selectors with Android classes, in this case, AudioAttributes, and constants, in this case, you would use USAGE_GAME (faster response), or even USAGE_MEDIA if you prefer, or even USAGE_UNKNOWN. If you were coding an animal sounds alarm app, you could select USAGE_ALARM. All of these usage constants are shown in a light blue drop-down selector, in Figure [12-25](#Fig25).Next, let’s add another method call to .setContentType(CONTENT_TYPE) into the builder method chain, which specifies CONTENT_TYPE_SONIFICATION. Sonification is exactly what it sounds like it is: it is adding samples or sound effects to visual things, such as user interface elements (Button, ImageButton, or ImageView) to sonify them. Since this is precisely what we are doing in this application, this is the content type constant that we’ll utilize.The Java programming constructs for adding yet another method call to this growing method chain necessitates that we use a different code indentation (formatting) approach. The code, shown in Figure [12-26](#Fig26), looks like this:![A324674_4_En_12_Fig26_HTML.jpg](A324674_4_En_12_Fig26_HTML.jpg)Figure 12-26.Add .setContentType() method into method chain, and specify CONTENT_TYPE_SONIFICATIONpublic void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {sampleAttributes =new AudioAttributes.Builder().setUsage(AudioAttributes.USAGE_GAME).setContentType(AudioAttributes.CONTENT_TYPE_SONIFICATION).build();animalSamples = new SoundPool.Builder().setMaxStreams(6).build();} else { ... }To wire the AudioAttributes object into the SoundPool object, you use the .setAudioAttributes() method from the SoundPool class and pass over the AudioAttributes object as its sole parameter. This would be done using the following line of Java 8 code, as is shown highlighted in pale yellow in Figure [12-27](#Fig27):![A324674_4_En_12_Fig27_HTML.jpg](A324674_4_En_12_Fig27_HTML.jpg)Figure 12-27.Wire AudioAttributes object to SoundPool object with .setAudioAttributes and add soundId integersanimalSamples=new SoundPool.Builder().setMaxStreams(6). setAudioAttributes ( sampleAttributes ).build();Make sure that this method call is inserted in your SoundPool.Builder chain either before or after your .setMaxStreams() method call, and before the final .build() method call that actually builds a SoundPool object.Now that you have declared, instantiated, and wired together your SoundPool and AudioAttributes objects in setupAnimalSamples() you can initialize the SoundPool object, by using the .load() method to load your six animal sound samples, and assigning them into their own sample identification numbers (integers). Before you write this code be sure that you have copied the six animal sample assets you optimized earlier into the /res/raw/ folder, as you learned how to do (and create), in the previous chapter on digital video.Declare six integer sId (sample identification) variables, named for each animal, using the following compound Java code statement, which can be seen at the top of Figure [12-27](#Fig27), located under the two object declarations:int sIdCat, sIdDog, sIdBird, sIdLion, sIdHorse, sIdMonkey;After the conditional if-else statement in the first part of the setupAnimalSamples() method that instantiates and configures your SoundPool digital audio engine based on Android version 7.1.1 (Nougat), you will add six Java statements that use the SoundPool .load() method to load the digital audio asset data from the /app/res/raw folder into the SoundPool, assigning a priority and passing the application Context object by using the Java this keyword. The result of this operation is assigned to a sample ID integer value, which you will later use with the SoundPool .play() method call, inside of the onClick() event handler constructs, which used to contain SnackBar calls to display messages at the bottom of the device screen. The completed setupAnimalSamples() method can be seen below, and is also shown in Figure [12-27](#Fig27):public void setupAnimalSamples() {if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {sampleAttributes =new AudioAttributes.Builder().setUsage(AudioAttributes.USAGE_GAME).setContentType(AudioAttributes.CONTENT_TYPE_SONIFICATION).build();animalSamples =new SoundPool.Builder().setMaxStreams(6).setAudioAttributes(sampleAttributes).build();} else {animalSamples = new SoundPool(6, AudioManager.STREAM_MUSIC, 0);}sIdCat = animalSamples.load(this, R.raw.cat, 1);sIdDog = animalSamples.load(this, R.raw.dog, 2);sIdBird = animalSamples.load(this, R.raw.bird, 3);sIdLion = animalSamples.load(this, R.raw.lion, 4);sIdHorse = animalSamples.load(this, R.raw.horse, 5);sIdMonkey = animalSamples.load(this, R.raw.monkey, 6);}We are now ready to replace the Snackbar code in the onClick() event handler, which we morphed earlier in the chapter into an ImageView named bird, with .play() method calls off of an animalSamples SoundPool object, as you can see being created in Android Studio in Figure [12-28](#Fig28). Inside of the bird.setOnClickListener() event listening construct, type the object name animalSamples, hit a period key, and select the play(int soundId, float leftVolume, float rightVolume, int priority, int loop, float rate) pop-up menu option.![A324674_4_En_12_Fig28_HTML.jpg](A324674_4_En_12_Fig28_HTML.jpg)Figure 12-28.Inside the onClick( ) event handler type the animalSamples object and select the .play( ) method callAfter you type the period and double-click on the play() SoundPool method call option, Android Studio will add it for you, which is shown on the left half of Figure [12-29](#Fig29). Type an “s” inside the parameter area and select your sIdBird integer sample ID for the play action for the bird ImageView onClick() event handler. Type a comma, a space, and then Android Studio will pop up a parameter list balloon helper, which is shown on the right half of Figure [12-29](#Fig29). I used 100% (1.0f, or float) for leftVolume and rightVolume, 1 (integer) for priority, 0 (integer) for loop and no change (1.0f, or float) for the sample rate pitch-shifter, which implements audio wave synthesis.![A324674_4_En_12_Fig29_HTML.jpg](A324674_4_En_12_Fig29_HTML.jpg)Figure 12-29.Use Android Studio to guide you through all the configuration parameters of the .play( ) method callMake sure you use the f (float) after decimal numbers like 1.0, otherwise Android Studio will give you an error, thinking that you are using the Java double variable data value. Also make sure integers do not use any decimal, and use correct float data value as the SoundPool engine seems to have problems “casting” number types (from integer 1 to float 1.0, for instance). I was getting runtime errors, using .play(sIdBird, 1, 1, 1, 0, 1), and when I then implemented the more precise float number format specifications, the application worked perfectly.Now we are ready to copy and paste the ImageView related Java constructs for the bird, which we created using the SnackBar construct that Android Studio originally created for us. First make sure to run the application and click on the bird and make sure it chirps, as a great work process for an Absolute Beginner is to make sure that the app works after any material change or addition of new Java 8 programming logic, classes, methods, interfaces, constants, asset references, or variables. The same goes for new XML markup as well.Now select the four lines of code starting with ImageView bird, right-click on the selection and click copy, and add a line of code under the construct and right-click in it and select paste. You can also use CTRL-C (Copy) and then CTRL-V (Paste) twice. The first CTRL-V replaces the selection, and the second CTRL-V adds the second (copied) construct, which you will then replace the word bird (or Bird) with horse (or Horse), as is shown in Figure [12-30](#Fig30). On the next iteration of copy, paste, and replace, you can copy both constructs and paste both constructs (twice, which is thrice if you are using the CRTL-V method) and easily create all six event listening (outer) plus event handling (inner) structures.![A324674_4_En_12_Fig30_HTML.jpg](A324674_4_En_12_Fig30_HTML.jpg)Figure 12-30.Copy and paste ImageView bird statement and event listener structure to create ImageView horseMake sure that all of your ImageView names, digital image asset reference names, .setOnClickListener() calls off the ImageView object name, and sample ID integer data value names are all the same for each of the six Java constructs that you are duplicating. This is shown in the Java 8 code below using bold text. This code is also shown along with all of the other Java statements that are in the onCreate() method in Figure [12-31](#Fig31).![A324674_4_En_12_Fig31_HTML.jpg](A324674_4_En_12_Fig31_HTML.jpg)Figure 12-31.Copy and paste all six animal ImageView instantiation and event listening and handling constructsImageView bird = (ImageView) findViewById(R.id.bird);bird.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdBird, 1.0f, 1.0f, 1, 0, 1.0f);}});ImageView horse = (ImageView) findViewById(R.id.horse);horse.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdHorse, 1.0f, 1.0f, 1, 0, 1.0f);}});ImageView monkey = (ImageView) findViewById(R.id.monkey);monkey.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdMonkey, 1.0f, 1.0f, 1, 0, 1.0f);}});ImageView lion = (ImageView) findViewById(R.id.lion);lion.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdLion, 1.0f, 1.0f, 1, 0, 1.0f);}});ImageView dog = (ImageView) findViewById(R.id.dog);dog.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdDog, 1.0f, 1.0f, 1, 0, 1.0f);}});ImageView cat = (ImageView) findViewById(R.id.cat);cat.setOnClickListener(new View.OnClickListener() {@Overridepublic void onClick(View view) {animalSamples.play(sIdCat, 1.0f, 1.0f, 1, 0, 1.0f);}});A completed onCreate() method is seen in Figure [12-31](#Fig31), and implements the creation of the application’s functionality using less than three dozen lines of code, including declarations at the top of your class. The method that you wrote was around a dozen lines of code, and including imports, you have added (or modified), nearly 50 lines of code (not including XML UI), allowing you to create a learning app for children using Android Studio’s scrolling design pattern.Notice that Android Studio automatically implements the Java lambda expression format for the event listener constructs, shown in Figure [12-31,](#Fig31) in the first three of your animal audio processing constructs. This is a coding shortcut that removes the (new View.OnClickListener) and replaces this part of the structure with the view object and an arrow and then replaces the { @Override public void onClick(View view) } inner structure with your (inner) event handling structure’s programming logic, which greatly simplifies the event listening and event handling structure. In this case, this is animalSamples.play(sIdAnimal, 1.0f, 1.0f, 1, 0, 1.0f).As you can see, implementing SoundPool is not so complex that an Absolute Beginner cannot understand it and harness its power, but it does require a decent understanding of digital audio concepts, optimization techniques, and the SoundPool class (engine), which is much more complex than what I have exposed you to here. I wanted to show you how to get it working and that it can indeed work in many applications even at a rudimentary level. If digital audio interests you there is a Digital Audio Editing Fundamentals (2015) title available from Apress.There is a ton of fundamental digital audio effects (panning) and synthesis (pitch-shifting) that you can tap into using the left and right audio level (leftVolume, rightVolume) and sample rate settings. You can also loop audio (loop = -1 for infinite looping), or prioritize memory holding audio effects for your custom game engine design. Just remember, the less memory your samples use (the better optimized they are), the better SoundPool works.This class also has some code optimization of its own to implement still, and probably a few bugs left to fix, so you may get some Gradle runtime warnings regarding audio playback speed, AudioAttributes, AudioManager (constants) used, and the like. If you get these, simply Google the error message and SoundPool and Android, in the same keyword string, and go to StackOverFlow, and see how other developers are solving the same problem. For instance I got an AUDIO_OUTPUT_FLAG_FAST denied by client, possibly because I set USAGE_GAME. The audio still worked perfectly, at least in an AVD emulator (which is the client in this case).If you are going to use the more powerful multimedia engines in Android OS 7.1.1 and JavaFX8, such as SoundPool, MediaPlayer, PorterDuff, SVGPath, Vulkan, OpenGL ES, and i3D rendering, and the SQLite databases, some complexity and ongoing development bugs are to be expected as these advanced features are still being added, debugged, and hopefully, perfected.

## 摘要

In this chapter, you learned all about analog and digital audio concepts and techniques that will allow you to create digital audio applications for the Android 7.x OS, Android Wear, Android TV, and Android Auto platforms. I covered analog and digital audio concepts such as MIDI, waves, sample rate, frequency, PCM, codecs, and how to use PCM format as a baseline, and how to optimize it using sample frequency and stereo versus mono tracks, and how codecs can further reduce your APK file size with little audible loss of perceptible quality to end users.You learned about how to use Audacity 2.1.2 to optimize digital audio samples that you found on the free audio sample website on the Internet. You learned how to create yet another of Android Studio’s pure android design pattern templates, ScrollingActivity, and how to customize it for use as a children’s animal learning application. You went onto the Internet and found free-for-commercial-use PCM audio samples at CD quality as well as matching images in HD quality to use in creating a young (age 2–6) children’s educational application.You learned about the Android SoundPool digital audio sequencing engine class, and how it allows you to add multiple audio samples to your Android application, so that you can add digital audio sequencing capabilities to your application. You learned about the various ins and outs of the SoundPool digital audio sequencing and synthesis engine, as well as about all of its caveats and considerations regarding how it works. You also learned about the related AudioManager and AudioAttributes classes, which contain constants that are used to configure the audio-related capabilities of your Android OS, Android TV, Wear, and Auto applications.You added ImageView tags in your content_scrolling.xml user interface definition and declared and instantiated them in the ScrollingActivity.java code using existing code that Android Studio created for you and added event listening and handling for SoundPool.play(), replacing SnackBar. You created a setupAnimalSamples() method and added digital audio sound effects to each of the ImageView UI elements to teach kids sounds animals make.Next, in Chapter [13](13.html), you’ll learn all about Service classes in Android, which you actually got some exposure to in this chapter when you implemented the AudioManager object. An instance of this AudioManager object can be obtained using a call to Context.getSystemService(Context.AUDIO_SERVICE) (an Android Service). The next chapter is thus a logical extension to what you learned in this chapter, as well as previous chapters where a Service was used. As you can see, I’m trying to cover things in the most logical, optimal fashion.