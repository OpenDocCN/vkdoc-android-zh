© Wallace Jackson 2017Wallace JacksonAndroid Apps for Absolute Beginners10.1007/978-1-4842-2268-3_9

# 9.Android 图形设计:使 UI 设计可视化

Wallace Jackson<sup>1</sup>(1)Lompoc, California, USAIn the first half of this book, I tried to stay as much “inside” of Android Studio, the Android API, and Android OS as much as possible, so that we can get the IDEA to either code or help us design as much of an application as possible. This approach still gives you a good “head start” on the core classes, methods, and design concepts that an Android 7 developer should have knowledge of, and eventually mastery of. I am trying to give you the global overview of what the main components are in Android currently, and how everything in Android OS fits together.In the second half of the book, which will show you how to incorporate new media elements into your app’s UI design, such as digital imagery, digital audio, digital video, and 2D animation, we will venture “outside” of the core Android development tools, allowing you to use open software packages outside of Android Studio that are commonly used by application developers for new media content development. In this chapter, we’ll cover digital imagery.We will utilize GIMP 2.8 in this chapter, to explore the graphics concepts for this chapter and get some hands-on experience in applying image compositing concepts and techniques we will be discovering during the course of the chapter. There is no way I can cover all of the graphics concepts and topics I would like to in one single chapter, at least not using less than a hundred pages! If you want to dive into digital imaging specifically, check out the Digital Image Compositing Fundamentals (2015) title or the Android Studio New Media Fundamentals (2015) title, both of which are currently available on the [http://www.apress.com](http://www.apress.com) website (search under my author name of Wallace Jackson).We will be covering several of the core Android classes that are used to implement graphic design elements, such as ImageButton, ImageView (which we have already covered in Chapter [6](06.html)), and NinePatch. During this chapter, you will utilize these three Android graphics classes to skin the Sliding Drawer UI layout container design. Skinning the UI design using graphics elements will enhance the visual aspects of this UI design, and increase its interest to the end user, as well as increasing its perceived level of professionalism. First, we will go over some foundational digital imaging concepts, so that this material makes more sense to you.

## 成像概念、格式和技术

The first thing that we need to do is to get a knowledge foundation regarding the concepts, formats, and terms that we are going to use during the rest of this book regarding working with digital images. This area is most commonly called graphic design, although I am going to approach it from a more professional “digital imaging” or “image compositing” standpoint. As you already know, Android terms graphic design elements drawables.The concepts, formats, techniques , and terminology that I cover in this chapter will also apply to animation and digital video new media assets, as both of these are based on digital imagery in one way or another, so we will be able to build on the knowledge base created in this chapter in future chapters, which I am trying to do in each and every chapter in this book, so your knowledge of Android 7.1.1 development increases exponentially, and in a logical fashion.First, we will cover the pixel, the foundational element of the digital image; and then the concept of resolution, or the size of the digital image; and then the concept of aspect ratio , or the shape of the digital image. Once we are done with the second dimension (2D) aspects, we will go into the third dimension (3D) and look at how the colors of each pixel are created, using red, green, and blue (RGB) layers of color, and then at how transparency is defined within an image using a fourth alpha layer that contains transparency values. After that, we’ll get into more advanced digital imaging concepts, like compositing and pixel blending, and take a look at how all this knowledge is used when using digital image formats and their codecs to compress digital image assets.

### 数字图像的基础:像素

Digital imagery is made up of two-dimensional (2D) arrays (grids) containing pixels. The industry term pixel is a conjugation of two words: pictures (commonly called pix) and elements (shortened, to be simply els). The number of pixels in a digital image asset should be expressed using a term called resolution . This is the number of pixels in both the width (denoted using a W, or an X for the x-axis) and the height (denoted using an H, or a Y for the y-axis) image dimensions. The resolution of an image asset is usually expressed using two (X and Y) numbers with an “x” in the middle, such as 800x480, or using the word “by,” such as 800 by 480 pixels.To find the total number of pixels that are in a 2D image, simply multiply the width pixels by the height pixels. For instance, an HDTV resolution 1920 by 1080 image will contain 2,073,600 pixels, over 2 million pixels , also referred to as two megapixels . The more pixels that are in an image, the higher its resolution. Just like with digital cameras, which range from 3 megapixel smartphone cameras to 75 megapixel DSLRs, the more pixels that are in the digital image grid or array, the higher quality level the image will have.Android supports everything from low- resolution , 320 or 480 pixel display screens for Android smartwatches, or entry-level flip-phones and small screen phones; to medium-resolution, 854 by 480 pixel display screens for mini-tablets and smartphones; to high-resolution, 1280 by 720 pixel display screens, for HD smartphones and medium tablets; to extra-high-resolution, 1920 by 1080 pixel display screens for large tablets and iTV sets, to the recent super-high-resolution, 2560 by 1440 phones (such as Samsung’s Galaxy S5), and finally, new ultra-high-resolution, 4096 by 2160 (known as “4K”) pixel display screens for tablets, smartphones, and 4K iTV sets.

### 数字图像的形状:纵横比

A slightly more complicated aspect (no pun intended) of image resolution would be the image’s aspect ratio , a concept which also applies to Android device (hardware) display screens. Aspect ratio is the ratio of width to height or W:H, or, if you like to think in terms of an x-axis and y-axis, it would be X:Y. The aspect ratio will define the shape of an image or display screen; that is, how square or rectangular (popularly called widescreen) the image or the display screen might be. A tall rectangular display is popularly termed a portrait display.A 1:1 aspect ratio display screen (or digital image) is perfectly square, like a smartwatch screen. It is important to note that it is the ratio between these two numbers that defines the shape of the image or screen and not these numbers themselves. That is why it is called the aspect ratio, although it is often called the “aspect” for short.Many Android display screens these days use an HDTV widescreen aspect ratio, which is 16:9. However, some use a less wide, or taller, 16:10 (or 8:5, if you prefer) aspect ratio. Even wider screens will also surely appear on the market soon, so look for 16:8 (or 2:1, if you prefer) ultra-wide screens, which will have the 2160 by 1080 resolution, and instead of being taller than 16:9 aspect ratio screens will be shorter than 16:9 screens.An image aspect ratio is usually expressed as the smallest set or pair of numbers that can be achieved on either side of the aspect ratio colon . If you paid attention in high school, when you were learning all about lowest (or least) common denominators, then this aspect ratio mathematics should be fairly easy to understand and to calculate.I usually do this mathematical matriculation (say this five times rapidly, to make what we are about to do seem easier) by continuing to divide each side by two. Taking the fairly odd-ball 1280 by 1024 (called SXGA in the display industry) resolution as our example, half of 1280:1024 is 640:512, and half of that would be 320:256, half of that is 160:128, and half of that again is 80:64, half of that is 40:32, half of that is 20:16, half of that is 10:8, half of that is 5:4, so an SXGA screen will be said to utilize a 5:4 aspect ratio.All of the above ratios are the same aspect ratio, and all are valid. So if you want to take the really easy way out, replace the “x” in your image resolution with a colon and you have an aspect ratio for the image, although distilling it down to its lowest format, as we’ve done here, is far more useful to visualize the shape.Original PC screens used a squarer 4:3 aspect ratio, and early 3:2 aspect ratio CRT “cathode ray tube” TV sets were nearly square. Smartwatch aspect ratios are square, at 320x320, 400x400, 480x480 or 1:1 aspect ratio. HD and UHD iTV sets use a 16:9 widescreen aspect ratio. The closer the numbers on either side of the colon are to each other in size, the more square an image or a screen will be.A 2:1 aspect is a widescreen display, and a 3:1 aspect display would be downright panoramic! The current market trend is certainly toward wider screens and higher resolution displays; however, Android smartwatches could change this trend back toward square aspect ratios, which are certainly useful in a wide variety of applications.

### 给你的数码图像上色:RGB 色彩理论

So now you understand digital image pixels , and how they’re arranged using 2D rectangular arrays, at a specific aspect ratio that defines their rectangular shape. The next logical aspect (again, no pun intended) to look into, is how each of those pixels assign their color values . Color values for pixels are defined by the amount of three colors, red, green, and blue (hence the term RGB), which are present in varying amounts in each pixel.Android (as well as other device) display screens utilize additive color, which is where the wavelengths of light for each RGB color plane are summed together. Additive color can be used by consumer electronics devices to create literally billions of different color values, and is used in popular LED displays used in iTVs, smartphones, laptops, e-readers, and tablets. Additive color is the opposite of subtractive color , which is utilized in printers .To show the difference, under a subtractive color model, mixing a red color with a green (using inks) will yield a purple color, whereas in an additive color model, mixing a red color with a green color (using light) will yield a yellow color. Subtractive color models are limited in the spectrum of color that they can produce, whereas the additive color model can produce every color under the rainbow (or, I should say, every color in the universe!).The amount, or numbers, of red, green, and blue “shades” or intensities of light that you have available to mix together will determine the total amount of colors that you will be able to reproduce. In today’s digital device hardware capabilities, we can produce eight bits (8-bit) or 256 levels of light intensity for each red, green, and blue (RGB) color. Some of the newer devices are supporting ten bits (10-bit) or 1024 levels of light intensity .We can generate these color values for each pixel individually, so every pixel in your image can have 256 levels (or 1024 levels) of color intensity variation for each of the red, green, and blue values. This will therefore use one byte (8-bits) of data per red, green, and blue color, unless you are using 10-bit color. A byte uses an 8-bit data value, which allows it to represent the color intensity level from a minimum of zero (off, no color contributed, or black, if all the RGB planes are using this value) to a maximum of 255 (on, maximum color contributed, or white, if all RGB planes are using this value).

#### 数字图像中的颜色数量:色深

The amount of data that is used to represent the amount or number of colors in a digital image is referred to as the color depth of an image. It’s important to note that in digital images less than 8 bits can be used to represent the amount of color in an image when using an indexed color model that uses a color palette instead of RGB. There are several common color depth levels used in the digital imaging industry. I’ll outline the most common ones here, along with the digital image file formats used with each color depth in the Android 7 OS. The lowest color depth exists using 8-bit indexed color digital image format. The indexed color image has a maximum of 256 total color values per pixel, and would use the GIF and PNG8 image formats to contain this indexed color type of digital image data. An indexed color image does not have three RGB color planes, so it is generally three times smaller than “True Color” RGB imagery. Instead of three RGB color planes, indexed color uses a palette , which is a data array containing up to 256 color values, used to represent all of the colors in the indexed image.A medium color depth image, which is not natively supported in Android, but which I will discuss here, for continuity of learning, will feature the 16-bit “High Color” color depth. A high color depth image will contain 65,536 colors. This would be calculated as 256 times 256 (8-bit and 8-bit is 16-bit), and is supported using TARGA (TGA), Tagged Image File Format (TIFF), and the Windows BMP digital image file format.A 24-bit truecolor color depth image will feature the full 8-bit color data values for each RGB color plate (color plane) and will be capable of displaying more than 16.7 million potential colors per pixel. This is calculated as 256 times 256 times 256 (8-bit red and 8-bit green and 8-bit blue is 24-bit RGB), and equals 16,777,216 colors. Android file formats that are capable of supporting 24-bit color include JPEG (using the .jpg file extension), PNG24 and PNG32 (using a .png extension), and WebP (using a .webp extension), which stands for WebPhoto.Using 24-bit color depth will give you the highest digital image quality level, which is why Android 7.1.1 prefers the use of the PNG24 or the JPEG image file format. Since PNG24 is lossless, which means that it loses no quality (and none of the original image’s data) during the compression process, it has the highest quality compression, and the lowest original image data loss, along with the highest quality color depth. This is a reason why PNG24 and PNG32 are the preferred digital image formats to use, as far as Android OS is concerned, because the usage of Portable Network Graphics ultimately produces the highest quality visual result for an Android application.It is important to note that there are higher color depth images out there, as we discussed with 30-bit color, or “Deep Color,” images. These are also called HDRI, or High Dynamic Range Images , which use 30-bit, 36-bit, 48-bit, and even 64-bit color depth (if there is an alpha channel). The hope is that the Android 7.1.1 OS (and device hardware) will move to support these extremely high quality digital image color depth standards, which are now currently being utilized for advanced i3D console games, as well as in the new Samsung SUHD 4K iTV Set.A 30-bit deep color depth image will feature a full 10-bit color data value for each RGB color plate (color plane), and will be capable of displaying 1,073,741,824 potential colors per image pixel. This is calculated as 1024 times 1024 times 1024 (10-bit and 10-bit and 10-bit is 30-bit). High Efficiency Video Coding (HEVC) which Android now supports, allows a color depth of 8-bits to 10-bits per color plane . The second version of HEVC will feature five profiles, which allow for a bit depth of 8-bits to 16-bits per sample, or 48-bit color digital video assets.

#### 用 Android 表示颜色:十六进制符号

Now that you know what color depth is, and that colors are represented as a combination of three different red, green, and blue (RGB) color channels within any given image, we need to look at how as Android 7.x, Java 8, and JavaFX 8 (now part of Java) programmers, we are going to represent these three RGB color channel values inside of our Android applications. This knowledge will allow us to create any single one of these 16,777,216 colors, and even add alpha channel transparency values, which we will be covering in the next section of this chapter.It is important to note that in the Android OS, color is not only used in digital images, commonly called bitmap images, but also in 2D illustration, which is commonly referred to using shapes or vector SVG imagery, as well as in color settings, such as the background color value utilized in your Android 7 Theme specifications , or for your textColor values, for instance, that define what color your text UI elements will be.In Android, different levels of RGB color intensity are represented as data values using hexadecimal notation . Hexadecimal notation is based on the original Base-16 computer notation used decades ago to represent 16 bits of data value. Unlike Base-10, which counts from zero through 9, Base-16 counts from zero through F, where F would represent a Base-10 value of 15 (remember that counting from zero through 15 gives you 16 data values).To tell the Android OS that we are giving it a hexadecimal value, we preface these Base-16 values with a pound sign like this: #FFFFFF. Because each slot in this 24-bit hexadecimal representation represents one Base-16 value, to get the 256 values we need for each RGB color will take 2 of these slots, as 16 times 16 equals 256\. Thus, for a 24-bit image, you would need 6 slots after the pound sign, and for a 32-bit image, we would need 8 slots after the pound sign. We will be covering what these 32-bit type of digital images are, and what they are used for, during the next section of this chapter, when we look at alpha channels.These hexadecimal data slots represent the RGB values in the following format: #RRGGBB . Thus, for the color white, all red, green, and blue channels in this hexadecimal color data value representation are at the maximum luminosity of fully on, or FF, which would be 16 times 16, or a full 256 data value for each RGB color channel. For this reason, the value #FFFFFF would represent the whitest white that is possible on a given device screen.NoteAs you can see, I'm giving you all of the different industry terminology (color channel, color plane, color plate) that you will find currently being used in the graphics industry. All of these terms can be used interchangeably.When you additively sum all of the RGB colors together, you will get white light. As I have mentioned before, the color yellow is represented by the red and green channels being on, and the blue channel being off, thus the hexadecimal notation representation for the color yellow would be #FFFF00, where both red and green channel slots are on, using FF for a color intensity (level) value of 256, and the blue channel slots are fully off, using 00.As I mentioned earlier in this section, there is also a 32-bit image color depth whose data values are represented using the ARGB color channel model, where the A stands for alpha, which is short for alpha channel. We’ll be going over the concept of image alpha and alpha channels in detail during the next section of the chapter, and we will also cover the related concept of pixel blending .The hexadecimal notation data slots for an ARGB color channel model data values will hold data in the following format: #AARRGGBB . Thus, to represent the fully opaque color white, all alpha, red, green, and blue channels in this hexadecimal color data value representation would be at a maximum luminosity (as well as opacity), and the alpha channel fully opaque, as represented by an FF value, so its hexadecimal value would be #FFFFFFFF.A 100% transparent alpha channel is represented by the alpha channel slots being set to zero; thus, a fully transparent image pixel could be #00FFFFFF , or #00000000\. It is important to notice that if an image alpha channel is set to be fully transparent, then each pixel’s color value (represented by the last six hexadecimal data slot values) does not even matter , and thus you could put any color value into these last six data slots.

### 图像合成透明度:Alpha 通道

In this section of the chapter, we will take a look at how digital images are composited together, which is a process known as image compositing , which is usually done by a professional who is called a digital image compositor. Digital image compositing is a process of blending together more than one layer of digital imagery in order to obtain a resulting image. A composite image on a display screen will appear as if it is one single image, when, in fact, it is actually a collection (a “layer stack”) of several seamlessly composited digital image layers.To be able to accomplish seamless image compositing using layers, the images used in each layer other than the bottom-most background or “backplate” layer need to have alpha channel (transparency) values associated with each pixel in the image contained within that layer. We can utilize this alpha value for each pixel in the image to precisely control the blending of that pixel with other pixels in the same overall image composite coordinate or location, that is, pixels on other layers above and below that particular image layer using the same pixel coordinates.It is because of this stacked layer paradigm that I refer to this compositing as 3D, as the layers are stacked into place along, or using, the Z-axis, and can be said to have a particular Z-order or Z-depth. Don’t get this confused with 3D modeling software, such as Silo or Blender, as the end result of your digital image compositing (layer) stack is still a resulting 2D digital image asset, and not a 3D geometric model asset.Like the other RGB channels, the alpha channel also has 256 levels of transparency that are represented via the first two slots in the hexadecimal representation for the ARGB data value , which as you have seen has eight slots (for 32-bits) of data, rather than the 6 slots used to represent a 24-bit image. A 24-bit image could thus be thought of as being a 32-bit image with no alpha channel data.If you think about it, if there is no alpha channel data, why waste another 8 bits of data storage, especially if that alpha channel is filled with #FF alpha values, representing fully opaque pixel transparency values. The uncompressed 32-bit image with an alpha channel filled with #FF values has 25% more data than a 24-bit image with no alpha channel, and yet it would yield the same exact visual result. So don’t use a 32-bit image unless you need to use transparency values for the pixels in that image (for multi-layered digital image compositing purposes).Therefore, to summarize, 24-bit imagery has no alpha channel, and isn’t going to be used for image compositing, unless it is the bottom backplate in the compositing layer stack. A 32-bit image is always going to be used as a compositing layer, on top of something else which will need the ability to show through via transparency values defined in the image alpha channel. These transparency values may vary from pixel location to pixel location, in order to allow compositors to create special effects and apply image enhancing treatments, such as anti-aliasing, which we will be discussing later.How does having an alpha channel and using digital image compositing factor into my Android graphics design pipeline, you might be wondering? Your primary advantage here is the ability to split what looks like one single image into a number of component layers. A reason for doing this may be to apply Java code to individual layer elements in order to control various parts of a 2D image that you would not be otherwise able to individually control, were the image simply one single 24-bit image. A single image can be transformed as a whole, or as a pixel array, but not on a per-subject (per pixel area) basis, as it could be if each image element were on its own layer, and thus a separate element in memory, which could later be controlled by using your Java program logic. The Apress Pro Android Graphics title covers this in detail.

### 算法图像合成:混合模式

There is another even more powerful aspect of image compositing called blending modes . Any of you familiar with Photoshop or GIMP know that you can set each layer in your image composite to use a different blending mode . Blending modes are algorithms that specify how the pixels that are contained within a layer are blended (mathematically) with the previous layers (underneath that layer). These pixel blending algorithms will take into account the transparency level. By combining blending modes and transparency, you can achieve virtually any digital image compositing result, or special effect, that you are trying to achieve. Since there are entire books written on using blending modes, and the effects that you can create, I won’t get into this too much here.Interestingly, blending modes can be implemented in the Android OS, by using the Android PorterDuff class. This is a real tribute to, and an indicator of, the 2D power that lies in the Android 7.1.1 software development APIs. The PorterDuff class gives developers many of the powerful blending modes that Photoshop (or GIMP 2.10) affords to digital imaging artisans. The PorterDuff class essentially allows Android apps to implement powerful image compositing features similar to GIMP or Photoshop. The major difference, of course, is that you can control the blending modes interactively, using custom Java programming logic, which is the exciting part for Android 7.1.1 developers. The Pro Android Graphics (2014) title from Apress covers how to implement PorterDuff blending modes inside a complete digital image compositing pipeline, if you are interested in diving into this advanced graphics compositing and blending area of Android 7.1.1 in greater detail.

### 遮罩数字图像:使用 Alpha 通道

One of the most popular uses of the alpha channel is to mask out an area of a digital image. This is usually done in order to create a layer that can be utilized in the digital image compositing layer stack. Masking is the process of extracting subject matter, essentially cutting that subject matter right out of the image , by placing it (pasting it) onto its own transparent layer. I will show you how this can be done in GIMP a bit later.The masking process yields a part of your image on its own layer. The masked subject will be isolated from the rest of the source image, but because of layer transparency, will look like it is still in the final image composite. The advantage to this is that now you can do things to that subject matter, in GIMP, or later on in Android 7.1.1, and not have those operations, whatever they may be (rotate, tint, distort, fade and so forth) affect the rest of that image. If you save one of these transparency layers that has subject matter on it in GIMP or Photoshop the transparency layer will be converted into an alpha channel.The masking work process allows you to put image elements (subject material) to use inside of other images, or in an animation, or to use in a special effects application like Fusion. Digital imaging software (Photoshop, GIMP, Painter, Fusion, or Corel Draw) has many tools and features that are specifically there for use in masking images for use in image compositing. You can’t really do effective image compositing without creating a mask, so this is an important area to master for graphic designers and Android 7.1.1 application developers alike.You can mask automatically, using blue screen or green screen backdrops, and computer software that can automatically extract those exact color values, in order to create a mask using an alpha channel. You can also mask manually, by hand, using a digital imaging software package such as GIMP, and its wide array of pixel selection tools .The most important consideration in the masking process is getting smooth but crisp edges around your masked object, so that when you drop it in a layer over a background image, it looks as though it had been photographed there in the first place. The key to this is a proper selection work process, using digital image software selection tools (and there are a half-dozen of these in GIMP 2.10) in the proper way. Using the optimal work process is the key to “pulling a clean image mask” (more cool graphics industry terms for you to throw around, to make you appear savvy and professional). The Digital Image Compositing Fundamentals (2015) book from Apress covers this work process in more detail, if you happen to be interested in this area of digital imaging, and VFX Fundamentals covers this using Fusion 8.For instance, if there are areas of uniform color around the subject matter you wish to mask, maybe you shot it using a blue screen or green screen, you can use the magic wand tool along with a threshold setting to select everything except the object, and then invert that selection set, in order to obtain a selection set containing the object. Other selection tools contain complex algorithms which can look at color changes between pixels, which can be very useful for edge detection , which you can use in other types of selection work processes. The edge detection selection tool will allow you to drag your cursor along the edge of the object that you wish to mask, while the edge detecting selection tool’s algorithm lays down a precise, pixel-perfect placement of the selection edge, ultimately creating or “pulling” that object’s mask for you. You can also use spline tools to create your Bezier outline, and then convert that outline into a selection set. This approach is covered in the VFX Fundamentals (2016) title from Apress.

### 平滑边缘:抗锯齿的概念

Anti-aliasing is a digital imaging technique that is implemented using an algorithm, where two adjacent colors in an image that share an edge between two color areas are blended together along that edge. This will make that edge appear to be smoother (crisper, or more razor sharp) when the image is zoomed out; that is, when the pixels are not individually visible. What anti-aliasing does is it tricks your eyes into seeing a smoother edge, to eliminate what is commonly termed “the jaggies ” due to a jagged appearance along the edges in the imagery.Anti-aliasing provides impressive results , and does so by using only a very few (usually seven or eight) averaged color values of the pixels that lie along the edge that needs to be made smoother. By averaged, I mean some colors, or spectrum of colors, that are a portion of the way between the two colors that are intersecting at the jagged edge in an image, which you can see along the right-hand side in Figure [9-1](#Fig1).![A324674_4_En_9_Fig1_HTML.jpg](A324674_4_En_9_Fig1_HTML.jpg)Figure 9-1.A red circle on a yellow background (left) and a zoomed-in view (right) showing the anti-aliasingI created a basic example of anti-aliasing to show you visually exactly what I mean. In Figure [9-1](#Fig1), you will see that I created a (seemingly) smooth red circle against a yellow background. I then zoomed into the edge of that circle, and I grabbed a screenshot. I placed this alongside of the zoomed-out circle to show these anti-aliasing (orange) values of a color between (that is, made using) the red and yellow color values that border each other along the edge of the circle. If you are looking at black and white, you will see all of this anti-aliasing using grayscale. Luckily for alpha channels, anti-aliasing works just as well in grayscale as it does using color values.The best way to get good anti-aliasing is to use a proper image masking work process, and using the proper settings for any given selection tool that you might be using. One of the other tricks to implementing your own anti-aliasing is to use a Gaussian Blur tool with a very low blur value (0.15 to 0.35) on the (transparency) layer containing the object that has the jagged edges. This will provide the same effect seen in Figure [9-1](#Fig1), and not only that, it will “blur” the transparency values for the alpha channel (mask) itself as well, allowing you to anti-alias that image object with any background imagery you may be attempting to seamlessly composite it against.

### 优化数字图像:压缩和抖动

There are a number of technical factors that affect digital image compression, which is the process of using a codec , which is short for COder-DECoder. A codec is also an algorithm that looks at image data and finds a way to save it in a file using significantly less data. A codec’s encoder essentially finds data patterns in an image, and then turns these data patterns into a form of data that the decoder part of the same codec can reconstruct the original image from, many times with zero loss of image quality. There are strategies that you can use to get a better quality image compression result, which will result in a smaller file size, along with higher image quality. Images with a small file size, and a high level of quality can be said to have a “highly optimized data footprint .”Let’s start out by discussing all of the digital image attributes that affect the image data footprint the most, and later we can examine how each of these aspects will contribute to the data footprint optimization for any given digital image. Interestingly, the order of aspects that are important to data footprint optimization are similar to the order of the digital imaging concepts that we have covered thus far during this chapter.The most critical contributor to the resulting image file size (that is, the data footprint) is the number of pixels, or the resolution of the digital image. This is logical because each of these pixels needs to be stored, along with the color values for each of these pixel’s (ARGB) color and alpha channels. Thus, the smaller you can make the image resolution, while still having the image still look detailed, the smaller the resulting file size will be. This is because there is less data overall that will need to be compressed by the codec’s data encoding algorithm.You can calculate raw, uncompressed image data footprint using the formula:

> 宽度乘以高度乘以颜色通道

Recall that for 24-bit RGB images, there are three (RGB) color channels, and there are four (ARGB) color channels for a 32-bit image. Thus, any uncompressed, true color (24-bit) VGA image will have 640 times 480 times 3, equaling 921,600 bytes of original uncompressed data. If you divide 921,600 by 1024 (the number of bytes in a kilobyte), this will give you the number of kilobytes (K or KB) that are in a raw VGA image, and that number is an even 900KB. Since deep color imagery uses 16-bits per color channel , simply double this amount of data used.As you can see, image color depth is therefore the next most critical contributor to a data footprint of an image, because the number of pixels in that image is multiplied by 1 (8-bit) or 2 (16-bit) or 3 (24-bit) or 4 (32-bit) or 6 (48-bit) or 8 (64-bit) color and alpha data channels. Both GIMP and Photoshop have color channel palettes just like they have layer palettes. Both channels and layers are represented in the same way in these digital imaging software packages, as composited layers along a Z-axis, in a given Z-order. For color channels, this Z-order goes R-G-B.Compact pixel color data (one channel) is one of the reasons indexed color (8-bit) images are still being widely used today, usually via the PNG8 image format, which features a superior lossless compression algorithm to the one that the outdated CompuServe GIF format utilizes. Lossless compression algorithms such as PNG lose zero image data (quality). A lossy compression algorithm, such as JPEG, will throw away original image data, and therefore, some of your image’s quality, to achieve more data compression at the expense of the visual quality.

### 使用索引彩色影像:抖动像素

Indexed color images can simulate truecolor images, if the colors that are used to create the image do not vary widely. Indexed color images use 8-bit data to define the image colors, using a palette of 256 optimally selected colors, rather than 3 RGB color channels. Depending on how many colors are used in the image, using only 256 colors to represent an image can cause an effect called banding, where the transfer between adjoining colors is not smooth. Indexed color image codecs have an option to correct for this visually called dithering. Dithering is the process of creating dot patterns along the edges of two adjoining colors in an image. This tricks your eye into thinking there is a third color being used besides these two colors, when, in fact, there is not.Dithering gives you a maximum perceptual amount of colors of 65,536 colors, (256 times 256), but only if each of those 256 colors borders on each of the other 256 colors; otherwise, this would be less than 65,536 perceived colors. You can see the potential there for creating a plethora of additional colors, and you will be amazed at the result an indexed color image can achieve in some scenarios (that is, with certain images ). Let’s take a true color 3D image, such as the one shown in Figure [9-2](#Fig2), and save it as an indexed color image to show you the dithering effect. We will take a look at this dithering effect on the driver’s side rear fender on the Audi racecar 3D image, as it contains a smooth gradient of gray color.![A324674_4_En_9_Fig2_HTML.jpg](A324674_4_En_9_Fig2_HTML.jpg)Figure 9-2.Truecolor source image using up to 16,777,216 colors that we are going to optimize to 5-bit PNG5I set the Photoshop Save For Web and Devices dialog’s codec to encode a PNG8 image , shown in Figure [9-3](#Fig3), using 5-bit color (32 colors), so that you can see this dithering effect. As you can see, the dot patterns are made between adjacent colors, in this case this is shades of gray, to create a perception that there are additional gray colors beyond the 32 total colors (5-bit data) that are being used to create the PNG5 indexed color image asset.![A324674_4_En_9_Fig3_HTML.jpg](A324674_4_En_9_Fig3_HTML.jpg)Figure 9-3.The dithering effect in an indexed color image with compression set to 32 colors (5-bit color)It is interesting to note that there is this option to use less than 256 colors when compressing your 8-bit indexed color image. This is usually done to reduce the data footprint even further. For instance, an image that can attain good results using 32 colors would actually be a 5-bit image, and thus is PNG5, even though the general format is termed PNG8, or Indexed PNG. The Colors: “spinner ” value selector is where you set this number of colors, and you can set any number from 2 (1-bit color, or on or off, or black and white) up to 256 colors (8-bits data).Also, notice that you can set a percentage of dithering to use. I usually select either the 0% or 100% setting, but you could fine-tune your dithering effect anywhere in between those two extreme values. You can also choose a dithering algorithm type; I use diffusion dithering , as it will yield a smoother gradient effect along an irregularly shaped gradient, such as the one that you see in Figure [9-3](#Fig3), on the Audi racecar fender.Dithering, as you might well imagine, will add data patterns to an image that are more difficult for the codec’s algorithm to compress. Because of this dithering should increase your data footprint by a few percentage points. Be sure to check the resulting file size with, and without, dithering applied, to make sure the dithering provides improved visual results, and to see if the dithering adds any “data weight” to the resulting file size (data footprint).The final concept that we have learned about so far that can increase the data footprint of the image is the alpha channel, as adding an alpha channel will add another 8-bit (or 16-bit for deep color imagery) color data channel containing pixel transparency data values to the image being compressed. If you need an alpha channel to define transparency, in order to support future compositing needs with that image, there is not much of a choice but to include this alpha channel data. Just make sure not to use a 32-bit image format to contain a 24-bit image with an empty (completely opaque, and not defining any transparency) alpha channel .It is interesting to note that many alpha channels that are used to mask objects in an image will compress very well. This is because alpha channels contain large areas of white (opaque) or black (transparent), with very little gray value in the pixels along edges between the two colors to anti-alias the mask. These gray values in an alpha channel are essentially the anti-aliasing values, and as you now know, are used to provide visually smooth edge transitions between the masked object and the imagery that will be used behind it. Large areas of the same color will yield the best compression as the codec can essentially say “this entire area is white, and this entire area is black,” rather than “this pixel is this color, and that pixel is that color,” and so on.Alpha channels can also provide real-time anti-aliasing for applications that utilize compositing. The reason for this is because in an alpha channel image mask, the 8-bit transparency gradient is defined using a white to black spectrum (gradient) that defines the alpha channel transparency. So these gray values along the edges of each object in the mask are essentially averaging, or blending, the color of the (RGB) subject object (the image that carries the mask with it) with color in the target background image. This essentially provides a real-time anti-aliasing for the image element (object) on your transparency layer with any background imagery that might be placed behind the masked image that carries its own transparency (and anti-aliasing) data via PNG32 format .

### Android 图像格式:无损与有损

Android supports several popular digital image file formats, some of which have been around for decades, and which are also available in other popular open source content development platforms, such as HTML5 and JavaFX.These range from the decades-old CompuServe GIF (Graphic Information Format) and the Joint Photographic Experts Group (JPEG) formats, to the more recent PNG (Portable Network Graphics) and WebP (Web Photo) formats. I will cover these in order of origin, from the older (and less desirable) GIF, to the newer WebP format . WebP support has recently been added to the Opera and Chrome HTML5 browsers.CompuServe GIF is fully supported by the Android OS; however, it’s not recommended for general use. GIF is a lossless digital image file format, as it does not throw away any image data to achieve its better compression result . This is because this GIF compression algorithm (codec) is not as refined (effective) as PNG, and it only supports indexed color, which we covered earlier in the chapter. That said, if all your image assets are already created, and they use the GIF format, you will still be able to use them without any problems (other than the resulting quality to file size ratio) in an Android app. Android 7.x does not yet support the Animated GIF (aGIF) format.The next oldest digital image file format that Android supports is the JPEG format, which uses the truecolor color depth, instead of an indexed color depth. JPEG is a lossy digital image file format, because it throws away the original image data in order to be able to achieve the smaller file size. The file sizes achieved by using this JPEG algorithm can be an order of magnitude (10X) smaller than the original raw uncompressed image data.It is important to note that the original (often referred to by using the term raw) uncompressed image data is unrecoverable after compression by the JPEG codec’s encoder has taken place. For this reason, you will want to make sure you have saved your original uncompressed image file before you compress the image through the JPEG compression algorithm.If you zoom into a JPEG image after compression, you will see discolored areas that clearly were not present in the original image. These “degraded” areas in the JPEG image data are termed compression artifacts in the digital imaging industry, and compression artifacts only occur when you utilize lossy image compression . This is the primary reason why the JPEG file format is not the most highly recommended digital image format for use in Android, as the Pure Android approach we covered in Chapter [8](08.html) seeks to provide a pristine visual result.The most recommended image format for use in Android application development is called PNG, or Portable Network Graphic, file format. PNG is always pronounced Ping in the digital image industry. PNG has both an indexed color version, called PNG8, or PNG5 if you only need to use 32 colors as we saw earlier in the chapter; and a truecolor version, called PNG24, and a truecolor with alpha channel version, called PNG32.The PNG8, PNG24, and PNG32 numbering extensions I am using represent the bit depth of color support, so a truecolor PNG that has an alpha channel would technically be referred to as a PNG32\. Similarly, a PNG using 16 colors would be said to be a PNG4, a PNG using 64 colors could be referred to as a PNG6 , and a PNG using 128 colors could be referred to as a PNG7, and so forth. The reason PNG is the recommended format for use with Android is because it uses lossless compression , and yields high image quality along with a very decent (respectable) compression efficiency.The most recent image format was added to Android when Google acquired ON2 and is called the WebP image format . The format is supported under Android 2.3.7 for image read, or playback support, and in Android 4.0 or later for image write, or file saving support. Image write support in Android, in case you might be wondering, would be used with the Android device camera, so that your users can save (write) images to their SD card or to the “cloud” via remote web server. WebP is a static image version of the WebM video file format, which is also known in the industry as the ON2 VP8/VP9 codec, which was acquired by Google, and released as open source .

## 创建 Android NinePatchDrawable 资产

This section of the chapter will outline how to create the NinePatch graphic using the Android Draw 9-patch tool, which used to be a DOS command prompt based tool (in the previous three editions of this book), but is now integrated into Android Studio 2.3\. NinePatchDrawable objects are unique to Android, although there is some movement to add this intelligent-tiling digital image format to the HTML5 and JavaFX standards as well.A NinePatch image uses the PNG image format, and is designed to be able to tile efficiently and asymmetrically in either the X or the Y image dimension, or in both dimensions at the same time. This allows NinePatch images to be able to morph to fit different size and shape UI widgets and/or display screens, if used as a border element.You will need a source PNG image with which to create your NinePatchDrawable object . I have provided a PNG32 (truecolor with alpha channel) digital image asset, which you’ll find in the project assets repository for the book, called ninepatchframe. The reason I am using an alpha channel to define transparency in the center area of the 9-patch that we’re about to create is so that any image layers (intended composites) that are behind the image asset inside Android will composite perfectly with the 9-patch image asset in the image compositing stack, which we will be implementing using XML UI layout design definition markup, later on in this chapter.

### 安装 Draw 9-Patch 源 PNG32 图像

Let’s get started by copying the ninepatchframe.png source image that we are going to use with the Draw 9-patch editor in Android Studio into your project folder hierarchy, in the tools subfolder. Open your operating system file navigation utility, for Windows this is the Explorer. Download the ZIP file containing the assets for this book from the software repository on Apress.com and copy the ninepatchframe PNG32 file to your project AndroidStudioProjects/NavDrawerPattern/app/src/main/res/drawable folder, as shown in Figure [9-4](#Fig4).![A324674_4_En_9_Fig4_HTML.jpg](A324674_4_En_9_Fig4_HTML.jpg)Figure 9-4.Copy the ninepatchframe.png source file into the NavDrawerPattern/app/src/main/res/drawable folderOnce you launch Android Studio, as can be seen in Figure [9-5](#Fig5), you’ll see the ninepatchframe.png source file in the /app/res/drawable project folder. Right-click on the ninepatchframe.png file and select the Create 9-Patch file menu option. This will bring up the Save As .9.png dialog shown on the right side of Figure [9-5](#Fig5) where you select the same app/src/main/res/drawable folder path that you did in Figure [9-4](#Fig4) to save your NinePatch asset in.![A324674_4_En_9_Fig5_HTML.jpg](A324674_4_En_9_Fig5_HTML.jpg)Figure 9-5.Right-click on the ninepatchframe.png file and select Create 9-Patch file, and save in drawable folderCreating a NinePatch asset does not open it for editing. Right-click the ninepatchframe.9.png file created by the steps in Figure [9-5](#Fig5), and use a Jump to Source option, to open it in a 9-Patch Editor tab, as is seen in Figure [9-6](#Fig6).![A324674_4_En_9_Fig6_HTML.jpg](A324674_4_En_9_Fig6_HTML.jpg)Figure 9-6.Right-click the ninepatchframe.9.png 9-Patch image, and Jump to Source to open the 9-Patch Editor

### 探索 Android Studio 的 9 补丁编辑器

Make sure that the ninepatchframe.9.png tab is selected at the top of the IDE, as well as the 9-Patch tab at the bottom , as seen in Figure [9-7](#Fig7). The left gray pane is the NinePatchDrawable asset editing pane , where you will create your one-pixel wide black lines which allow you to define “9 patches.” These patches will tell the NinePatch class (the 9-Patch engine) in Android what the scalable areas are for this NinePatchDrawable asset are, as well as defining where the center (called the padding area for a 9-Patch asset) content area will be.![A324674_4_En_9_Fig7_HTML.jpg](A324674_4_En_9_Fig7_HTML.jpg)Figure 9-7.Draw out a horizontal patch using the top one-pixel black line segment to define an active X-axis areaThe right pane is the result of the NinePatchDrawable asset configuration shown using a preview pane, as you can see on the right side in Figure [9-7](#Fig7), where you can see what the resulting NinePatchDrawable asset will look like when it is scaled according to the one-pixel black border line definitions that you are about to learn how to define using the left editing pane of the Android Studio 9-Patch Editor. As you can see, currently the 9-Patch definition is incorrect, and skews part of the tiling effect, warping the screws, and the top (and bottom) frame.

#### 定义 NinePatchDrawable 资产的可伸缩区域

To define how your NinePatchDrawable asset will scale along the X-axis dimension, click in the top, one-pixel transparent perimeter area, starting on the right side, after the corner screw, as shown in Figure [9-7](#Fig7), and drag toward the left corner screw. This will draw out a one-pixel black line, which will define your X-axis scalable patch . Once you draw in the rough approximation of what you want, you can fine-tune the line using one pixel, light gray lines that extend into the medium gray surrounding areas (these are difficult to see in the current 9-Patch UI color schema). If you place the mouse-over these gray lines the cursor will change into a double arrow, and you can then click and drag the grayed-out area, until it fits pixel perfectly with the transparency area in the center of the ninepatchframe PNG32 source image, which you are using to create the NinePatchDrawable asset.You can also right-click (or if you use macOS, hold the Shift key, and click) to erase any previously drawn lines. As you can see in the preview pane on the right side of Figure [9-8](#Fig8), you are now getting a visual result that is more in line for what we are going for, as the NinePatchDrawable asset is not distorted.![A324674_4_En_9_Fig8_HTML.jpg](A324674_4_En_9_Fig8_HTML.jpg)Figure 9-8.Turn on the Show patches check box option; finished drawing of the top one-pixel black line segmentNext, let's use one of the more colorful 9-Patch Editor options, the Show patches checkbox option . As you can see circled in red at the bottom of Figure [9-8](#Fig8), this is located at the very bottom of the 9-Patch editing pane. This option is there so that you can visualize your 9-patch X or Y settings using different colors. Look for the empty checkbox next to this Show patches option, and select it, and turn this feature on. As you can see in Figure [9-8](#Fig8), this option will provide coloration for your selection areas, by using a combination of purple and green colors. This will make it more clear to you during your editing process which areas of an image asset are being affected by the patch definitions that you are implementing, by drawing in these one-pixel black perimeter lines.As you can see in Figure [9-8](#Fig8), a number of other useful controls exist at the bottom of the 9-Patch editing pane. These include a Zoom slider, which we will be using later to fine-tune all of our control lines, which will allow you to adjust the zoom level of your source graphic in the editing area from 1X up to 8X. The other slider at the bottom is the Patch scale slider , which will allow you to adjust the preview scaling of your NinePatchDrawable asset, from 2X (200%) to 6X (600%), which is being shown in the preview area , on the right side of the display.The Show lock option check box will allow you to visualize the non-drawable areas of the NinePatchDrawable when you mouse-over them. The Show content option checkbox, which you will be using later on, highlights your content area in the preview pane image, where blue areas will show the region in which any Android View subclass (widget or layout container) content will be allowed to composite (to display, or to show through your transparency from other z-order layers in your user interface compositing stack) with your NinePatchDrawable.Finally, at the top of the editing area, there is a Show bad patches button , which will add a red border around patch areas which might (emphasis on “might”) produce scaling artifacts when the 9-Patch graphic is scaled. Visual excellence for scaled 9-Patch images can be achieved if you strive to eliminate all bad patches in your NinePatchDrawable design, however, depending on the pixel colors used, artifacts may not be an issue (visible).Now it is time to draw in our left one-pixel border to define the Y-axis scaling behavior, as shown in Figure [9-9](#Fig9). As you can see in Figure [9-9](#Fig9), I did not draw this one-pixel black border line all the way down the left side. I did this so that you could visualize how well this Show patches option works. This option will allow us to visualize exactly what we are doing right down to the pixel level, as you can see in Figure [9-9](#Fig9), when you look at the color areas and how they blend with the transparency checkerboard pattern and the source image asset. This precision is necessary if you want to define a pixel-perfect 9-patch image asset for use in your NinePatchDrawable object.![A324674_4_En_9_Fig9_HTML.jpg](A324674_4_En_9_Fig9_HTML.jpg)Figure 9-9.Draw down a vertical Y patch using the left one-pixel black line segment to define a Y-axis patch areaFigure [9-10](#Fig10) now shows your NinePatchDrawable PNG32 image asset with both the top, as well as the left, one-pixel (border) black line definitions in place. As you can now visualize, thanks to the Show patches option, we have now defined our static areas, shown as clear (no coloration), which will not scale, and our scalable areas , shown using the green overlay. The Show patches option has allowed us to do this with surgical pixel precision.![A324674_4_En_9_Fig10_HTML.jpg](A324674_4_En_9_Fig10_HTML.jpg)Figure 9-10.Both your horizontal and vertical patch one-pixel black line segments now define active axis areasAlso notice in Figure [9-10](#Fig10) in the right-hand preview of the 9-Patch Editor that the NinePatchDrawable PNG32 image asset’s patch scale definition is now giving you a professional scaling result. If you grab that scrollbar on the right side of the preview area of the screen, and pull it up or down, you will see that the NinePatch algorithm is now scaling in the portrait as well as in the landscape container shape, with perfect visual results. This is what NinePatch technology is for, after all, to provide asynchronous scaling regardless of the aspect ratio (shape) of a container that is holding the NinePatch asset, so that you don’t have to “lock” aspect ratio, to prevent distortion .

#### 定义 NinePatchDrawable 资产的填充区域

Now that you have defined the scalable areas for your NinePatchDrawable image asset, it is time to define the padding areas for a NinePatchDrawable image asset . This is accomplished by using the one-pixel black border lines on the right and bottom of the editing pane. As you can see in Figure [9-11](#Fig11), I have drawn in, on the right-hand side, the one-pixel black border line segment that is necessary to define the Y image dimension for our center (padding) area for the NinePatchDrawable image asset. The center area, in the case of this PNG32 asset, contains an alpha channel value of #00000000, or 100% transparency, which you could also define, as you now well know, using any other color value, such as white, or #00FFFFFF, and the editor will still display it using a checkerboard pattern, which those of you who use GIMP 2.8.20 or 2.10 know represents transparent pixel values.![A324674_4_En_9_Fig11_HTML.jpg](A324674_4_En_9_Fig11_HTML.jpg)Figure 9-11.Defining the interior padding areas, using the one-pixel black line segments on the right and bottomAlso notice in Figure [9-11](#Fig11) that I have also drawn in a second, one-pixel black border line segment at the bottom of the image. I am doing this in order to define the X dimension for the center padding area, which will define where content or other (composited) assets for our user interface design will display in the NinePatchDrawable image asset. If I didn’t have transparency in this graphic, image assets behind it (on a lower z-order layer) would not show through, so this transparent area further increases the flexibility of using this 9-Patch asset for your Android UI compositing purposes. Due to the padding, images on top of this image would draw inside the NinePatch image. So if the NinePatch had a white interior, and you used it in the background container, the image in the source UI plate would respect the interior padding area, drawing on top of it (background is behind source or foreground).Notice the muted colors in Figures [9-11](#Fig11) and [9-12](#Fig12), which are used to show different layers of the scalable versus padding area definitions. The padding definitions use a gray overlay on a green or purple (or pink, if you prefer) scalable area definition . As you can see on the right side, the NinePatchDrawable scaling result is giving us an exceptionally professional result, regardless of the source image’s orientation or dimensions, which the 9-patch image asset is being scaled into. Notice in Figure [9-12](#Fig12) that I’m pulling the right side one-pixel black border line segment up, showing the light gray patch adjust guides, and how you can adjust padding parameters precisely.![A324674_4_En_9_Fig12_HTML.jpg](A324674_4_En_9_Fig12_HTML.jpg)Figure 9-12.Adjust Padding area via the bottom one-pixel black line segment (showing patch adjust guide)Figure [9-13](#Fig13) shows a finished NinePatch image asset definition, with both a scaling set of border line segments, as well as a padding set of border line segments. We’ve utilized patch definition guides on four sides of a PNG. It’s important to note that if you place the mouse in the left editing pane over any section of a 9-Patch definition , and then hold it there, a tool-tip pop-up will appear giving you the precise pixel patch coordinates for your final NinePatchDrawable component definitions, which you may want to know about for your XML markup or Java 8 code.![A324674_4_En_9_Fig13_HTML.jpg](A324674_4_En_9_Fig13_HTML.jpg)Figure 9-13.Final patch definition using the Show Content checkbox to fine-tune how the padding fits the contentIn our case, this will show that you have utilized 256 pixels minus 26 pixels, or 230 pixels of our total 280-pixel image dimension, for our center scalable area. Note that 256 is a numeric value that scales quite well, because it is a “power of 2” data value (2, 4, 8, 16, 32, 64, 128, 256, 512, 1024). This means that you have used 25 pixels , or half of the 50 remaining pixels, for the actual image assets (bars and screws) that will be scaled. The reason this isn’t 26 pixels is because the NinePatchDrawable image format uses one-pixel borders to define its patches.This also means that the fixed areas of this 9-Patch, in this case, it is a corner of the frame with a standard screw in it (to hold the frame in place, of course, or so it appears), will each be exactly 25 pixels square in size. These corner areas of the NinePatchDrawable will not be distorted in either the X or Y dimension, although these may be uniformly scaled as needed. There are enough corner pixels (more than two dozen) to be able to scale up, if it is used on high pixel density (resolution) screens, and to have the detail to appear photo realistic if scaled down.As you can see in Figure [9-13](#Fig13), on the right-hand side of your 9-Patch Editor preview pane, our scaling picture frame graphic looks extremely crisp, and quite realistic. If you scroll the preview pane, this holds across all of the scaling orientation previews . The reason that the interior of the NinePatchDrawable asset is blue is because I have selected the Show content (area definition, that is, padding) checkbox, as shown in red in Figure [9-13](#Fig13).

### 在 Android 中使用您的 NinePatchDrawable 资产

Android Studio will automatically save your NinePatchDrawable image assets as you work on them, saving any modifications to the new asset that has the required .9.png file name extension, which is required by Android OS when using NinePatchDrawable assets. If you want to “force” a save, use a File ➤ Save All menu sequence.When Android detects this type of PNG file in your app/res/drawable folder, it automatically loads it, using the NinePatch class algorithm, and converts it into a NinePatchDrawable image asset once it is referenced in your XML markup. A NinePatchDrawable can be utilized within any Android class that supports using an Android drawable asset. In UI design this is referenced using either a source (android:src) image plate or the background image plate (android:background) parameter. Classes (widgets and layout containers) that are logical for the use of NinePatchDrawable objects include ImageButton, ImageView, View widgets, and ViewGroup subclasses.Now that you’ve created a NinePatchDrawable asset, let’s go into the XML markup for your NavDrawerPattern project that you started creating in the previous chapter, and we'll see how a NinePatchDrawable is installed. After that we will get into using external digital image editing software (GIMP) with Android Studio, and create the digital image assets needed for the multi-state ImageButton widget. I will then show you how to modify the code to change the static image button in your NavDrawerPattern project into a dynamic multi-state UI button.

## 在应用程序中使用 NinePatchDrawable 资源

Let’s fire up Android Studio 2.3 and implement the new NinePatchDrawable asset you just created. As you can see at the bottom of Figure [9-14](#Fig14), an error has appeared in a Messages pane in Android Studio, which informs us that duplicate resources have been detected. Since we just added these PNG files, this tells us and .png and .9.png image assets might be being considered to be the same asset, if they have the same file name. This is fine with us, as we want to use a NinePatchDrawable rather than a BitmapDrawable, because it has unique asymmetric scaling superpowers!![A324674_4_En_9_Fig14_HTML.jpg](A324674_4_En_9_Fig14_HTML.jpg)Figure 9-14.Add the android:background parameter to your content_main.xml’s parent <RelativeLayout> tagLet’s ignore this for now, since we are going to investigate what is causing this, and add an android:background parameter to the UI layout container parent <RelativeLayout> tag, as seen highlighted at the top of Figure [9-14](#Fig14). If you type in “android:bac” you’ll get a pop-up helper with all of the background related parameters. Double-click on the one that says background to insert (add) the parameter in your parent layout container specification.Just to make sure that Android Studio (Android SDK and API) considers both of these files to be the same, let’s start to type the @Drawable referencing path inside of the quotation marks in the android:background parameter we have just added, and see what Android Studio brings up in the pop-up helper dialog . If it shows two of these ninepatchframe assets, we’ll know Android Studio can tell the difference between these two files; if not, it will tell us that a .9.png and .png are considered to be the same if they have identical file name prefixes.As you can see in Figure [9-15](#Fig15), there’s only one ninepatchframe asset on the list of drawable assets, so you need to delete the ninepatchframe.png file, which is easy to do in Android Studio, using the context-sensitive menu. Double-click on the @drawable/ninepatchframe reference to insert it as an android:background reference value.![A324674_4_En_9_Fig15_HTML.jpg](A324674_4_En_9_Fig15_HTML.jpg)Figure 9-15.Reference a ninepatchframe NinePatchDrawable asset inside of the parameter value, using quotesNext, right-click on the ninepatchframe.png file in the Android Studio project pane, and select the Delete option shown numbered as 1 in red in Figure [9-16](#Fig16). When the Safe Delete dialog opens, shown numbered as 2 in red in Figure [9-16](#Fig16), leave both options selected and click OK. When the Usages Detected dialog appears, shown numbered as 3 in red in Figure [9-16](#Fig16) click on the Delete Anyway button since Android Studio can’t differentiate between the .9.png and the .png files, and therefore, in fact, the ninepatchframe.png version is safe to delete.![A324674_4_En_9_Fig16_HTML.jpg](A324674_4_En_9_Fig16_HTML.jpg)Figure 9-16.Right-click on ninepatchframe.png and delete it using Safe Delete option and Delete Anyway buttonAs you can see in Figure [9-17](#Fig17), ninepatchframe.png is now gone and there are no wavy red error underline markings in Android Studio, so we have cured the problem. Use the Run ➤ Run ‘app’ menu sequence to launch the AVD and test the application to make sure there are no gradle, compile, or runtime errors, and to preview how the NinePatchDrawable asset scales to fit the UI design. As you can see at the bottom of Figure [9-17](#Fig17), there are no errors in the build, the load into the AVD, or in the launch of the application in the Nexus AVD emulator.![A324674_4_En_9_Fig17_HTML.jpg](A324674_4_En_9_Fig17_HTML.jpg)Figure 9-17.Review completed error-free content_main.xml, and use Run ➤ Run ‘app’ and see no runtime errorsAs you can see in Figure [9-18](#Fig18), the ninepatchframe.9.png asset scales to fit the content view layout container perfectly, with no scaling or stretching artifacts, and a visually professional looking end result.![A324674_4_En_9_Fig18_HTML.jpg](A324674_4_En_9_Fig18_HTML.jpg)Figure 9-18.Run the app, and make sure the NinePatchDrawable is in the background of content_view UI layoutClick on the Menu (three vertical bars) icon and side the drawer out over the content view to make sure the app is working , and to see how your NinePatchDrawable asset looks with the tinting that dims out the content view.Notice that we will either have to add some padding to the RelativeLayout, or to the TextView, and to the ImageButton UI element in order to nudge them back inside of the NinePatchDrawable image frame, which we will do during the next section of the chapter when we upgrade the ImageButton element to use multi-state imagery with 3D components to add some pizzazz to the current application.Next, let’s create different density PNG32 image assets for use in upgrading the e–mail button to be multi-state.

## 创建多态 PNG32 图像资源

Since we are going to look at the ImageButton class next as well as how to create multi-state (mouse-out or un-clicked, mouse-over or hovered, mouse-down or clicked UI button use-states) UI elements, let’s take a quick look at how to create digital image assets using GIMP and how to save them as XXHDPI, XHDPI, HDPI, and MPDI assets for use with 4K, 2K, Blu-ray, and 1K screen resolution devices. Since this is the chapter on image design and implementation this is important for Pure Android design patterns to make an app look professional.Let’s find a better envelope graphic online. Google “free commercial use artwork” and locate pixabay.com, as shown in Figure [9-19](#Fig19), and enter the word “envelope” in the search bar and then click the magnifying glass icon.![A324674_4_En_9_Fig19_HTML.jpg](A324674_4_En_9_Fig19_HTML.jpg)Figure 9-19.Find the [www.​pixabay.​com](http://www.pixabay.com) free high quality images website and enter “envelope” in the search barClick on the envelope graphics you want to use, and download the native or original (print) resolution that they were created in, as is shown on the right side of Figure [9-20](#Fig20) in green. Click the Download button to download.![A324674_4_En_9_Fig20_HTML.jpg](A324674_4_En_9_Fig20_HTML.jpg)Figure 9-20.Select a professional looking set of envelope graphics and download the original hi-resolution imageIf you want to use the ones I extracted from the above file, visit the book repository and download the PNG files that start with the word envelope and are centered in a 500-pixel square file that fits the ButtonDecoration.png file that we are going to use in GIMP to create image composites for the various ImageButton states. Also, if you have now downloaded and installed either GIMP 2.8.20 (used here) or GIMP 2.10 do so now, and launch it.Use the File ➤ Open as Layers menu sequence to open EnvelopeClosed.png (PNG32 file) into its own layer in GIMP, as shown in Figure [9-21](#Fig21). The checkerboard pattern represents transparent areas (the alpha channel data).![A324674_4_En_9_Fig21_HTML.jpg](A324674_4_En_9_Fig21_HTML.jpg)Figure 9-21.Use the File ➤ Open as Layers menu sequence, and import EnvelopeClosed.png into its own layerUse the File ➤ Open as Layers command a second time to import the ButtonBorder.png file into the layer that is above EnvelopeClosed.png, as shown in the Layers palette shown in the center of Figure [9-22](#Fig22). Use the File ➤ Export As menu sequence to access the Export Image dialog shown on the right side in Figure [9-22](#Fig22), and name the file envelopeclosed.png and use it as an XXHDPI 500-pixel digital image, for use with the IMAX (4K by 4K) or UHD iTV Set (4K by 2K) resolution.![A324674_4_En_9_Fig22_HTML.jpg](A324674_4_En_9_Fig22_HTML.jpg)Figure 9-22.Use File ➤ Open as Layers to import ButtonBorder.png into a layer and export as envelopeclosedNext, click on the EnvelopeClosed.png layer to select it in blue, as shown in the left half of Figure [9-23](#Fig23), so that when you import (open) the next graphic it goes underneath the topmost ButtonBorder.png layer, as can be seen on the right half of Figure [9-23](#Fig23). Use the File ➤ Open as Layer to open the EnvelopeOpened.png file and place it into its own layer. Then select the ButtonBorder.png layer, seen in blue on the right, and use the Colors ➤ Hue-Saturation menu sequence, and shift the Hue value -20 degrees to a bright cyan color, as shown in the Preview area on the right half of Figure [9-23](#Fig23). In this way the hoop will brighten on mouse-over (hover) and the envelope will magically open, connoting the “open mail” function for the ImageButton. Use the File ➤ Export As menu sequence to open the Export Image dialog, as shown on the right side in Figure [9-24](#Fig24), and name this PNG32 file envelopeopened.png.![A324674_4_En_9_Fig23_HTML.jpg](A324674_4_En_9_Fig23_HTML.jpg)Figure 9-23.Select the EnvelopeClosed layer, and use Open as Layers to add the EnvelopeOpened layer on top![A324674_4_En_9_Fig24_HTML.jpg](A324674_4_En_9_Fig24_HTML.jpg)Figure 9-24.Use Open as Layer to import EnvelopeFilled.png into a layer and shift the ButtonBorder.png to goldFollow the same work process and select the EnvelopeOpened.png layer and import the EnvelopeFilled.png into its own layer, as can be seen on the left side of Figure [9-24](#Fig24). Use the Colors ➤ Hue-Saturation dialog to shift the UI hoop border decoration to a gold color, to offset the blue envelope color , and use the File ➤ Export As menu sequence to open the Export Image dialog, as shown on the right side in Figure [9-24](#Fig24), and name this PNG32 file envelopefilled.png. I’m saving my ImageButton files in an AA4AB4/CH09/ImageButton folder, as you can see.The next step in creating multi-state image assets for an Android application is to save multiple resolution DPI versions to fit the Android XXHDPI, XHDPI, HDPI and MDPI constants used to define screen density and resolution. I’ll use the original 500 pixels for Ultra High (4K) XXHDPI, 250 pixels for Extra High (2K) XHDPI and 125 pixels for High (Blu-ray 1280) HDPI and we’ll even create a Medium (1024 and 854 resolution) MDPI asset for older (or less expensive) low-resolution phones and tablets. Let’s use the Image ➤ Scale Image menu sequence to open the Scale Image dialog, seen in Figure [9-25](#Fig25), and down sample the image pixels 100%, to 250 pixels, and then use the File ➤ Export As work process, and export (save) the file as envelopefilledxhdpi.png.![A324674_4_En_9_Fig25_HTML.jpg](A324674_4_En_9_Fig25_HTML.jpg)Figure 9-25.Use Image ➤ Scale to create 250-pixel XHDPI version and File ➤ Export to save envelopefilledxhdpiOnce you have saved the XHDPI asset, use the Edit ➤ Undo Scale Image to return to the 500-pixel version, so that when you scale to 125 pixels the algorithm has the original (and more) data to resample so it can obtain the best result. Then use the Scale Image dialog and Export Image dialog to create and save envelopefilledhdpi.png.Perform the same work process (Undo, Sample to 64 pixels, Export/Save) and create a envelopefilledmdpi.png asset. Then you will have all four assets done for the envelopefilled ImageButton state. Repeat this process for the other two envelopeclosed and envelopeopened multi-state buttons to create image states for all resolution devices. The Scale Image dialog settings can be seen in Figure [9-26](#Fig26) for the last two button state resolutions .![A324674_4_En_9_Fig26_HTML.jpg](A324674_4_En_9_Fig26_HTML.jpg)Figure 9-26.Create HDPI 125 pixel and MDPI 64 pixel resolution versions, for use with Blu-ray and 1K and lower

## ImageButton 类:多状态按钮

Android’s ImageButton class is a direct subclass of the ImageView class, which is itself a subclass of the View superclass, which, as you learned in Chapter [6](06.html), is a subclass of the java.lang.Object master class. It is primarily utilized to create UI widgets and layout containers (via the ViewGroup superclass). The Android class hierarchy for the ImageButton class would thus be structured as follows:java.lang.Object> android.view.View> android.widget.ImageView> android.widget.ImageButtonThe ImageButton class, like its parent class ImageView, is stored in a separate Android package for UI widgets, which is called the android.widget package . This is because the ImageButton is an often-used UI widget which can be leveraged to create custom button UI widgets which can be skinned using Android drawable objects. We’re looking at this class in this chapter not only because it is used frequently by Android developers, but also because it is the parent class (superclass) used to create the FloatingActionButton used in NavDrawerPattern, so anything we can do with an ImageButton should also apply to the FloatingActionButton class.The ImageButton class is extremely powerful, as Android drawable objects can be BitmapDrawable (images), NinePatchDrawable (asymmetric tiling imagery), AnimationDrawable (animation), ShapeDrawable (vector illustrations) or any other Android drawable subclass. I wish I could cover all the Android drawable subclasses in this book; however, if you wanted to explore Android drawables further, you’ll want to check out the Apress Pro Android Graphics (2013) title.The ImageButton UI widget would be used when developers need to create a custom button UI element, which will display the button using an image instead of using a standard text label on a square gray background, as the standard UI Button widget should do. We have already implemented both Button and ImageButton elements in previous chapters that covered user interface layout design classes, including the LinearLayout, RelativeLayout, CoordinatorLayout, and DrawerLayout.Just like the Android Button class UI widget, an ImageButton UI widget can be pressed, using a click or a touch event by the user, and can have button focus characteristics defined as well by implementing multi-state images , such as hovered (over but not pressed), and the default (not pressed and not hovered, that is, not used yet) state.If you don’t utilize any of its custom parameters, your ImageButton widget should have a visual appearance of a standard Android Button UI object with a gray button background that changes color to blue when the button is pressed. The real power of the ImageButton class comes when you use it with alpha channel capable images, in conjunction with multiple image states, both of which you will be learning about in detail during this chapter.The ImageButton UI widget can define up to four different ImageButton states that are defined using XML markup, which we will be doing a bit later on in this chapter. We will cover these ImageButton states in detail in the next section of this chapter, after I cover a couple of the key XML parameters and Java methods here in this section on the ImageButton class member methods and features.The default image for your ImageButton, or in our case FloatingActionButton, UI widget, which will define its normal state, can be defined statically using the app:srcCompat XML parameter in a <FloatingActionButton> child tag inside of a CoordinatorLayout XML layout container UI definition, as you did for your DrawerLayout in Chapter [8](08.html).You can also define a default image for the FloatingActionButton UI widget, which will define its normal state, dynamically (at runtime) in your Java code. You’ll implement this by using the .setImageResource() method. Static definition would be defined as setting an ImageButton state in advance, that is, before execution of your application, using XML markup. Dynamic definition on the other hand occurs during an application execution, in real-time, if you will, by using Java programming logic. Most Absolute Beginners will use a static definition .For this reason, I will be using XML to define our UI designs, which the Android OS prefers that we do, for this book. If you use the app:srcCompat parameter to reference Drawable assets, this will put the Drawable asset on your FloatingActionButton, as you saw in Chapter [8](08.html). Now we are going to enhance this UI design.It is important to note developers can use both the android:background parameter, which allows a background image plate, or layer, to be added to the ImageButton element, as well as the android:src parameter, or, in the case of appCompat the app:srcCompat parameter, which allows you to install a foreground image plate (layer). This enables you to perform image compositing inside of any ImageButton UI element (or subclass) itself.If you do this, you will want to use alpha channels in your images, as we have been, and will be, doing in this chapter. This is why I have been getting into alpha channels so deeply during this chapter, as they will allow you far greater flexibility inside of your Android graphic design pipeline.The reason that you would want to define both a foreground image plate and a background image plate at the same time, in the same UI element, would be so that you could take advantage of the power of digital image compositing that Android affords you by allowing multiple image plates (parameters that support drawable objects). This would give you two image layers (24-bit backplate and one 32-bit compositing layer) and at least one text layer, very similar to what you could do in GIMP, but within a single Android UI element.You can extend this amount of compositing layers in your UI design by using your parent layout container and nested layout containers to also do the same thing, as long as you use alpha channels creatively, which is again why I went into this in such detail at the beginning of this chapter.For instance, if you also set the FloatingActionButton background color value to transparent (#00000000), you can composite it with a background image in your CoordinatorLayout container. This holds true for any other UI elements you position behind the FloatingActionButton, as well as containers (like NavigationView) which are nested inside of other layout containers (such as the DrawerLayout class), such as we’ve done in the current NavDrawerPattern application we are working with during this chapter and the previous one. Next, let’s look at the ImageButton states, which work with hardware pointers (mouse, trackball, etc.) as well as touchscreens to a more limited extent.

### 状态:正常、按下、聚焦、悬停

An ImageButton object allows you to define a custom image asset for each of the states for your UI buttons. States include normal (the default or not in use), pressed (a user touching, or pressing down on, device click selection hardware), focused (recently touched and released, or recently clicked and released) and hovered (a user is over an ImageButton with the mouse or navigation keys, but has not touched it, or clicked on it, as yet).The hovered state was added recently in Android 4, API Level 14, possibly in anticipation of using the Android OS for the Google Chromebook product, which now runs Android applications natively, or in anticipation of an Amazon Fire iTV set or nVidia Shield iTV set product, or other 2K or 4K iTV sets, which come with a mouse, a keyboard, and one or more game controllers. I’ve summarized the four currently supported ImageButton states as of Android 4.4.4, the last 32-bit Android OS, along with their mouse event programming equivalents, that would be used on non-touchscreen devices, in Table [9-1](#Tab1).Table 9-1.The Android ImageButton Class Primary Image Asset State Constants and Mouse Usage Equivalents

<colgroup><col> <col></colgroup> 
| ImageButtonT3 状态 | 按钮状态描述及其鼠标事件等效 |
| --- | --- |
| 正常 | 默认 ImageButton 不使用时的状态。相当于:鼠标退出T5】 |
| 按下 | ImageButton 被触摸或点击时的状态。相当于:鼠标按下T5】 |
| 专注 | ImageButton 触摸并释放时的状态。相当于:鼠标抬起T5】 |
| 徘徊 (API 14) | ImageButton 状态如果聚焦(未触摸)相当于:鼠标滑过 |

ImageButton UI elements are time consuming to create, because you will want to create a unique digital image asset for each ImageButton state . Different images will visually indicate to a user a different ImageButton state. The reason this can be difficult is not because of the XML markup that is involved, but rather due to extensive digital imaging work that you will need to do for each button, across several ImageButton states and across several different resolution densities, as you have seen earlier in this chapter, when we created the minimum 12 image assets for NORMAL, PRESSED and HOVERED in the four most common resolution densities (4K, 2K, Blu-ray, and 1K or 854 pixel).After learning the lengthy work process for creating the dozen digital image assets you will need to create your XML structures that will implement these multi-state ImageButton UI elements we can then move on to learn the standard work process for defining each ImageButton state. This is done by using an XML drawable asset, in the form of an image selector definition file, which lives in your root /res/drawable folder. This file will use the parent <selector> tag with child <item> tags. The <item> tags will define each of your ImageButton’s states, using digital image asset references. Once this XML definition is set up, Android will handle changing the image state for you based on what the end user is doing with the device hardware.

## 创建 Android 多态图像按钮

The standard work process to define an Android multi-state ImageButton UI element is to utilize an XML Drawable definition file, which will use a <selector> parent tag and be located in the /res/drawable folder .This file will have a parent <selector> tag and child <item> tags that define each of your ImageButton states, using custom PNG digital image asset references. Once an XML definition is set up, Android OS will select the correct image asset to utilize. It will do this based upon hardware device resolution, and the ImageButton state that is needed (normal, pressed, focused, or hovered) at the time.First we need to set up the res/drawable alternate folders that will hold the XXHDPI down to MDPI assets we created earlier, which we will do now, so that you see the correct files displayed in Android Studio once we launch it and begin to create the fab_state.xml file, which will define the various image button visuals (states).Right-click on the /app/res folder, and select the New Directory context-sensitive menu option. This will open the New Directory dialog that is seen in Figure [9-27](#Fig27). Name your first alternate app/res/drawable folder drawable-xxhdpi, and then click on the OK button, and create it. Repeat this work process three more times, and create alternate drawable directories that are named drawable-xhdpi, drawable-hdpi, and drawable-mdpi.![A324674_4_En_9_Fig27_HTML.jpg](A324674_4_En_9_Fig27_HTML.jpg)Figure 9-27.Right-click the /res folder and create a new directory named drawable-xxhdpiYou may notice that as you create these alternate folders that they do not show up under the /app/res/ directory. This is because the Project pane is set to “Android” perspective, that is, how the Android OS “sees” the project resources. This can be seen at the top left of the Android Studio Project pane in a drop-down selector, which has a downward facing arrow on the right side, which you can click to access the drop-down menu item list, as can be seen in Figure [9-28](#Fig28) in the top left portion of the screenshot .![A324674_4_En_9_Fig28_HTML.jpg](A324674_4_En_9_Fig28_HTML.jpg)Figure 9-28.Select the Project perspective from your Project navigation pane drop-down menu item selectorSelect the “Package” perspective, which is shown selected in blue in Figure [9-28](#Fig28). This will show you the actual directory and subdirectory path infrastructure that is being used for your project. This can also be seen in the User/user/AndroidStudioProjects folder hierarchy on your hard disk drive, which can be seen in Figure [9-29](#Fig29) on the left side of the screenshot. If you are the inquisitive type, you can take some time now to try some of the other options, and see what the other eight perspective views will show you regarding your Android Studio project.![A324674_4_En_9_Fig29_HTML.jpg](A324674_4_En_9_Fig29_HTML.jpg)Figure 9-29.Select each of the three matching resolution density versions, and drag them into their correct folderNext, let’s open your operating system file management utility, and select the XXHDPI PNG files (hold down a CTRL key modifier to group-select non-contiguous files), as shown in Figure [9-29](#Fig29), and drag (and drop) them in the /res/drawable-xxhdpi folder. If you want to copy (instead of move) the files, you can hold down the CTRL key before you drop the files into the folder, which is also shown on the left side in the middle of Figure [9-29](#Fig29).Figure [9-30](#Fig30) shows resolution density specific files, in resolution density specific folders, in Project perspective.![A324674_4_En_9_Fig30_HTML.jpg](A324674_4_En_9_Fig30_HTML.jpg)Figure 9-30.The density specific PNG files are now in density specific alternate folders in the Project perspectiveRight-click on the res/drawable folder as seen in Figure [9-31](#Fig31) and select New ➤ Drawable resource file. In the New Drawable Resource File dialog name the file fab_state, and keep the default <selector> root (parent) tag .![A324674_4_En_9_Fig31_HTML.jpg](A324674_4_En_9_Fig31_HTML.jpg)Figure 9-31.Right-click on the /res/drawable folder and select New Drawable resource file, and name it fab_stateAndroid Studio will open up the fab_state.xml definition file in your editing area with the <xml> and <selector> tags in place, and ready to add child <item> tags, which hold the state definitions and drawable asset references .The order of the state definitions inside of the <selector> parent selection container is important. This is because these states will be evaluated in the order that they are encountered in the XML drawable definition file. This is why your normal image asset that has the default normal state (that is, no state defined) must be referenced last, because it will only be displayed after android:state_hovered, android:state_pressed , and android:state_focused states are evaluated by the Android operating system. Read it like this: Any hovering? No. Anything Pressed? No. Does it have Focus? No. Then display the normal (unpressed, unclicked, not focused) default state graphic.The <selector> XML state definition parent tag will allow you to implement a selection set, much like the Java switch-case or if-then-else statements. A selector will allow the Android OS to select among several different ImageButton, or its FloatingActionButton subclass, drawable assets based on the set of android:state parameters that will define your ImageButton states. There are dozens of these state parameters, which you will see in your Android Studio pop-up helper, when you type the <item> tags inside the <selector>, which we’ll be doing next.The child <item> tags inside of the parent <selector> tag will implement the android:state parameters , as well as referencing the image assets, using the android:drawable parameter . This is shown in the fab_state.xml tab in Android Studio in Figure [9-32](#Fig32), and utilizes the following XML markup, with pressed and focused using the same graphic:![A324674_4_En_9_Fig32_HTML.jpg](A324674_4_En_9_Fig32_HTML.jpg)Figure 9-32.Enter the four <item> child tags inside of the parent <selector> tag defining your multi-state imagery<? xml version="1.0" encoding="utf-8" ?><selector xmlns:android=http://schemas.android.com/apk/res/android><item android:state_hovered="true" android:drawable="@drawable/envelopeopened" /><item android:state_pressed="true" android:drawable="@drawable/envelopefilled" /><item android:state_focused="true" android:drawable="@drawable/envelopefilled" /><item android:drawable="@drawable/envelopeclosed" /></selector>To get some practice, try using envelopeopened for both hovered and pressed so that you can see all three states when you test the XML (and app) in the AVD emulator, since the emulator simulates a touchscreen and doesn’t support the hover state. With that drawable set up, the envelope will open on mouse-down and fill on mouse-up.You may have noticed when you typed in the @drawable part of the <item> tags that a lot of envelope assets came up in the helper, and we really want only the three asset names to come up, even though we have a dozen different resolution density versions of those PNG32 files. What you need to do is to go into your file manager software (seen in Figure [9-29](#Fig29)) and remove the density part of the filename for all of these (dozen) assets. The result of this can be seen in Figure [9-33](#Fig33), on the left side of the screen , in your Project pane. The reason we can now use the same file name for different resolution density image assets is because they are now “sequestered” away from each other, using different directory (folder) names, so we can use the same names and not have the files replace each other or get a naming error from the operating system regarding duplicate file names, because the folder name is part of the file name’s path. To select between different screen densities at runtime using the different alternate drawable folder names, the actual file names inside of these folders must be exactly the same.![A324674_4_En_9_Fig33_HTML.jpg](A324674_4_En_9_Fig33_HTML.jpg)Figure 9-33.Add an app:backgroundTint parameter to remove a pink background, and tweak the button spacingAll you have left to do to upgrade the FloatingActionButton XML is add the app:backgroundTint="#FFFFFF" parameter, to remove the pink background color, upgrade your android:layout_margin to 40DP, to push in your button corner spacing, upgrade your layout_width and layout_height parameters to use the value of 120DP, and change the app:srcCompat parameter to reference the @drawable/fab_state value, all of which can be seen in Figure [9-33](#Fig33) at the bottom left in the Android Studio app_bar_main.xml editing pane (see yellow highlight bar).It's important to note that not all of these interactive ImageButton states work in every AVD emulator, so if you want to test the focused or hovered states, be sure to use an Android hardware device that supports these to test your multi-state XML markup. The reason I taught you the most extensive and complete work process, all four state across all four resolution densities that would apply to 99% of the devices out there, is so you know how to do this in case you want your application to span every possible hardware device type and configuration. If one of these states is not supported on a given hardware device , the Android OS will simply not access that image asset that is attached to that android:state parameter. To play it safe, implement all these states in all densities!Figure [9-34](#Fig34) shows the envelope closed and envelope filled button states, and the SlidingDrawer is still working.![A324674_4_En_9_Fig34_HTML.jpg](A324674_4_En_9_Fig34_HTML.jpg)Figure 9-34.Test the multi-state FloatingActionButton and SlidingDrawer layout in the Nexus 5 AVD emulator

## 摘要

In this ninth chapter, you learned all about 2D graphic design concepts and principles, as well as about Android drawables and supported image formats, including a NinePatch image format, used to create a type of drawable in Android called a NinePatchDrawable. You learned how to create NinePatchDrawable assets using Android Studio’s Draw 9-patch Editor and then implemented this NinePatch asset inside your SlidingDrawer UI design.Next, you learned about the Android ImageButton class and used GIMP to create all of your assets, 12 of them in total, needed to implement 4 different resolution density multi-state ImageButton elements, each having 4 UI states. You learned quite a bit about the proper way to utilize GIMP for streamlined digital image editing and compositing, and got a lot of practice creating image assets for ImageButton interactive states (which animate a button based on interaction) across 4 resolution densities, spanning smartwatches through 4K iTV sets. You also learned how to implement multi-state ImageButtons and NinePatchDrawables all by using XML markup.Next, in Chapter [10](10.html), you will learn how to create animation in Android, including animation theory, concepts; how to create a wide spectrum of resolution density animation assets, as well as how to create frame animation assets for use with Android's ImageView class; and how to make them work within your SlidingDrawer layout.